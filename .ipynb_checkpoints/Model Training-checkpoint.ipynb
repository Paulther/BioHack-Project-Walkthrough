{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4593a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import obonet\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import Bio.PDB\n",
    "import urllib.request\n",
    "import py3Dmol\n",
    "import pylab\n",
    "import pickle as pickle\n",
    "import torch.nn as nn\n",
    "from torch.nn import Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.utils import erdos_renyi_graph\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import LayerNorm\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Bio import PDB\n",
    "from rdkit import Chem\n",
    "import blosum as bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0856970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMask(graph,indicies,num_masked):\n",
    "    size = graph.x.size()[0]\n",
    "    protein_mask = [False]*size\n",
    "    true_mask = [True] * num_masked\n",
    "    indicies_mask = [False]*(len(indicies) - num_masked)\n",
    "    design_mask = np.hstack((true_mask,indicies_mask))\n",
    "    random.shuffle(design_mask)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(protein_mask)):\n",
    "        if i in indicies:\n",
    "            protein_mask[i] = design_mask[count]\n",
    "            count += 1\n",
    "            \n",
    "    for i, j in enumerate(protein_mask):\n",
    "        if j == 1.0:\n",
    "            protein_mask[i] = True\n",
    "    \n",
    "    return protein_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d77ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5316\n"
     ]
    }
   ],
   "source": [
    "graph_list = torch.load('binding_pocket_graphs.pt')\n",
    "\n",
    "smallest = 5\n",
    "count = 0\n",
    "graph_list_clean = []\n",
    "for entry in graph_list:\n",
    "    if len(entry.designable_indicies) > smallest:\n",
    "        graph_list_clean.append(entry)\n",
    "        count += 1\n",
    "print(count)\n",
    "        \n",
    "        \n",
    "for i, graph in enumerate(graph_list_clean):\n",
    "    graph_list_clean[i].mask = createMask(graph,graph.designable_indicies,int(len(graph.designable_indicies)))\n",
    "    graph_list_clean[i].inv_mask = [not i for i in graph_list_clean[i].mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed2287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_assignment = torch.load('group_assignment_30p.pt')\n",
    "group_size = torch.load('group_size_30p.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8703523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, graph in enumerate(graph_list_clean):\n",
    "    graph_list_clean[i].group = group_assignment[graph.label]\n",
    "    graph_list_clean[i].weight = 1.0/group_size[graph_list_clean[i].group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1049730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[89, 133], edge_index=[2, 2289], edge_attr=[2289, 114], pos=[88, 3], y=[89, 20], label='1ndw', designable_indicies=[23], mask=[89], inv_mask=[89], Kd=[11], group='1ndw', weight=0.1111111111111111)\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(graph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "099e7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Attn(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels, nheads = 8, attn_dropout = 0.5, mlp_dropout = 0.0, neg_slope = 0.2):\n",
    "        super(Graph_Attn, self).__init__(node_dim=0, aggr='add')  # 'add' aggregation for summing messages\n",
    "        \n",
    "        self.in_channels = float(in_channels)\n",
    "        self.neg_slope = neg_slope\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.relu = nn.LeakyReLU(negative_slope = neg_slope)\n",
    "        \n",
    "        self.nheads = nheads\n",
    "        self.c = hidden_channels        \n",
    "        \n",
    "        self.Wq = Linear(in_channels, nheads*hidden_channels)\n",
    "        self.Wz = Linear((in_channels+edge_attr_dim), nheads*hidden_channels)\n",
    "        self.Wv = Linear((in_channels+edge_attr_dim), nheads*hidden_channels)\n",
    "        self.W0 = Linear(nheads*hidden_channels, out_channels)\n",
    "        #self.W0 = MLP(in_channels=nheads*out_channels, hidden_channels= 2*nheads*out_channels ,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = mlp_dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = self.attn_dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        return self.W0(out.view(-1,self.nheads*self.c))\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor, index)-> Tensor:\n",
    "        rij = torch.cat([x_j , edge_attr], dim=-1)\n",
    "        qi = self.Wq(x_i).view(-1,self.nheads,self.c)\n",
    "        zij = self.Wz(rij).view(-1,self.nheads,self.c)\n",
    "        vij = self.Wv(rij).view(-1,self.nheads,self.c)\n",
    "        mij = torch.sum(qi * zij * ((1.0/self.in_channels) ** 0.5), dim = -1)\n",
    "        alphaij = softmax(mij, index)\n",
    "        alphaij = self.dropout(alphaij)\n",
    "        msg = vij*alphaij.unsqueeze(-1)\n",
    "        return  msg\n",
    "    \n",
    "x = torch.rand(14,7)\n",
    "edge_index  = erdos_renyi_graph(14, 0.5)\n",
    "edge_attr = torch.rand(edge_index.size()[1], 5)\n",
    "\n",
    "graph = Data(x = x, edge_index = edge_index, edge_attr = edge_attr)\n",
    "model = Graph_Attn(7,5,10,7)\n",
    "out = model(graph.x,graph.edge_index,graph.edge_attr)\n",
    "#print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f25c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Conv_nodes(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels):\n",
    "        super(MLP_Conv_nodes, self).__init__(aggr='add')  # 'add' aggregation for summing messages\n",
    "        #self.mlp = nn.Sequential(\n",
    "        #    nn.Linear(2*in_channels + edge_attr_dim, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, out_channels)\n",
    "        #    \n",
    "        #)\n",
    "        self.mlp=MLP(in_channels= 2 * in_channels + edge_attr_dim, hidden_channels= hidden_channels,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor)-> Tensor:\n",
    "        return self.mlp(torch.cat([x_j, x_i, edge_attr], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f7ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Conv_edges(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels):\n",
    "        super(MLP_Conv_edges, self).__init__(aggr='add')  # 'add' aggregation for summing messages\n",
    "        #self.mlp = nn.Sequential(\n",
    "        #    nn.Linear(2*in_channels + edge_attr_dim, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, out_channels)\n",
    "        #    \n",
    "        #)\n",
    "        self.mlp=MLP(in_channels= 2 * in_channels + edge_attr_dim, hidden_channels= hidden_channels,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        return self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def edge_update(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor)-> Tensor:\n",
    "        return self.mlp(torch.cat([x_j, x_i, edge_attr], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6695e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Layer(torch.nn.Module):\n",
    "    def __init__(self, node_size, edge_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.msg1 = Graph_Attn(node_size,edge_size,hidden_size,node_size, nheads = 3)\n",
    "        #self.msg1 = MLP_Conv_nodes(node_size,edge_size,hidden_size,node_size)\n",
    "        self.norm_node1 = LayerNorm(node_size, mode = 'node')\n",
    "        self.norm_node2 = LayerNorm(node_size, mode = 'node')\n",
    "        self.norm_edge1 = LayerNorm(edge_size, mode = 'node')\n",
    "        #self.feed_forward = MLP(in_channels=node_size, hidden_channels= 4*node_size,out_channels=node_size, num_layers=2, norm = 'layer', dropout = 0.1)\n",
    "        self.feed_forward = MLP(in_channels=node_size, hidden_channels= 4*node_size,out_channels=node_size, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "        self.edge_message = MLP_Conv_edges(node_size,edge_size,int(4*hidden_size),edge_size)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        msg = self.msg1(x,edge_index,edge_attr)\n",
    "        x1 = self.norm_node1(x + self.dropout(msg))\n",
    "        x2 = self.feed_forward(x1)\n",
    "        x3 = self.norm_node2(x1 + self.dropout(x2))\n",
    "        edge_msg = self.edge_message(x3,edge_index,edge_attr)\n",
    "        edge_attr1 = self.norm_edge1(edge_attr + self.dropout(edge_msg))\n",
    "        return x3, edge_attr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd6f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AA_Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AA_Classifier, self).__init__()\n",
    "        self.node_feature_size = 133\n",
    "        self.node_feature_hidden_size = 128\n",
    "        self.node_feature_size_out = 133\n",
    "        self.edge_dim = 114\n",
    "        self.dropout = 0.1\n",
    "        self.Droput = nn.Dropout(p = self.dropout)\n",
    "        #self.ff_out = MLP(in_channels=self.node_feature_size, hidden_channels= 64,out_channels=20, num_layers=2, norm = 'layer', dropout = 0.0)\n",
    "        self.ff_out = Linear(self.node_feature_size, 20)\n",
    "        #self.ff_out2 = Linear(self.node_feature_size, 11)\n",
    "        #self.relu = nn.LeakyReLU(negative_slope = 0.2)\n",
    "        \n",
    "        self.conv1 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv2 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv3 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv4 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv5 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv6 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv7 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv8 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv9 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self,graph):\n",
    "        x, edge_index, edge_attr = graph.x,graph.edge_index,graph.edge_attr\n",
    "        \n",
    "        x1, new_attr = self.conv1(x, edge_index,edge_attr)\n",
    "        x1, new_attr = self.conv2(x1, edge_index,new_attr)\n",
    "        x1, new_attr = self.conv3(x1, edge_index,new_attr)\n",
    "        x1, new_attr = self.conv4(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv5(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv6(x1, edge_index,new_attr)    \n",
    "        #x1, new_attr = self.conv7(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv8(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv9(x1, edge_index,new_attr)\n",
    "        \n",
    "        return self.ff_out(x1)#, self.ff_out2(x1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b3d4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_list_clean = torch.load('full_graphs_mn_rm_6_11302023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90cf2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = torch.load('train_name_no_sampling.pt')\n",
    "test_name = torch.load('test_name_no_sampling.pt')\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for graph in graph_list_clean:\n",
    "    if graph.label in train_name:\n",
    "        train_data.append(graph)\n",
    "    elif graph.label in test_name:\n",
    "        val_data.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f579a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3986\n",
      "Val: 1330\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", len(train_data))\n",
    "print(\"Val:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "828931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxs = torch.load('idxs_11302023.pt')\n",
    "#train_idxs = idxs[0].detach().numpy()\n",
    "#val_idxs = idxs[1].detach().numpy()\n",
    "#train_data = [graph_list_clean[int(i)] for i in train_idxs]\n",
    "#val_data = [graph_list_clean[int(i)] for i in val_idxs]\n",
    "#torch.save(train_data, 'train_data_12152023')\n",
    "#torch.save(val_data, 'val_data_12152023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee8e11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_data, 'train_data_12202023')\n",
    "#torch.save(val_data, 'val_data_12202023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c84b4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_batch_size = 1\n",
    "full_dl = DataLoader(graph_list,batch_size = 1, shuffle = True)\n",
    "train_dl = DataLoader(train_data,batch_size = train_batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_data,batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "662598a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AA_embeddings_11172023.pkl', 'rb') as f:\n",
    "    AA_embeddings = pickle.load(f)\n",
    "\n",
    "AA_3_letters = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE','LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d51399da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = torch.nn.CrossEntropyLoss(reduction = 'mean', label_smoothing = 0.1)\n",
    "def custom_loss(predict, truth, lossf):\n",
    "    loss1 = lossf(predict[0], truth[0])\n",
    "    loss2 = lossf(predict[1], truth[1])\n",
    "    return (loss1 + loss2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dd24a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Loss: 18.6789 Val Loss: 21.8291\n",
      "Epoch [2/1000] Loss: 18.3039 Val Loss: 21.7545\n",
      "Epoch [3/1000] Loss: 18.2072 Val Loss: 21.6093\n",
      "Epoch [4/1000] Loss: 18.1600 Val Loss: 21.6299\n",
      "Epoch [5/1000] Loss: 18.0932 Val Loss: 21.5989\n",
      "Epoch [6/1000] Loss: 18.0233 Val Loss: 21.4920\n",
      "Epoch [7/1000] Loss: 18.0710 Val Loss: 21.4093\n",
      "Epoch [8/1000] Loss: 17.9798 Val Loss: 21.4368\n",
      "Epoch [9/1000] Loss: 17.9094 Val Loss: 21.3629\n",
      "Epoch [10/1000] Loss: 17.9161 Val Loss: 21.4641\n",
      "Epoch [11/1000] Loss: 17.9201 Val Loss: 21.4089\n",
      "Epoch [12/1000] Loss: 17.8812 Val Loss: 21.3147\n",
      "Epoch [13/1000] Loss: 17.8091 Val Loss: 21.3528\n",
      "Epoch [14/1000] Loss: 17.8378 Val Loss: 21.2082\n",
      "Epoch [15/1000] Loss: 17.8236 Val Loss: 21.3116\n",
      "Epoch [16/1000] Loss: 17.8069 Val Loss: 21.2927\n",
      "Epoch [17/1000] Loss: 17.7780 Val Loss: 21.2753\n",
      "Epoch [18/1000] Loss: 17.7323 Val Loss: 21.1745\n",
      "Epoch [19/1000] Loss: 17.7066 Val Loss: 21.2153\n",
      "Epoch [20/1000] Loss: 17.6974 Val Loss: 21.1496\n",
      "Epoch [21/1000] Loss: 17.6515 Val Loss: 21.0359\n",
      "Epoch [22/1000] Loss: 17.6192 Val Loss: 20.9847\n",
      "Epoch [23/1000] Loss: 17.5883 Val Loss: 21.1005\n",
      "Epoch [24/1000] Loss: 17.5558 Val Loss: 20.9813\n",
      "Epoch [25/1000] Loss: 17.5631 Val Loss: 21.0407\n",
      "Epoch [26/1000] Loss: 17.4984 Val Loss: 21.1061\n",
      "Epoch [27/1000] Loss: 17.4745 Val Loss: 21.1286\n",
      "Epoch [28/1000] Loss: 17.4152 Val Loss: 21.0006\n",
      "Epoch [29/1000] Loss: 17.4300 Val Loss: 20.9876\n",
      "Epoch [30/1000] Loss: 17.3684 Val Loss: 21.1279\n",
      "Epoch [31/1000] Loss: 17.3827 Val Loss: 21.0150\n",
      "Epoch [32/1000] Loss: 17.3496 Val Loss: 20.9511\n",
      "Epoch [33/1000] Loss: 17.3053 Val Loss: 20.7664\n",
      "Epoch [34/1000] Loss: 17.2886 Val Loss: 20.7467\n",
      "Epoch [35/1000] Loss: 17.2556 Val Loss: 20.7762\n",
      "Epoch [36/1000] Loss: 17.2281 Val Loss: 20.6428\n",
      "Epoch [37/1000] Loss: 17.1797 Val Loss: 20.5870\n",
      "Epoch [38/1000] Loss: 17.1920 Val Loss: 20.6602\n",
      "Epoch [39/1000] Loss: 17.1696 Val Loss: 20.6014\n",
      "Epoch [40/1000] Loss: 17.1521 Val Loss: 20.5888\n",
      "Epoch [41/1000] Loss: 17.0972 Val Loss: 20.6044\n",
      "Epoch [42/1000] Loss: 17.0725 Val Loss: 20.5834\n",
      "Epoch [43/1000] Loss: 17.0711 Val Loss: 20.6309\n",
      "Epoch [44/1000] Loss: 17.0499 Val Loss: 20.4469\n",
      "Epoch [45/1000] Loss: 16.9802 Val Loss: 20.5718\n",
      "Epoch [46/1000] Loss: 17.0413 Val Loss: 20.4405\n",
      "Epoch [47/1000] Loss: 16.9538 Val Loss: 20.3304\n",
      "Epoch [48/1000] Loss: 16.9319 Val Loss: 20.4686\n",
      "Epoch [49/1000] Loss: 16.9203 Val Loss: 20.3335\n",
      "Epoch [50/1000] Loss: 16.8947 Val Loss: 20.3518\n",
      "Epoch [51/1000] Loss: 16.8777 Val Loss: 20.2418\n",
      "Epoch [52/1000] Loss: 16.8101 Val Loss: 20.2201\n",
      "Epoch [53/1000] Loss: 16.8311 Val Loss: 20.3958\n",
      "Epoch [54/1000] Loss: 16.7819 Val Loss: 20.2880\n",
      "Epoch [55/1000] Loss: 16.8129 Val Loss: 20.2720\n",
      "Epoch [56/1000] Loss: 16.6961 Val Loss: 20.1101\n",
      "Epoch [57/1000] Loss: 16.6835 Val Loss: 20.1670\n",
      "Epoch [58/1000] Loss: 16.7150 Val Loss: 20.1412\n",
      "Epoch [59/1000] Loss: 16.6535 Val Loss: 20.2066\n",
      "Epoch [60/1000] Loss: 16.6388 Val Loss: 20.2425\n",
      "Epoch [61/1000] Loss: 16.5820 Val Loss: 19.9699\n",
      "Epoch [62/1000] Loss: 16.5871 Val Loss: 20.0804\n",
      "Epoch [63/1000] Loss: 16.5990 Val Loss: 20.1578\n",
      "Epoch [64/1000] Loss: 16.5414 Val Loss: 20.0341\n",
      "Epoch [65/1000] Loss: 16.5496 Val Loss: 19.9220\n",
      "Epoch [66/1000] Loss: 16.5018 Val Loss: 19.9732\n",
      "Epoch [67/1000] Loss: 16.4789 Val Loss: 19.8973\n",
      "Epoch [68/1000] Loss: 16.4361 Val Loss: 19.9195\n",
      "Epoch [69/1000] Loss: 16.4029 Val Loss: 19.9182\n",
      "Epoch [70/1000] Loss: 16.3813 Val Loss: 19.9753\n",
      "Epoch [71/1000] Loss: 16.3417 Val Loss: 19.8896\n",
      "Epoch [72/1000] Loss: 16.3267 Val Loss: 19.9148\n",
      "Epoch [73/1000] Loss: 16.2960 Val Loss: 19.7511\n",
      "Epoch [74/1000] Loss: 16.2649 Val Loss: 19.7045\n",
      "Epoch [75/1000] Loss: 16.2705 Val Loss: 19.6395\n",
      "Epoch [76/1000] Loss: 16.2536 Val Loss: 19.6787\n",
      "Epoch [77/1000] Loss: 16.1847 Val Loss: 19.7562\n",
      "Epoch [78/1000] Loss: 16.1756 Val Loss: 19.6934\n",
      "Epoch [79/1000] Loss: 16.1161 Val Loss: 19.6315\n",
      "Epoch [80/1000] Loss: 16.0998 Val Loss: 19.7086\n",
      "Epoch [81/1000] Loss: 16.0948 Val Loss: 19.7128\n",
      "Epoch [82/1000] Loss: 16.0717 Val Loss: 19.5538\n",
      "Epoch [83/1000] Loss: 16.0374 Val Loss: 19.4502\n",
      "Epoch [84/1000] Loss: 16.0310 Val Loss: 19.5226\n",
      "Epoch [85/1000] Loss: 15.9878 Val Loss: 19.4767\n",
      "Epoch [86/1000] Loss: 15.9515 Val Loss: 19.4868\n",
      "Epoch [87/1000] Loss: 15.9285 Val Loss: 19.5112\n",
      "Epoch [88/1000] Loss: 15.9147 Val Loss: 19.5282\n",
      "Epoch [89/1000] Loss: 15.8751 Val Loss: 19.3794\n",
      "Epoch [90/1000] Loss: 15.8671 Val Loss: 19.4246\n",
      "Epoch [91/1000] Loss: 15.8667 Val Loss: 19.3287\n",
      "Epoch [92/1000] Loss: 15.7826 Val Loss: 19.2903\n",
      "Epoch [93/1000] Loss: 15.7575 Val Loss: 19.4297\n",
      "Epoch [94/1000] Loss: 15.7236 Val Loss: 19.4153\n",
      "Epoch [95/1000] Loss: 15.7223 Val Loss: 19.4842\n",
      "Epoch [96/1000] Loss: 15.6911 Val Loss: 19.1749\n",
      "Epoch [97/1000] Loss: 15.6571 Val Loss: 19.3520\n",
      "Epoch [98/1000] Loss: 15.6147 Val Loss: 19.3143\n",
      "Epoch [99/1000] Loss: 15.6369 Val Loss: 19.2616\n",
      "Epoch [100/1000] Loss: 15.5594 Val Loss: 19.3085\n",
      "Epoch [101/1000] Loss: 15.5503 Val Loss: 19.2243\n",
      "Epoch [102/1000] Loss: 15.5515 Val Loss: 19.2036\n",
      "Epoch [103/1000] Loss: 15.4749 Val Loss: 19.1406\n",
      "Epoch [104/1000] Loss: 15.4623 Val Loss: 19.0618\n",
      "Epoch [105/1000] Loss: 15.4343 Val Loss: 19.1382\n",
      "Epoch [106/1000] Loss: 15.4326 Val Loss: 19.0344\n",
      "Epoch [107/1000] Loss: 15.3989 Val Loss: 19.1905\n",
      "Epoch [108/1000] Loss: 15.3218 Val Loss: 18.9846\n",
      "Epoch [109/1000] Loss: 15.3027 Val Loss: 19.0444\n",
      "Epoch [110/1000] Loss: 15.3014 Val Loss: 19.0174\n",
      "Epoch [111/1000] Loss: 15.2947 Val Loss: 19.0428\n",
      "Epoch [112/1000] Loss: 15.2751 Val Loss: 18.9377\n",
      "Epoch [113/1000] Loss: 15.2300 Val Loss: 18.9647\n",
      "Epoch [114/1000] Loss: 15.2504 Val Loss: 19.0359\n",
      "Epoch [115/1000] Loss: 15.1851 Val Loss: 19.0390\n",
      "Epoch [116/1000] Loss: 15.1940 Val Loss: 18.8941\n",
      "Epoch [117/1000] Loss: 15.1314 Val Loss: 18.8287\n",
      "Epoch [118/1000] Loss: 15.0629 Val Loss: 18.8220\n",
      "Epoch [119/1000] Loss: 15.0641 Val Loss: 18.8267\n",
      "Epoch [120/1000] Loss: 15.0188 Val Loss: 18.8923\n",
      "Epoch [121/1000] Loss: 15.0174 Val Loss: 18.8106\n",
      "Epoch [122/1000] Loss: 14.9681 Val Loss: 18.7827\n",
      "Epoch [123/1000] Loss: 14.9359 Val Loss: 18.7795\n",
      "Epoch [124/1000] Loss: 14.9395 Val Loss: 18.7608\n",
      "Epoch [125/1000] Loss: 14.8843 Val Loss: 18.8203\n",
      "Epoch [126/1000] Loss: 14.8585 Val Loss: 18.8289\n",
      "Epoch [127/1000] Loss: 14.8585 Val Loss: 18.7062\n",
      "Epoch [128/1000] Loss: 14.8766 Val Loss: 18.6541\n",
      "Epoch [129/1000] Loss: 14.8108 Val Loss: 18.6766\n",
      "Epoch [130/1000] Loss: 14.8062 Val Loss: 18.7617\n",
      "Epoch [131/1000] Loss: 14.7107 Val Loss: 18.6587\n",
      "Epoch [132/1000] Loss: 14.7617 Val Loss: 18.7490\n",
      "Epoch [133/1000] Loss: 14.6645 Val Loss: 18.6363\n",
      "Epoch [134/1000] Loss: 14.6857 Val Loss: 18.5633\n",
      "Epoch [135/1000] Loss: 14.6105 Val Loss: 18.5522\n",
      "Epoch [136/1000] Loss: 14.6602 Val Loss: 18.6068\n",
      "Epoch [137/1000] Loss: 14.6078 Val Loss: 18.5209\n",
      "Epoch [138/1000] Loss: 14.5648 Val Loss: 18.5862\n",
      "Epoch [139/1000] Loss: 14.5775 Val Loss: 18.5402\n",
      "Epoch [140/1000] Loss: 14.5368 Val Loss: 18.4713\n",
      "Epoch [141/1000] Loss: 14.4681 Val Loss: 18.4611\n",
      "Epoch [142/1000] Loss: 14.4325 Val Loss: 18.4484\n",
      "Epoch [143/1000] Loss: 14.4600 Val Loss: 18.4948\n",
      "Epoch [144/1000] Loss: 14.4111 Val Loss: 18.4775\n",
      "Epoch [145/1000] Loss: 14.3721 Val Loss: 18.3891\n",
      "Epoch [146/1000] Loss: 14.3273 Val Loss: 18.5560\n",
      "Epoch [147/1000] Loss: 14.3198 Val Loss: 18.3717\n",
      "Epoch [148/1000] Loss: 14.2823 Val Loss: 18.3859\n",
      "Epoch [149/1000] Loss: 14.2645 Val Loss: 18.4565\n",
      "Epoch [150/1000] Loss: 14.2153 Val Loss: 18.4923\n",
      "Epoch [151/1000] Loss: 14.2698 Val Loss: 18.3526\n",
      "Epoch [152/1000] Loss: 14.1831 Val Loss: 18.3058\n",
      "Epoch [153/1000] Loss: 14.1836 Val Loss: 18.3455\n",
      "Epoch [154/1000] Loss: 14.1585 Val Loss: 18.4061\n",
      "Epoch [155/1000] Loss: 14.1312 Val Loss: 18.3519\n",
      "Epoch [156/1000] Loss: 14.1328 Val Loss: 18.3030\n",
      "Epoch [157/1000] Loss: 14.0824 Val Loss: 18.2887\n",
      "Epoch [158/1000] Loss: 14.0581 Val Loss: 18.3082\n",
      "Epoch [159/1000] Loss: 14.0440 Val Loss: 18.1893\n",
      "Epoch [160/1000] Loss: 14.0268 Val Loss: 18.2964\n",
      "Epoch [161/1000] Loss: 14.0057 Val Loss: 18.2651\n",
      "Epoch [162/1000] Loss: 13.9299 Val Loss: 18.1148\n",
      "Epoch [163/1000] Loss: 13.9226 Val Loss: 18.1479\n",
      "Epoch [164/1000] Loss: 13.8400 Val Loss: 18.2064\n",
      "Epoch [165/1000] Loss: 13.8908 Val Loss: 18.0990\n",
      "Epoch [166/1000] Loss: 13.8662 Val Loss: 18.1009\n",
      "Epoch [167/1000] Loss: 13.8446 Val Loss: 18.1069\n",
      "Epoch [168/1000] Loss: 13.8295 Val Loss: 18.2144\n",
      "Epoch [169/1000] Loss: 13.7727 Val Loss: 18.2059\n",
      "Epoch [170/1000] Loss: 13.7451 Val Loss: 18.0766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/1000] Loss: 13.7094 Val Loss: 18.0419\n",
      "Epoch [172/1000] Loss: 13.6997 Val Loss: 18.1499\n",
      "Epoch [173/1000] Loss: 13.6723 Val Loss: 18.0475\n",
      "Epoch [174/1000] Loss: 13.6594 Val Loss: 18.0794\n",
      "Epoch [175/1000] Loss: 13.6255 Val Loss: 18.0625\n",
      "Epoch [176/1000] Loss: 13.5981 Val Loss: 18.1868\n",
      "Epoch [177/1000] Loss: 13.6038 Val Loss: 18.0495\n",
      "Epoch [178/1000] Loss: 13.5533 Val Loss: 17.9498\n",
      "Epoch [179/1000] Loss: 13.5840 Val Loss: 17.9889\n",
      "Epoch [180/1000] Loss: 13.4811 Val Loss: 18.0531\n",
      "Epoch [181/1000] Loss: 13.5269 Val Loss: 18.0000\n",
      "Epoch [182/1000] Loss: 13.4402 Val Loss: 18.0650\n",
      "Epoch [183/1000] Loss: 13.4542 Val Loss: 18.0282\n",
      "Epoch [184/1000] Loss: 13.4055 Val Loss: 18.0115\n",
      "Epoch [185/1000] Loss: 13.3664 Val Loss: 17.9533\n",
      "Epoch [186/1000] Loss: 13.3639 Val Loss: 17.9433\n",
      "Epoch [187/1000] Loss: 13.3131 Val Loss: 17.8809\n",
      "Epoch [188/1000] Loss: 13.3303 Val Loss: 17.8851\n",
      "Epoch [189/1000] Loss: 13.3037 Val Loss: 17.8243\n",
      "Epoch [190/1000] Loss: 13.2501 Val Loss: 17.8767\n",
      "Epoch [191/1000] Loss: 13.2768 Val Loss: 17.8208\n",
      "Epoch [192/1000] Loss: 13.2499 Val Loss: 17.9177\n",
      "Epoch [193/1000] Loss: 13.2399 Val Loss: 17.9047\n",
      "Epoch [194/1000] Loss: 13.1755 Val Loss: 17.8690\n",
      "Epoch [195/1000] Loss: 13.1324 Val Loss: 17.7699\n",
      "Epoch [196/1000] Loss: 13.1448 Val Loss: 17.8523\n",
      "Epoch [197/1000] Loss: 13.1449 Val Loss: 17.8179\n",
      "Epoch [198/1000] Loss: 13.0888 Val Loss: 17.7438\n",
      "Epoch [199/1000] Loss: 13.0521 Val Loss: 17.8315\n",
      "Epoch [200/1000] Loss: 13.0319 Val Loss: 17.9260\n",
      "Epoch [201/1000] Loss: 12.9894 Val Loss: 17.7895\n",
      "Epoch [202/1000] Loss: 12.9526 Val Loss: 17.7455\n",
      "Epoch [203/1000] Loss: 12.9547 Val Loss: 17.8660\n",
      "Epoch [204/1000] Loss: 12.9282 Val Loss: 17.7081\n",
      "Epoch [205/1000] Loss: 12.9538 Val Loss: 17.7051\n",
      "Epoch [206/1000] Loss: 12.9081 Val Loss: 17.8697\n",
      "Epoch [207/1000] Loss: 12.8884 Val Loss: 17.8095\n",
      "Epoch [208/1000] Loss: 12.8060 Val Loss: 17.7366\n",
      "Epoch [209/1000] Loss: 12.8446 Val Loss: 17.7848\n",
      "Epoch [210/1000] Loss: 12.8295 Val Loss: 17.7346\n",
      "Epoch [211/1000] Loss: 12.7521 Val Loss: 17.6869\n",
      "Epoch [212/1000] Loss: 12.7258 Val Loss: 17.7440\n",
      "Epoch [213/1000] Loss: 12.7070 Val Loss: 17.6496\n",
      "Epoch [214/1000] Loss: 12.7144 Val Loss: 17.7008\n",
      "Epoch [215/1000] Loss: 12.6834 Val Loss: 17.6990\n",
      "Epoch [216/1000] Loss: 12.6268 Val Loss: 17.7077\n",
      "Epoch [217/1000] Loss: 12.6362 Val Loss: 17.6661\n",
      "Epoch [218/1000] Loss: 12.6076 Val Loss: 17.5675\n",
      "Epoch [219/1000] Loss: 12.6091 Val Loss: 17.6210\n",
      "Epoch [220/1000] Loss: 12.5648 Val Loss: 17.6657\n",
      "Epoch [221/1000] Loss: 12.5836 Val Loss: 17.6612\n",
      "Epoch [222/1000] Loss: 12.5220 Val Loss: 17.6997\n",
      "Epoch [223/1000] Loss: 12.5231 Val Loss: 17.6174\n",
      "Epoch [224/1000] Loss: 12.4737 Val Loss: 17.6922\n",
      "Epoch [225/1000] Loss: 12.4876 Val Loss: 17.5795\n",
      "Epoch [226/1000] Loss: 12.4500 Val Loss: 17.7325\n",
      "Epoch [227/1000] Loss: 12.4893 Val Loss: 17.5938\n",
      "Epoch [228/1000] Loss: 12.3483 Val Loss: 17.7407\n",
      "Epoch [229/1000] Loss: 12.4492 Val Loss: 17.5395\n",
      "Epoch [230/1000] Loss: 12.3632 Val Loss: 17.5200\n",
      "Epoch [231/1000] Loss: 12.2978 Val Loss: 17.6215\n",
      "Epoch [232/1000] Loss: 12.3390 Val Loss: 17.6344\n",
      "Epoch [233/1000] Loss: 12.3209 Val Loss: 17.4831\n",
      "Epoch [234/1000] Loss: 12.2408 Val Loss: 17.5270\n",
      "Epoch [235/1000] Loss: 12.2428 Val Loss: 17.5243\n",
      "Epoch [236/1000] Loss: 12.2199 Val Loss: 17.5238\n",
      "Epoch [237/1000] Loss: 12.2588 Val Loss: 17.6175\n",
      "Epoch [238/1000] Loss: 12.2179 Val Loss: 17.4762\n",
      "Epoch [239/1000] Loss: 12.1830 Val Loss: 17.5959\n",
      "Epoch [240/1000] Loss: 12.1175 Val Loss: 17.4161\n",
      "Epoch [241/1000] Loss: 12.1482 Val Loss: 17.4475\n",
      "Epoch [242/1000] Loss: 12.0820 Val Loss: 17.5464\n",
      "Epoch [243/1000] Loss: 12.0280 Val Loss: 17.5425\n",
      "Epoch [244/1000] Loss: 12.0220 Val Loss: 17.5157\n",
      "Epoch [245/1000] Loss: 12.0222 Val Loss: 17.4866\n",
      "Epoch [246/1000] Loss: 12.0203 Val Loss: 17.4662\n",
      "Epoch [247/1000] Loss: 12.0574 Val Loss: 17.4862\n",
      "Epoch [248/1000] Loss: 12.0025 Val Loss: 17.4129\n",
      "Epoch [249/1000] Loss: 12.0032 Val Loss: 17.3869\n",
      "Epoch [250/1000] Loss: 11.9342 Val Loss: 17.4630\n",
      "Epoch [251/1000] Loss: 11.9347 Val Loss: 17.4158\n",
      "Epoch [252/1000] Loss: 11.8544 Val Loss: 17.4393\n",
      "Epoch [253/1000] Loss: 11.8772 Val Loss: 17.3740\n",
      "Epoch [254/1000] Loss: 11.8493 Val Loss: 17.4093\n",
      "Epoch [255/1000] Loss: 11.8710 Val Loss: 17.4768\n",
      "Epoch [256/1000] Loss: 11.8580 Val Loss: 17.4129\n",
      "Epoch [257/1000] Loss: 11.8302 Val Loss: 17.4739\n",
      "Epoch [258/1000] Loss: 11.7750 Val Loss: 17.4272\n",
      "Epoch [259/1000] Loss: 11.8487 Val Loss: 17.4016\n",
      "Epoch [260/1000] Loss: 11.7514 Val Loss: 17.5116\n",
      "Epoch [261/1000] Loss: 11.7256 Val Loss: 17.4070\n",
      "Epoch [262/1000] Loss: 11.7143 Val Loss: 17.3848\n",
      "Epoch [263/1000] Loss: 11.6807 Val Loss: 17.3786\n",
      "Epoch [264/1000] Loss: 11.6678 Val Loss: 17.4264\n",
      "Epoch [265/1000] Loss: 11.6879 Val Loss: 17.4559\n",
      "Epoch [266/1000] Loss: 11.6441 Val Loss: 17.3234\n",
      "Epoch [267/1000] Loss: 11.6379 Val Loss: 17.3285\n",
      "Epoch [268/1000] Loss: 11.5605 Val Loss: 17.3315\n",
      "Epoch [269/1000] Loss: 11.5991 Val Loss: 17.3012\n",
      "Epoch [270/1000] Loss: 11.5661 Val Loss: 17.3437\n",
      "Epoch [271/1000] Loss: 11.5740 Val Loss: 17.4512\n",
      "Epoch [272/1000] Loss: 11.5477 Val Loss: 17.3300\n",
      "Epoch [273/1000] Loss: 11.4767 Val Loss: 17.2874\n",
      "Epoch [274/1000] Loss: 11.4994 Val Loss: 17.2444\n",
      "Epoch [275/1000] Loss: 11.5067 Val Loss: 17.2088\n",
      "Epoch [276/1000] Loss: 11.3981 Val Loss: 17.2497\n",
      "Epoch [277/1000] Loss: 11.4398 Val Loss: 17.3008\n",
      "Epoch [278/1000] Loss: 11.4280 Val Loss: 17.2179\n",
      "Epoch [279/1000] Loss: 11.3628 Val Loss: 17.2802\n",
      "Epoch [280/1000] Loss: 11.3473 Val Loss: 17.2647\n",
      "Epoch [281/1000] Loss: 11.3874 Val Loss: 17.3174\n",
      "Epoch [282/1000] Loss: 11.3917 Val Loss: 17.2989\n",
      "Epoch [283/1000] Loss: 11.2424 Val Loss: 17.2411\n",
      "Epoch [284/1000] Loss: 11.2644 Val Loss: 17.3091\n",
      "Epoch [285/1000] Loss: 11.2867 Val Loss: 17.2464\n",
      "Epoch [286/1000] Loss: 11.2352 Val Loss: 17.2053\n",
      "Epoch [287/1000] Loss: 11.2542 Val Loss: 17.3262\n",
      "Epoch [288/1000] Loss: 11.2455 Val Loss: 17.1806\n",
      "Epoch [289/1000] Loss: 11.2825 Val Loss: 17.3062\n",
      "Epoch [290/1000] Loss: 11.1976 Val Loss: 17.2595\n",
      "Epoch [291/1000] Loss: 11.1948 Val Loss: 17.1629\n",
      "Epoch [292/1000] Loss: 11.1721 Val Loss: 17.2732\n",
      "Epoch [293/1000] Loss: 11.1787 Val Loss: 17.2115\n",
      "Epoch [294/1000] Loss: 11.2137 Val Loss: 17.2606\n",
      "Epoch [295/1000] Loss: 11.1397 Val Loss: 17.2483\n",
      "Epoch [296/1000] Loss: 11.0784 Val Loss: 17.1811\n",
      "Epoch [297/1000] Loss: 11.0708 Val Loss: 17.2060\n",
      "Epoch [298/1000] Loss: 11.0466 Val Loss: 17.2784\n",
      "Epoch [299/1000] Loss: 11.0950 Val Loss: 17.1856\n",
      "Epoch [300/1000] Loss: 11.0248 Val Loss: 17.2040\n",
      "Epoch [301/1000] Loss: 11.0477 Val Loss: 17.2060\n",
      "Epoch [302/1000] Loss: 11.0016 Val Loss: 17.2713\n",
      "Epoch [303/1000] Loss: 10.9797 Val Loss: 17.2060\n",
      "Epoch [304/1000] Loss: 10.9521 Val Loss: 17.2351\n",
      "Epoch [305/1000] Loss: 10.9284 Val Loss: 17.1706\n",
      "Epoch [306/1000] Loss: 10.9054 Val Loss: 17.3124\n",
      "Epoch [307/1000] Loss: 10.8671 Val Loss: 17.1333\n",
      "Epoch [308/1000] Loss: 10.9240 Val Loss: 17.1933\n",
      "Epoch [309/1000] Loss: 10.9206 Val Loss: 17.1964\n",
      "Epoch [310/1000] Loss: 10.8197 Val Loss: 17.1727\n",
      "Epoch [311/1000] Loss: 10.8164 Val Loss: 17.1360\n",
      "Epoch [312/1000] Loss: 10.8683 Val Loss: 17.1580\n",
      "Epoch [313/1000] Loss: 10.8313 Val Loss: 17.0695\n",
      "Epoch [314/1000] Loss: 10.7647 Val Loss: 17.1312\n",
      "Epoch [315/1000] Loss: 10.7431 Val Loss: 17.0799\n",
      "Epoch [316/1000] Loss: 10.7707 Val Loss: 17.1388\n",
      "Epoch [317/1000] Loss: 10.7775 Val Loss: 17.0805\n",
      "Epoch [318/1000] Loss: 10.7369 Val Loss: 17.0698\n",
      "Epoch [319/1000] Loss: 10.7146 Val Loss: 17.1082\n",
      "Epoch [320/1000] Loss: 10.6772 Val Loss: 17.0718\n",
      "Epoch [321/1000] Loss: 10.7008 Val Loss: 17.0981\n",
      "Epoch [322/1000] Loss: 10.7008 Val Loss: 17.0400\n",
      "Epoch [323/1000] Loss: 10.6462 Val Loss: 17.1578\n",
      "Epoch [324/1000] Loss: 10.6381 Val Loss: 17.0405\n",
      "Epoch [325/1000] Loss: 10.6095 Val Loss: 17.0917\n",
      "Epoch [326/1000] Loss: 10.6156 Val Loss: 17.1475\n",
      "Epoch [327/1000] Loss: 10.6100 Val Loss: 17.1013\n",
      "Epoch [328/1000] Loss: 10.5927 Val Loss: 17.0921\n",
      "Epoch [329/1000] Loss: 10.5216 Val Loss: 16.9912\n",
      "Epoch [330/1000] Loss: 10.5256 Val Loss: 17.0665\n",
      "Epoch [331/1000] Loss: 10.5201 Val Loss: 17.0667\n",
      "Epoch [332/1000] Loss: 10.5047 Val Loss: 17.1039\n",
      "Epoch [333/1000] Loss: 10.5565 Val Loss: 17.0538\n",
      "Epoch [334/1000] Loss: 10.5058 Val Loss: 17.0613\n",
      "Epoch [335/1000] Loss: 10.5293 Val Loss: 17.0386\n",
      "Epoch [336/1000] Loss: 10.4621 Val Loss: 17.0059\n",
      "Epoch [337/1000] Loss: 10.4417 Val Loss: 16.9277\n",
      "Epoch [338/1000] Loss: 10.4263 Val Loss: 17.0393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [339/1000] Loss: 10.4231 Val Loss: 16.9439\n",
      "Epoch [340/1000] Loss: 10.4103 Val Loss: 17.0300\n",
      "Epoch [341/1000] Loss: 10.3830 Val Loss: 17.0154\n",
      "Epoch [342/1000] Loss: 10.4048 Val Loss: 16.9744\n",
      "Epoch [343/1000] Loss: 10.3937 Val Loss: 16.9793\n",
      "Epoch [344/1000] Loss: 10.3180 Val Loss: 17.0293\n",
      "Epoch [345/1000] Loss: 10.3183 Val Loss: 17.0257\n",
      "Epoch [346/1000] Loss: 10.2919 Val Loss: 17.0245\n",
      "Epoch [347/1000] Loss: 10.2997 Val Loss: 16.9978\n",
      "Epoch [348/1000] Loss: 10.2593 Val Loss: 16.9626\n",
      "Epoch [349/1000] Loss: 10.2792 Val Loss: 16.9258\n",
      "Epoch [350/1000] Loss: 10.2840 Val Loss: 17.0387\n",
      "Epoch [351/1000] Loss: 10.1974 Val Loss: 16.9323\n",
      "Epoch [352/1000] Loss: 10.1854 Val Loss: 17.0010\n",
      "Epoch [353/1000] Loss: 10.2288 Val Loss: 16.8756\n",
      "Epoch [354/1000] Loss: 10.2081 Val Loss: 16.9342\n",
      "Epoch [355/1000] Loss: 10.1842 Val Loss: 16.9146\n",
      "Epoch [356/1000] Loss: 10.1532 Val Loss: 16.9247\n",
      "Epoch [357/1000] Loss: 10.1378 Val Loss: 16.9850\n",
      "Epoch [358/1000] Loss: 10.1308 Val Loss: 16.8964\n",
      "Epoch [359/1000] Loss: 10.0751 Val Loss: 16.9242\n",
      "Epoch [360/1000] Loss: 10.1425 Val Loss: 16.8869\n",
      "Epoch [361/1000] Loss: 10.1422 Val Loss: 16.9806\n",
      "Epoch [362/1000] Loss: 10.0875 Val Loss: 16.9923\n",
      "Epoch [363/1000] Loss: 10.0879 Val Loss: 17.0297\n",
      "Epoch [364/1000] Loss: 10.0389 Val Loss: 16.9054\n",
      "Epoch [365/1000] Loss: 10.0354 Val Loss: 16.9406\n",
      "Epoch [366/1000] Loss: 10.0733 Val Loss: 16.9374\n",
      "Epoch [367/1000] Loss: 10.0033 Val Loss: 16.9726\n",
      "Epoch [368/1000] Loss: 10.0046 Val Loss: 16.9132\n",
      "Epoch [369/1000] Loss: 10.0022 Val Loss: 17.0039\n",
      "Epoch [370/1000] Loss: 9.9899 Val Loss: 16.9082\n",
      "Epoch [371/1000] Loss: 9.9399 Val Loss: 16.9072\n",
      "Epoch [372/1000] Loss: 9.9515 Val Loss: 16.9403\n",
      "Epoch [373/1000] Loss: 9.9347 Val Loss: 16.9337\n",
      "Epoch [374/1000] Loss: 9.9543 Val Loss: 16.9164\n",
      "Epoch [375/1000] Loss: 9.9319 Val Loss: 16.9232\n",
      "Epoch [376/1000] Loss: 9.8607 Val Loss: 16.9295\n",
      "Epoch [377/1000] Loss: 9.8614 Val Loss: 16.8653\n",
      "Epoch [378/1000] Loss: 9.9033 Val Loss: 16.8343\n",
      "Epoch [379/1000] Loss: 9.8408 Val Loss: 16.9412\n",
      "Epoch [380/1000] Loss: 9.8033 Val Loss: 16.8249\n",
      "Epoch [381/1000] Loss: 9.8723 Val Loss: 16.8245\n",
      "Epoch [382/1000] Loss: 9.8877 Val Loss: 16.8236\n",
      "Epoch [383/1000] Loss: 9.8929 Val Loss: 16.9123\n",
      "Epoch [384/1000] Loss: 9.7953 Val Loss: 16.8221\n",
      "Epoch [385/1000] Loss: 9.8187 Val Loss: 16.8078\n",
      "Epoch [386/1000] Loss: 9.7434 Val Loss: 16.8635\n",
      "Epoch [387/1000] Loss: 9.7605 Val Loss: 16.8907\n",
      "Epoch [388/1000] Loss: 9.7420 Val Loss: 16.9147\n",
      "Epoch [389/1000] Loss: 9.7354 Val Loss: 16.8840\n",
      "Epoch [390/1000] Loss: 9.7476 Val Loss: 16.8385\n",
      "Epoch [391/1000] Loss: 9.7636 Val Loss: 16.9239\n",
      "Epoch [392/1000] Loss: 9.7109 Val Loss: 16.9180\n",
      "Epoch [393/1000] Loss: 9.6564 Val Loss: 16.8780\n",
      "Epoch [394/1000] Loss: 9.6236 Val Loss: 16.9038\n",
      "Epoch [395/1000] Loss: 9.6767 Val Loss: 16.8485\n",
      "Epoch [396/1000] Loss: 9.7345 Val Loss: 16.8210\n",
      "Epoch [397/1000] Loss: 9.6169 Val Loss: 16.8810\n",
      "Epoch [398/1000] Loss: 9.6448 Val Loss: 16.8775\n",
      "Epoch [399/1000] Loss: 9.5864 Val Loss: 16.9851\n",
      "Epoch [400/1000] Loss: 9.6259 Val Loss: 16.7493\n",
      "Epoch [401/1000] Loss: 9.6374 Val Loss: 16.8318\n",
      "Epoch [402/1000] Loss: 9.5786 Val Loss: 16.8026\n",
      "Epoch [403/1000] Loss: 9.5973 Val Loss: 16.8291\n",
      "Epoch [404/1000] Loss: 9.5763 Val Loss: 16.7370\n",
      "Epoch [405/1000] Loss: 9.5051 Val Loss: 16.8256\n",
      "Epoch [406/1000] Loss: 9.5517 Val Loss: 16.8817\n",
      "Epoch [407/1000] Loss: 9.5316 Val Loss: 16.8248\n",
      "Epoch [408/1000] Loss: 9.5013 Val Loss: 16.7847\n",
      "Epoch [409/1000] Loss: 9.5340 Val Loss: 16.7473\n",
      "Epoch [410/1000] Loss: 9.5113 Val Loss: 16.6886\n",
      "Epoch [411/1000] Loss: 9.5195 Val Loss: 16.7779\n",
      "Epoch [412/1000] Loss: 9.4406 Val Loss: 16.7747\n",
      "Epoch [413/1000] Loss: 9.4073 Val Loss: 16.7345\n",
      "Epoch [414/1000] Loss: 9.4910 Val Loss: 16.7967\n",
      "Epoch [415/1000] Loss: 9.4964 Val Loss: 16.7142\n",
      "Epoch [416/1000] Loss: 9.4085 Val Loss: 16.8087\n",
      "Epoch [417/1000] Loss: 9.3644 Val Loss: 16.7704\n",
      "Epoch [418/1000] Loss: 9.3899 Val Loss: 16.8175\n",
      "Epoch [419/1000] Loss: 9.3719 Val Loss: 16.7565\n",
      "Epoch [420/1000] Loss: 9.4093 Val Loss: 16.7827\n",
      "Epoch [421/1000] Loss: 9.4537 Val Loss: 16.7259\n",
      "Epoch [422/1000] Loss: 9.3564 Val Loss: 16.7569\n",
      "Epoch [423/1000] Loss: 9.3763 Val Loss: 16.7538\n",
      "Epoch [424/1000] Loss: 9.3152 Val Loss: 16.7324\n",
      "Epoch [425/1000] Loss: 9.3696 Val Loss: 16.7267\n",
      "Epoch [426/1000] Loss: 9.3260 Val Loss: 16.6346\n",
      "Epoch [427/1000] Loss: 9.3519 Val Loss: 16.7265\n",
      "Epoch [428/1000] Loss: 9.2975 Val Loss: 16.7047\n",
      "Epoch [429/1000] Loss: 9.2916 Val Loss: 16.7420\n",
      "Epoch [430/1000] Loss: 9.3281 Val Loss: 16.6826\n",
      "Epoch [431/1000] Loss: 9.2363 Val Loss: 16.7200\n",
      "Epoch [432/1000] Loss: 9.2902 Val Loss: 16.6790\n",
      "Epoch [433/1000] Loss: 9.3084 Val Loss: 16.6752\n",
      "Epoch [434/1000] Loss: 9.1779 Val Loss: 16.6245\n",
      "Epoch [435/1000] Loss: 9.1822 Val Loss: 16.7009\n",
      "Epoch [436/1000] Loss: 9.2230 Val Loss: 16.6952\n",
      "Epoch [437/1000] Loss: 9.2450 Val Loss: 16.7001\n",
      "Epoch [438/1000] Loss: 9.1744 Val Loss: 16.7037\n",
      "Epoch [439/1000] Loss: 9.1950 Val Loss: 16.6598\n",
      "Epoch [440/1000] Loss: 9.2239 Val Loss: 16.6228\n",
      "Epoch [441/1000] Loss: 9.1555 Val Loss: 16.7321\n",
      "Epoch [442/1000] Loss: 9.1309 Val Loss: 16.7371\n",
      "Epoch [443/1000] Loss: 9.1500 Val Loss: 16.6170\n",
      "Epoch [444/1000] Loss: 9.1110 Val Loss: 16.6409\n",
      "Epoch [445/1000] Loss: 9.1417 Val Loss: 16.7582\n",
      "Epoch [446/1000] Loss: 9.1447 Val Loss: 16.6596\n",
      "Epoch [447/1000] Loss: 9.1194 Val Loss: 16.6590\n",
      "Epoch [448/1000] Loss: 9.1301 Val Loss: 16.7108\n",
      "Epoch [449/1000] Loss: 9.1633 Val Loss: 16.6682\n",
      "Epoch [450/1000] Loss: 9.0734 Val Loss: 16.6142\n",
      "Epoch [451/1000] Loss: 9.0694 Val Loss: 16.7651\n",
      "Epoch [452/1000] Loss: 9.0019 Val Loss: 16.6827\n",
      "Epoch [453/1000] Loss: 9.0645 Val Loss: 16.6679\n",
      "Epoch [454/1000] Loss: 9.0251 Val Loss: 16.6616\n",
      "Epoch [455/1000] Loss: 9.0922 Val Loss: 16.7173\n",
      "Epoch [456/1000] Loss: 9.0316 Val Loss: 16.6625\n",
      "Epoch [457/1000] Loss: 9.0430 Val Loss: 16.6422\n",
      "Epoch [458/1000] Loss: 9.0276 Val Loss: 16.6446\n",
      "Epoch [459/1000] Loss: 8.9682 Val Loss: 16.6589\n",
      "Epoch [460/1000] Loss: 8.9982 Val Loss: 16.6575\n",
      "Epoch [461/1000] Loss: 8.9948 Val Loss: 16.6402\n",
      "Epoch [462/1000] Loss: 8.9889 Val Loss: 16.6192\n",
      "Epoch [463/1000] Loss: 8.9672 Val Loss: 16.6468\n",
      "Epoch [464/1000] Loss: 8.9169 Val Loss: 16.6923\n",
      "Epoch [465/1000] Loss: 8.9816 Val Loss: 16.6296\n",
      "Epoch [466/1000] Loss: 8.9778 Val Loss: 16.6556\n",
      "Epoch [467/1000] Loss: 8.8817 Val Loss: 16.6458\n",
      "Epoch [468/1000] Loss: 8.9653 Val Loss: 16.6401\n",
      "Epoch [469/1000] Loss: 8.9130 Val Loss: 16.6040\n",
      "Epoch [470/1000] Loss: 8.8446 Val Loss: 16.6370\n",
      "Epoch [471/1000] Loss: 8.8985 Val Loss: 16.6543\n",
      "Epoch [472/1000] Loss: 8.8395 Val Loss: 16.6142\n",
      "Epoch [473/1000] Loss: 8.8647 Val Loss: 16.5822\n",
      "Epoch [474/1000] Loss: 8.8844 Val Loss: 16.5995\n",
      "Epoch [475/1000] Loss: 8.8285 Val Loss: 16.6145\n",
      "Epoch [476/1000] Loss: 8.8452 Val Loss: 16.5924\n",
      "Epoch [477/1000] Loss: 8.8064 Val Loss: 16.7307\n",
      "Epoch [478/1000] Loss: 8.8658 Val Loss: 16.6390\n",
      "Epoch [479/1000] Loss: 8.7780 Val Loss: 16.6368\n",
      "Epoch [480/1000] Loss: 8.7661 Val Loss: 16.6375\n",
      "Epoch [481/1000] Loss: 8.8183 Val Loss: 16.5650\n",
      "Epoch [482/1000] Loss: 8.7679 Val Loss: 16.6189\n",
      "Epoch [483/1000] Loss: 8.8002 Val Loss: 16.6135\n",
      "Epoch [484/1000] Loss: 8.7986 Val Loss: 16.6791\n",
      "Epoch [485/1000] Loss: 8.7597 Val Loss: 16.5644\n",
      "Epoch [486/1000] Loss: 8.7653 Val Loss: 16.5530\n",
      "Epoch [487/1000] Loss: 8.7266 Val Loss: 16.5475\n",
      "Epoch [488/1000] Loss: 8.7704 Val Loss: 16.5861\n",
      "Epoch [489/1000] Loss: 8.7114 Val Loss: 16.5621\n",
      "Epoch [490/1000] Loss: 8.6678 Val Loss: 16.5767\n",
      "Epoch [491/1000] Loss: 8.6824 Val Loss: 16.6422\n",
      "Epoch [492/1000] Loss: 8.6510 Val Loss: 16.6075\n",
      "Epoch [493/1000] Loss: 8.6588 Val Loss: 16.5678\n",
      "Epoch [494/1000] Loss: 8.6599 Val Loss: 16.6235\n",
      "Epoch [495/1000] Loss: 8.6845 Val Loss: 16.5666\n",
      "Epoch [496/1000] Loss: 8.6857 Val Loss: 16.5910\n",
      "Epoch [497/1000] Loss: 8.6822 Val Loss: 16.5436\n",
      "Epoch [498/1000] Loss: 8.5738 Val Loss: 16.5763\n",
      "Epoch [499/1000] Loss: 8.6873 Val Loss: 16.5242\n",
      "Epoch [500/1000] Loss: 8.6396 Val Loss: 16.5922\n",
      "Epoch [501/1000] Loss: 8.6342 Val Loss: 16.5383\n",
      "Epoch [502/1000] Loss: 8.5863 Val Loss: 16.4769\n",
      "Epoch [503/1000] Loss: 8.6527 Val Loss: 16.5259\n",
      "Epoch [504/1000] Loss: 8.5521 Val Loss: 16.5131\n",
      "Epoch [505/1000] Loss: 8.6143 Val Loss: 16.5301\n",
      "Epoch [506/1000] Loss: 8.6164 Val Loss: 16.4956\n",
      "Epoch [507/1000] Loss: 8.5761 Val Loss: 16.5016\n",
      "Epoch [508/1000] Loss: 8.5527 Val Loss: 16.5025\n",
      "Epoch [509/1000] Loss: 8.5935 Val Loss: 16.5453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [510/1000] Loss: 8.5853 Val Loss: 16.5264\n",
      "Epoch [511/1000] Loss: 8.5881 Val Loss: 16.5204\n",
      "Epoch [512/1000] Loss: 8.5670 Val Loss: 16.5305\n",
      "Epoch [513/1000] Loss: 8.5394 Val Loss: 16.6086\n",
      "Epoch [514/1000] Loss: 8.5065 Val Loss: 16.5776\n",
      "Epoch [515/1000] Loss: 8.4861 Val Loss: 16.5347\n",
      "Epoch [516/1000] Loss: 8.4915 Val Loss: 16.4889\n",
      "Epoch [517/1000] Loss: 8.5095 Val Loss: 16.4919\n",
      "Epoch [518/1000] Loss: 8.5357 Val Loss: 16.4967\n",
      "Epoch [519/1000] Loss: 8.4828 Val Loss: 16.5130\n",
      "Epoch [520/1000] Loss: 8.4782 Val Loss: 16.5161\n",
      "Epoch [521/1000] Loss: 8.4679 Val Loss: 16.5913\n",
      "Epoch [522/1000] Loss: 8.4751 Val Loss: 16.5289\n",
      "Epoch [523/1000] Loss: 8.5034 Val Loss: 16.4712\n",
      "Epoch [524/1000] Loss: 8.4856 Val Loss: 16.5309\n",
      "Epoch [525/1000] Loss: 8.4317 Val Loss: 16.5086\n",
      "Epoch [526/1000] Loss: 8.4490 Val Loss: 16.5255\n",
      "Epoch [527/1000] Loss: 8.3892 Val Loss: 16.5684\n",
      "Epoch [528/1000] Loss: 8.3796 Val Loss: 16.5204\n",
      "Epoch [529/1000] Loss: 8.4482 Val Loss: 16.4633\n",
      "Epoch [530/1000] Loss: 8.3996 Val Loss: 16.4810\n",
      "Epoch [531/1000] Loss: 8.3660 Val Loss: 16.4980\n",
      "Epoch [532/1000] Loss: 8.3606 Val Loss: 16.5225\n",
      "Epoch [533/1000] Loss: 8.3314 Val Loss: 16.5261\n",
      "Epoch [534/1000] Loss: 8.3521 Val Loss: 16.5496\n",
      "Epoch [535/1000] Loss: 8.3271 Val Loss: 16.5401\n",
      "Epoch [536/1000] Loss: 8.3158 Val Loss: 16.5091\n",
      "Epoch [537/1000] Loss: 8.3112 Val Loss: 16.5395\n",
      "Epoch [538/1000] Loss: 8.3627 Val Loss: 16.4645\n",
      "Epoch [539/1000] Loss: 8.3645 Val Loss: 16.4344\n",
      "Epoch [540/1000] Loss: 8.3009 Val Loss: 16.4598\n",
      "Epoch [541/1000] Loss: 8.3104 Val Loss: 16.5431\n",
      "Epoch [542/1000] Loss: 8.3618 Val Loss: 16.4628\n",
      "Epoch [543/1000] Loss: 8.3394 Val Loss: 16.4742\n",
      "Epoch [544/1000] Loss: 8.2589 Val Loss: 16.4912\n",
      "Epoch [545/1000] Loss: 8.2510 Val Loss: 16.5210\n",
      "Epoch [546/1000] Loss: 8.2673 Val Loss: 16.4593\n",
      "Epoch [547/1000] Loss: 8.2304 Val Loss: 16.4720\n",
      "Epoch [548/1000] Loss: 8.2567 Val Loss: 16.4166\n",
      "Epoch [549/1000] Loss: 8.2990 Val Loss: 16.4323\n",
      "Epoch [550/1000] Loss: 8.2242 Val Loss: 16.4982\n",
      "Epoch [551/1000] Loss: 8.2642 Val Loss: 16.4324\n",
      "Epoch [552/1000] Loss: 8.2575 Val Loss: 16.5244\n",
      "Epoch [553/1000] Loss: 8.2439 Val Loss: 16.4691\n",
      "Epoch [554/1000] Loss: 8.2474 Val Loss: 16.4363\n",
      "Epoch [555/1000] Loss: 8.2082 Val Loss: 16.4385\n",
      "Epoch [556/1000] Loss: 8.1656 Val Loss: 16.4215\n",
      "Epoch [557/1000] Loss: 8.1383 Val Loss: 16.4850\n",
      "Epoch [558/1000] Loss: 8.1709 Val Loss: 16.4138\n",
      "Epoch [559/1000] Loss: 8.2042 Val Loss: 16.4362\n",
      "Epoch [560/1000] Loss: 8.1921 Val Loss: 16.4393\n",
      "Epoch [561/1000] Loss: 8.1852 Val Loss: 16.4280\n",
      "Epoch [562/1000] Loss: 8.1240 Val Loss: 16.3952\n",
      "Epoch [563/1000] Loss: 8.1924 Val Loss: 16.4345\n",
      "Epoch [564/1000] Loss: 8.1144 Val Loss: 16.4076\n",
      "Epoch [565/1000] Loss: 8.1479 Val Loss: 16.4341\n",
      "Epoch [566/1000] Loss: 8.1538 Val Loss: 16.4373\n",
      "Epoch [567/1000] Loss: 8.1790 Val Loss: 16.3931\n",
      "Epoch [568/1000] Loss: 8.1285 Val Loss: 16.3199\n",
      "Epoch [569/1000] Loss: 8.1404 Val Loss: 16.4677\n",
      "Epoch [570/1000] Loss: 8.1094 Val Loss: 16.4567\n",
      "Epoch [571/1000] Loss: 8.1126 Val Loss: 16.3980\n",
      "Epoch [572/1000] Loss: 8.0903 Val Loss: 16.4336\n",
      "Epoch [573/1000] Loss: 8.1441 Val Loss: 16.4117\n",
      "Epoch [574/1000] Loss: 8.0625 Val Loss: 16.4664\n",
      "Epoch [575/1000] Loss: 8.0668 Val Loss: 16.3990\n",
      "Epoch [576/1000] Loss: 8.1111 Val Loss: 16.4219\n",
      "Epoch [577/1000] Loss: 8.0422 Val Loss: 16.4533\n",
      "Epoch [578/1000] Loss: 8.0250 Val Loss: 16.4620\n",
      "Epoch [579/1000] Loss: 8.0511 Val Loss: 16.4704\n",
      "Epoch [580/1000] Loss: 8.0880 Val Loss: 16.3950\n",
      "Epoch [581/1000] Loss: 8.0132 Val Loss: 16.3901\n",
      "Epoch [582/1000] Loss: 8.0487 Val Loss: 16.4238\n",
      "Epoch [583/1000] Loss: 8.0408 Val Loss: 16.4570\n",
      "Epoch [584/1000] Loss: 7.9767 Val Loss: 16.4387\n",
      "Epoch [585/1000] Loss: 8.0529 Val Loss: 16.4430\n",
      "Epoch [586/1000] Loss: 8.0666 Val Loss: 16.4297\n",
      "Epoch [587/1000] Loss: 7.9935 Val Loss: 16.5049\n",
      "Epoch [588/1000] Loss: 8.0078 Val Loss: 16.4728\n",
      "Epoch [589/1000] Loss: 7.9752 Val Loss: 16.3827\n",
      "Epoch [590/1000] Loss: 8.0111 Val Loss: 16.4182\n",
      "Epoch [591/1000] Loss: 7.9862 Val Loss: 16.3907\n",
      "Epoch [592/1000] Loss: 8.0030 Val Loss: 16.4084\n",
      "Epoch [593/1000] Loss: 7.9990 Val Loss: 16.4148\n",
      "Epoch [594/1000] Loss: 8.0123 Val Loss: 16.3116\n",
      "Epoch [595/1000] Loss: 7.9302 Val Loss: 16.3372\n",
      "Epoch [596/1000] Loss: 7.9431 Val Loss: 16.3734\n",
      "Epoch [597/1000] Loss: 7.9194 Val Loss: 16.3714\n",
      "Epoch [598/1000] Loss: 7.9549 Val Loss: 16.3522\n",
      "Epoch [599/1000] Loss: 7.8833 Val Loss: 16.4132\n",
      "Epoch [600/1000] Loss: 7.9588 Val Loss: 16.3460\n",
      "Epoch [601/1000] Loss: 7.9404 Val Loss: 16.3364\n",
      "Epoch [602/1000] Loss: 7.8698 Val Loss: 16.4505\n",
      "Epoch [603/1000] Loss: 7.8917 Val Loss: 16.3498\n",
      "Epoch [604/1000] Loss: 7.9413 Val Loss: 16.3853\n",
      "Epoch [605/1000] Loss: 7.9232 Val Loss: 16.5246\n",
      "Epoch [606/1000] Loss: 7.8857 Val Loss: 16.3310\n",
      "Epoch [607/1000] Loss: 7.8508 Val Loss: 16.3682\n",
      "Epoch [608/1000] Loss: 7.8712 Val Loss: 16.3928\n",
      "Epoch [609/1000] Loss: 7.8412 Val Loss: 16.3544\n",
      "Epoch [610/1000] Loss: 7.8916 Val Loss: 16.3632\n",
      "Epoch [611/1000] Loss: 7.8755 Val Loss: 16.3490\n",
      "Epoch [612/1000] Loss: 7.8828 Val Loss: 16.2973\n",
      "Epoch [613/1000] Loss: 7.8714 Val Loss: 16.4101\n",
      "Epoch [614/1000] Loss: 7.8148 Val Loss: 16.3219\n",
      "Epoch [615/1000] Loss: 7.8896 Val Loss: 16.3298\n",
      "Epoch [616/1000] Loss: 7.8139 Val Loss: 16.3736\n",
      "Epoch [617/1000] Loss: 7.8200 Val Loss: 16.4340\n",
      "Epoch [618/1000] Loss: 7.7806 Val Loss: 16.3764\n",
      "Epoch [619/1000] Loss: 7.8060 Val Loss: 16.4429\n",
      "Epoch [620/1000] Loss: 7.8923 Val Loss: 16.4133\n",
      "Epoch [621/1000] Loss: 7.8098 Val Loss: 16.4325\n",
      "Epoch [622/1000] Loss: 7.8317 Val Loss: 16.3928\n",
      "Epoch [623/1000] Loss: 7.7538 Val Loss: 16.3952\n",
      "Epoch [624/1000] Loss: 7.7785 Val Loss: 16.3721\n",
      "Epoch [625/1000] Loss: 7.7717 Val Loss: 16.3322\n",
      "Epoch [626/1000] Loss: 7.7490 Val Loss: 16.3121\n",
      "Epoch [627/1000] Loss: 7.8096 Val Loss: 16.3949\n",
      "Epoch [628/1000] Loss: 7.7311 Val Loss: 16.3819\n",
      "Epoch [629/1000] Loss: 7.7769 Val Loss: 16.3695\n",
      "Epoch [630/1000] Loss: 7.7653 Val Loss: 16.3429\n",
      "Epoch [631/1000] Loss: 7.6868 Val Loss: 16.3375\n",
      "Epoch [632/1000] Loss: 7.7188 Val Loss: 16.3532\n",
      "Epoch [633/1000] Loss: 7.7293 Val Loss: 16.3569\n",
      "Epoch [634/1000] Loss: 7.7256 Val Loss: 16.3813\n",
      "Epoch [635/1000] Loss: 7.7498 Val Loss: 16.4007\n",
      "Epoch [636/1000] Loss: 7.7153 Val Loss: 16.3653\n",
      "Epoch [637/1000] Loss: 7.6900 Val Loss: 16.3323\n",
      "Epoch [638/1000] Loss: 7.7218 Val Loss: 16.4126\n",
      "Epoch [639/1000] Loss: 7.6890 Val Loss: 16.3061\n",
      "Epoch [640/1000] Loss: 7.7137 Val Loss: 16.3223\n",
      "Epoch [641/1000] Loss: 7.6371 Val Loss: 16.3152\n",
      "Epoch [642/1000] Loss: 7.6493 Val Loss: 16.3842\n",
      "Epoch [643/1000] Loss: 7.6399 Val Loss: 16.3775\n",
      "Epoch [644/1000] Loss: 7.6519 Val Loss: 16.3520\n",
      "Epoch [645/1000] Loss: 7.6706 Val Loss: 16.3411\n",
      "Epoch [646/1000] Loss: 7.6653 Val Loss: 16.3013\n",
      "Epoch [647/1000] Loss: 7.6307 Val Loss: 16.2777\n",
      "Epoch [648/1000] Loss: 7.7017 Val Loss: 16.3640\n",
      "Epoch [649/1000] Loss: 7.6578 Val Loss: 16.2932\n",
      "Epoch [650/1000] Loss: 7.6375 Val Loss: 16.3100\n",
      "Epoch [651/1000] Loss: 7.6809 Val Loss: 16.3119\n",
      "Epoch [652/1000] Loss: 7.6738 Val Loss: 16.2381\n",
      "Epoch [653/1000] Loss: 7.6253 Val Loss: 16.3344\n",
      "Epoch [654/1000] Loss: 7.6354 Val Loss: 16.3407\n",
      "Epoch [655/1000] Loss: 7.6239 Val Loss: 16.3128\n",
      "Epoch [656/1000] Loss: 7.6106 Val Loss: 16.3543\n",
      "Epoch [657/1000] Loss: 7.6187 Val Loss: 16.2735\n",
      "Epoch [658/1000] Loss: 7.6226 Val Loss: 16.3008\n",
      "Epoch [659/1000] Loss: 7.6080 Val Loss: 16.3294\n",
      "Epoch [660/1000] Loss: 7.5737 Val Loss: 16.3715\n",
      "Epoch [661/1000] Loss: 7.5331 Val Loss: 16.4132\n",
      "Epoch [662/1000] Loss: 7.5538 Val Loss: 16.3285\n",
      "Epoch [663/1000] Loss: 7.5539 Val Loss: 16.3048\n",
      "Epoch [664/1000] Loss: 7.5849 Val Loss: 16.3484\n",
      "Epoch [665/1000] Loss: 7.5950 Val Loss: 16.3070\n",
      "Epoch [666/1000] Loss: 7.5611 Val Loss: 16.3469\n",
      "Epoch [667/1000] Loss: 7.4767 Val Loss: 16.3706\n",
      "Epoch [668/1000] Loss: 7.5707 Val Loss: 16.2970\n",
      "Epoch [669/1000] Loss: 7.5327 Val Loss: 16.3543\n",
      "Epoch [670/1000] Loss: 7.5386 Val Loss: 16.3317\n",
      "Epoch [671/1000] Loss: 7.5441 Val Loss: 16.2740\n",
      "Epoch [672/1000] Loss: 7.5040 Val Loss: 16.3978\n",
      "Epoch [673/1000] Loss: 7.5158 Val Loss: 16.3266\n",
      "Epoch [674/1000] Loss: 7.5468 Val Loss: 16.3855\n",
      "Epoch [675/1000] Loss: 7.5332 Val Loss: 16.3688\n",
      "Epoch [676/1000] Loss: 7.5319 Val Loss: 16.3229\n",
      "Epoch [677/1000] Loss: 7.5577 Val Loss: 16.2819\n",
      "Epoch [678/1000] Loss: 7.4992 Val Loss: 16.3261\n",
      "Epoch [679/1000] Loss: 7.5030 Val Loss: 16.3500\n",
      "Epoch [680/1000] Loss: 7.4838 Val Loss: 16.3572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [681/1000] Loss: 7.4004 Val Loss: 16.3919\n",
      "Epoch [682/1000] Loss: 7.4933 Val Loss: 16.3039\n",
      "Epoch [683/1000] Loss: 7.4663 Val Loss: 16.3079\n",
      "Epoch [684/1000] Loss: 7.4649 Val Loss: 16.3355\n",
      "Epoch [685/1000] Loss: 7.5030 Val Loss: 16.3011\n",
      "Epoch [686/1000] Loss: 7.4647 Val Loss: 16.3232\n",
      "Epoch [687/1000] Loss: 7.4389 Val Loss: 16.2734\n",
      "Epoch [688/1000] Loss: 7.4513 Val Loss: 16.3079\n",
      "Epoch [689/1000] Loss: 7.5222 Val Loss: 16.2878\n",
      "Epoch [690/1000] Loss: 7.4466 Val Loss: 16.2822\n",
      "Epoch [691/1000] Loss: 7.4093 Val Loss: 16.2609\n",
      "Epoch [692/1000] Loss: 7.4034 Val Loss: 16.3247\n",
      "Epoch [693/1000] Loss: 7.4407 Val Loss: 16.2822\n",
      "Epoch [694/1000] Loss: 7.4075 Val Loss: 16.3549\n",
      "Epoch [695/1000] Loss: 7.3761 Val Loss: 16.3159\n",
      "Epoch [696/1000] Loss: 7.4508 Val Loss: 16.2921\n",
      "Epoch [697/1000] Loss: 7.3667 Val Loss: 16.2969\n",
      "Epoch [698/1000] Loss: 7.4004 Val Loss: 16.3279\n",
      "Epoch [699/1000] Loss: 7.4059 Val Loss: 16.3244\n",
      "Epoch [700/1000] Loss: 7.3883 Val Loss: 16.2879\n",
      "Epoch [701/1000] Loss: 7.4235 Val Loss: 16.3097\n",
      "Epoch [702/1000] Loss: 7.3531 Val Loss: 16.3862\n",
      "Epoch [703/1000] Loss: 7.4465 Val Loss: 16.2505\n",
      "Epoch [704/1000] Loss: 7.3645 Val Loss: 16.2972\n",
      "Epoch [705/1000] Loss: 7.3799 Val Loss: 16.2723\n",
      "Epoch [706/1000] Loss: 7.3793 Val Loss: 16.3965\n",
      "Epoch [707/1000] Loss: 7.3762 Val Loss: 16.3117\n",
      "Epoch [708/1000] Loss: 7.3572 Val Loss: 16.3200\n",
      "Epoch [709/1000] Loss: 7.3191 Val Loss: 16.3143\n",
      "Epoch [710/1000] Loss: 7.3442 Val Loss: 16.3192\n",
      "Epoch [711/1000] Loss: 7.3364 Val Loss: 16.3431\n",
      "Epoch [712/1000] Loss: 7.3155 Val Loss: 16.2412\n",
      "Epoch [713/1000] Loss: 7.3445 Val Loss: 16.3456\n",
      "Epoch [714/1000] Loss: 7.2981 Val Loss: 16.3171\n",
      "Epoch [715/1000] Loss: 7.3814 Val Loss: 16.3130\n",
      "Epoch [716/1000] Loss: 7.3330 Val Loss: 16.3261\n",
      "Epoch [717/1000] Loss: 7.3119 Val Loss: 16.3282\n",
      "Epoch [718/1000] Loss: 7.3622 Val Loss: 16.3432\n",
      "Epoch [719/1000] Loss: 7.3504 Val Loss: 16.3518\n",
      "Epoch [720/1000] Loss: 7.3397 Val Loss: 16.2910\n",
      "Epoch [721/1000] Loss: 7.2832 Val Loss: 16.3506\n",
      "Epoch [722/1000] Loss: 7.2697 Val Loss: 16.3074\n",
      "Epoch [723/1000] Loss: 7.3031 Val Loss: 16.3179\n",
      "Epoch [724/1000] Loss: 7.3594 Val Loss: 16.2671\n",
      "Epoch [725/1000] Loss: 7.2524 Val Loss: 16.3426\n",
      "Epoch [726/1000] Loss: 7.2996 Val Loss: 16.2870\n",
      "Epoch [727/1000] Loss: 7.3008 Val Loss: 16.2852\n",
      "Epoch [728/1000] Loss: 7.2744 Val Loss: 16.2683\n",
      "Epoch [729/1000] Loss: 7.3093 Val Loss: 16.2704\n",
      "Epoch [730/1000] Loss: 7.2745 Val Loss: 16.1995\n",
      "Epoch [731/1000] Loss: 7.2718 Val Loss: 16.3246\n",
      "Epoch [732/1000] Loss: 7.2490 Val Loss: 16.2258\n",
      "Epoch [733/1000] Loss: 7.2645 Val Loss: 16.2773\n",
      "Epoch [734/1000] Loss: 7.2667 Val Loss: 16.2350\n",
      "Epoch [735/1000] Loss: 7.2103 Val Loss: 16.2780\n",
      "Epoch [736/1000] Loss: 7.2610 Val Loss: 16.3570\n",
      "Epoch [737/1000] Loss: 7.2708 Val Loss: 16.3164\n",
      "Epoch [738/1000] Loss: 7.2448 Val Loss: 16.2893\n",
      "Epoch [739/1000] Loss: 7.2097 Val Loss: 16.3367\n",
      "Epoch [740/1000] Loss: 7.2180 Val Loss: 16.2765\n",
      "Epoch [741/1000] Loss: 7.2288 Val Loss: 16.2694\n",
      "Epoch [742/1000] Loss: 7.1889 Val Loss: 16.2607\n",
      "Epoch [743/1000] Loss: 7.1916 Val Loss: 16.2585\n",
      "Epoch [744/1000] Loss: 7.2580 Val Loss: 16.3179\n",
      "Epoch [745/1000] Loss: 7.2323 Val Loss: 16.2847\n",
      "Epoch [746/1000] Loss: 7.1471 Val Loss: 16.2320\n",
      "Epoch [747/1000] Loss: 7.1993 Val Loss: 16.2949\n",
      "Epoch [748/1000] Loss: 7.2171 Val Loss: 16.2151\n",
      "Epoch [749/1000] Loss: 7.1558 Val Loss: 16.3186\n",
      "Epoch [750/1000] Loss: 7.1937 Val Loss: 16.2841\n",
      "Epoch [751/1000] Loss: 7.2008 Val Loss: 16.3101\n",
      "Epoch [752/1000] Loss: 7.2039 Val Loss: 16.3348\n",
      "Epoch [753/1000] Loss: 7.1917 Val Loss: 16.2831\n",
      "Epoch [754/1000] Loss: 7.1776 Val Loss: 16.2415\n",
      "Epoch [755/1000] Loss: 7.1989 Val Loss: 16.2625\n",
      "Epoch [756/1000] Loss: 7.1590 Val Loss: 16.2133\n",
      "Epoch [757/1000] Loss: 7.1669 Val Loss: 16.2276\n",
      "Epoch [758/1000] Loss: 7.1229 Val Loss: 16.2545\n",
      "Epoch [759/1000] Loss: 7.1890 Val Loss: 16.3260\n",
      "Epoch [760/1000] Loss: 7.1606 Val Loss: 16.2551\n",
      "Epoch [761/1000] Loss: 7.1582 Val Loss: 16.2181\n",
      "Epoch [762/1000] Loss: 7.1404 Val Loss: 16.3457\n",
      "Epoch [763/1000] Loss: 7.1496 Val Loss: 16.3000\n",
      "Epoch [764/1000] Loss: 7.1614 Val Loss: 16.2643\n",
      "Epoch [765/1000] Loss: 7.1169 Val Loss: 16.2914\n",
      "Epoch [766/1000] Loss: 7.1195 Val Loss: 16.2286\n",
      "Epoch [767/1000] Loss: 7.1203 Val Loss: 16.2456\n",
      "Epoch [768/1000] Loss: 7.1065 Val Loss: 16.2985\n",
      "Epoch [769/1000] Loss: 7.1091 Val Loss: 16.2770\n",
      "Epoch [770/1000] Loss: 7.1420 Val Loss: 16.3373\n",
      "Epoch [771/1000] Loss: 7.1211 Val Loss: 16.3282\n",
      "Epoch [772/1000] Loss: 7.1234 Val Loss: 16.3352\n",
      "Epoch [773/1000] Loss: 7.0314 Val Loss: 16.2728\n",
      "Epoch [774/1000] Loss: 7.0739 Val Loss: 16.3276\n",
      "Epoch [775/1000] Loss: 7.0841 Val Loss: 16.3287\n",
      "Epoch [776/1000] Loss: 7.1073 Val Loss: 16.3220\n",
      "Epoch [777/1000] Loss: 7.0754 Val Loss: 16.2706\n",
      "Epoch [778/1000] Loss: 7.1214 Val Loss: 16.3070\n",
      "Epoch [779/1000] Loss: 7.0761 Val Loss: 16.2682\n",
      "Epoch [780/1000] Loss: 7.0914 Val Loss: 16.2776\n",
      "Epoch [781/1000] Loss: 7.1235 Val Loss: 16.2780\n",
      "Epoch [782/1000] Loss: 7.0684 Val Loss: 16.2990\n",
      "Epoch [783/1000] Loss: 7.0642 Val Loss: 16.2881\n",
      "Epoch [784/1000] Loss: 7.0546 Val Loss: 16.4035\n",
      "Epoch [785/1000] Loss: 7.0524 Val Loss: 16.3217\n",
      "Epoch [786/1000] Loss: 7.0698 Val Loss: 16.3397\n",
      "Epoch [787/1000] Loss: 7.0570 Val Loss: 16.4348\n",
      "Epoch [788/1000] Loss: 7.0900 Val Loss: 16.3306\n",
      "Epoch [789/1000] Loss: 7.0268 Val Loss: 16.2591\n",
      "Epoch [790/1000] Loss: 7.0880 Val Loss: 16.3042\n",
      "Epoch [791/1000] Loss: 7.0914 Val Loss: 16.3254\n",
      "Epoch [792/1000] Loss: 6.9759 Val Loss: 16.3850\n",
      "Epoch [793/1000] Loss: 7.0280 Val Loss: 16.2996\n",
      "Epoch [794/1000] Loss: 7.0530 Val Loss: 16.3137\n",
      "Epoch [795/1000] Loss: 7.0266 Val Loss: 16.2970\n",
      "Epoch [796/1000] Loss: 7.0297 Val Loss: 16.2496\n",
      "Epoch [797/1000] Loss: 6.9952 Val Loss: 16.2640\n",
      "Epoch [798/1000] Loss: 7.0633 Val Loss: 16.3103\n",
      "Epoch [799/1000] Loss: 7.0011 Val Loss: 16.2622\n",
      "Epoch [800/1000] Loss: 7.0927 Val Loss: 16.3254\n",
      "Epoch [801/1000] Loss: 7.0087 Val Loss: 16.3053\n",
      "Epoch [802/1000] Loss: 7.0217 Val Loss: 16.3033\n",
      "Epoch [803/1000] Loss: 6.9789 Val Loss: 16.3032\n",
      "Epoch [804/1000] Loss: 7.0211 Val Loss: 16.3088\n",
      "Epoch [805/1000] Loss: 6.9775 Val Loss: 16.2886\n",
      "Epoch [806/1000] Loss: 6.9325 Val Loss: 16.3660\n",
      "Epoch [807/1000] Loss: 6.9611 Val Loss: 16.3596\n",
      "Epoch [808/1000] Loss: 6.9922 Val Loss: 16.3446\n",
      "Epoch [809/1000] Loss: 6.9622 Val Loss: 16.3196\n",
      "Epoch [810/1000] Loss: 7.0224 Val Loss: 16.2644\n",
      "Epoch [811/1000] Loss: 6.9765 Val Loss: 16.3016\n",
      "Epoch [812/1000] Loss: 6.9618 Val Loss: 16.3180\n",
      "Epoch [813/1000] Loss: 6.9755 Val Loss: 16.2856\n",
      "Epoch [814/1000] Loss: 6.9418 Val Loss: 16.3232\n",
      "Epoch [815/1000] Loss: 6.9720 Val Loss: 16.3051\n",
      "Epoch [816/1000] Loss: 6.9606 Val Loss: 16.3092\n",
      "Epoch [817/1000] Loss: 6.9818 Val Loss: 16.3175\n",
      "Epoch [818/1000] Loss: 6.9262 Val Loss: 16.3266\n",
      "Epoch [819/1000] Loss: 6.9693 Val Loss: 16.3446\n",
      "Epoch [820/1000] Loss: 6.9164 Val Loss: 16.4077\n",
      "Epoch [821/1000] Loss: 6.9259 Val Loss: 16.3622\n",
      "Epoch [822/1000] Loss: 6.9392 Val Loss: 16.3223\n",
      "Epoch [823/1000] Loss: 6.9633 Val Loss: 16.2168\n",
      "Epoch [824/1000] Loss: 6.8886 Val Loss: 16.3196\n",
      "Epoch [825/1000] Loss: 6.9310 Val Loss: 16.2866\n",
      "Epoch [826/1000] Loss: 6.8636 Val Loss: 16.2840\n",
      "Epoch [827/1000] Loss: 6.8920 Val Loss: 16.2642\n",
      "Epoch [828/1000] Loss: 6.9628 Val Loss: 16.2469\n",
      "Epoch [829/1000] Loss: 6.9458 Val Loss: 16.2751\n",
      "Epoch [830/1000] Loss: 6.9204 Val Loss: 16.3404\n",
      "Epoch [831/1000] Loss: 6.8929 Val Loss: 16.2534\n",
      "Epoch [832/1000] Loss: 6.8928 Val Loss: 16.3052\n",
      "Epoch [833/1000] Loss: 6.9266 Val Loss: 16.3031\n",
      "Epoch [834/1000] Loss: 6.8778 Val Loss: 16.2494\n",
      "Epoch [835/1000] Loss: 6.8552 Val Loss: 16.3479\n",
      "Epoch [836/1000] Loss: 6.8880 Val Loss: 16.2907\n",
      "Epoch [837/1000] Loss: 6.8990 Val Loss: 16.3350\n",
      "Epoch [838/1000] Loss: 6.8422 Val Loss: 16.2888\n",
      "Epoch [839/1000] Loss: 6.8738 Val Loss: 16.3918\n",
      "Epoch [840/1000] Loss: 6.9248 Val Loss: 16.2702\n",
      "Epoch [841/1000] Loss: 6.9022 Val Loss: 16.2513\n",
      "Epoch [842/1000] Loss: 6.8906 Val Loss: 16.3706\n",
      "Epoch [843/1000] Loss: 6.8759 Val Loss: 16.2805\n",
      "Epoch [844/1000] Loss: 6.8897 Val Loss: 16.2657\n",
      "Epoch [845/1000] Loss: 6.8352 Val Loss: 16.1975\n",
      "Epoch [846/1000] Loss: 6.8629 Val Loss: 16.2364\n",
      "Epoch [847/1000] Loss: 6.8166 Val Loss: 16.3054\n",
      "Epoch [848/1000] Loss: 6.8411 Val Loss: 16.2720\n",
      "Epoch [849/1000] Loss: 6.8555 Val Loss: 16.2891\n",
      "Epoch [850/1000] Loss: 6.8326 Val Loss: 16.3043\n",
      "Epoch [851/1000] Loss: 6.8319 Val Loss: 16.3081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [852/1000] Loss: 6.8527 Val Loss: 16.3105\n",
      "Epoch [853/1000] Loss: 6.8467 Val Loss: 16.2719\n",
      "Epoch [854/1000] Loss: 6.8583 Val Loss: 16.3354\n",
      "Epoch [855/1000] Loss: 6.8764 Val Loss: 16.2489\n",
      "Epoch [856/1000] Loss: 6.8315 Val Loss: 16.2655\n",
      "Epoch [857/1000] Loss: 6.8057 Val Loss: 16.3339\n",
      "Epoch [858/1000] Loss: 6.7916 Val Loss: 16.2859\n",
      "Epoch [859/1000] Loss: 6.8611 Val Loss: 16.3036\n",
      "Epoch [860/1000] Loss: 6.8194 Val Loss: 16.3213\n",
      "Epoch [861/1000] Loss: 6.8547 Val Loss: 16.3280\n",
      "Epoch [862/1000] Loss: 6.7937 Val Loss: 16.3267\n",
      "Epoch [863/1000] Loss: 6.7848 Val Loss: 16.4030\n",
      "Epoch [864/1000] Loss: 6.8230 Val Loss: 16.3366\n",
      "Epoch [865/1000] Loss: 6.7934 Val Loss: 16.3207\n",
      "Epoch [866/1000] Loss: 6.7910 Val Loss: 16.3873\n",
      "Epoch [867/1000] Loss: 6.7941 Val Loss: 16.3190\n",
      "Epoch [868/1000] Loss: 6.8073 Val Loss: 16.2928\n",
      "Epoch [869/1000] Loss: 6.8076 Val Loss: 16.2939\n",
      "Epoch [870/1000] Loss: 6.7991 Val Loss: 16.3168\n",
      "Epoch [871/1000] Loss: 6.7326 Val Loss: 16.3307\n",
      "Epoch [872/1000] Loss: 6.7620 Val Loss: 16.3151\n",
      "Epoch [873/1000] Loss: 6.7709 Val Loss: 16.3143\n",
      "Epoch [874/1000] Loss: 6.7428 Val Loss: 16.2951\n",
      "Epoch [875/1000] Loss: 6.7699 Val Loss: 16.3358\n",
      "Epoch [876/1000] Loss: 6.7488 Val Loss: 16.2782\n",
      "Epoch [877/1000] Loss: 6.7340 Val Loss: 16.3696\n",
      "Epoch [878/1000] Loss: 6.7721 Val Loss: 16.2418\n",
      "Epoch [879/1000] Loss: 6.7406 Val Loss: 16.3357\n",
      "Epoch [880/1000] Loss: 6.7260 Val Loss: 16.3018\n",
      "Epoch [881/1000] Loss: 6.7646 Val Loss: 16.2917\n",
      "Epoch [882/1000] Loss: 6.7797 Val Loss: 16.2519\n",
      "Epoch [883/1000] Loss: 6.7693 Val Loss: 16.2699\n",
      "Epoch [884/1000] Loss: 6.7767 Val Loss: 16.3332\n",
      "Epoch [885/1000] Loss: 6.7704 Val Loss: 16.3174\n",
      "Epoch [886/1000] Loss: 6.7031 Val Loss: 16.3349\n",
      "Epoch [887/1000] Loss: 6.7130 Val Loss: 16.2669\n",
      "Epoch [888/1000] Loss: 6.7805 Val Loss: 16.2733\n",
      "Epoch [889/1000] Loss: 6.7132 Val Loss: 16.3128\n",
      "Epoch [890/1000] Loss: 6.7685 Val Loss: 16.3126\n",
      "Epoch [891/1000] Loss: 6.7287 Val Loss: 16.2849\n",
      "Epoch [892/1000] Loss: 6.7540 Val Loss: 16.3150\n",
      "Epoch [893/1000] Loss: 6.7209 Val Loss: 16.2649\n",
      "Epoch [894/1000] Loss: 6.7460 Val Loss: 16.2371\n",
      "Epoch [895/1000] Loss: 6.7038 Val Loss: 16.1669\n",
      "Epoch [896/1000] Loss: 6.7201 Val Loss: 16.2990\n",
      "Epoch [897/1000] Loss: 6.7037 Val Loss: 16.2736\n",
      "Epoch [898/1000] Loss: 6.7577 Val Loss: 16.2366\n",
      "Epoch [899/1000] Loss: 6.7036 Val Loss: 16.3108\n",
      "Epoch [900/1000] Loss: 6.7233 Val Loss: 16.2438\n",
      "Epoch [901/1000] Loss: 6.6684 Val Loss: 16.2292\n",
      "Epoch [902/1000] Loss: 6.7482 Val Loss: 16.3332\n",
      "Epoch [903/1000] Loss: 6.6670 Val Loss: 16.3315\n",
      "Epoch [904/1000] Loss: 6.6645 Val Loss: 16.2588\n",
      "Epoch [905/1000] Loss: 6.6687 Val Loss: 16.3836\n",
      "Epoch [906/1000] Loss: 6.6503 Val Loss: 16.2818\n",
      "Epoch [907/1000] Loss: 6.6907 Val Loss: 16.2981\n",
      "Epoch [908/1000] Loss: 6.6467 Val Loss: 16.2636\n",
      "Epoch [909/1000] Loss: 6.7017 Val Loss: 16.2309\n",
      "Epoch [910/1000] Loss: 6.6971 Val Loss: 16.2421\n",
      "Epoch [911/1000] Loss: 6.6618 Val Loss: 16.3233\n",
      "Epoch [912/1000] Loss: 6.6737 Val Loss: 16.2974\n",
      "Epoch [913/1000] Loss: 6.6311 Val Loss: 16.2962\n",
      "Epoch [914/1000] Loss: 6.6282 Val Loss: 16.3431\n",
      "Epoch [915/1000] Loss: 6.6742 Val Loss: 16.2359\n",
      "Epoch [916/1000] Loss: 6.6235 Val Loss: 16.2727\n",
      "Epoch [917/1000] Loss: 6.6686 Val Loss: 16.2804\n",
      "Epoch [918/1000] Loss: 6.6792 Val Loss: 16.2963\n",
      "Epoch [919/1000] Loss: 6.6776 Val Loss: 16.2393\n",
      "Epoch [920/1000] Loss: 6.6312 Val Loss: 16.3073\n",
      "Epoch [921/1000] Loss: 6.6843 Val Loss: 16.2738\n",
      "Epoch [922/1000] Loss: 6.6265 Val Loss: 16.3120\n",
      "Epoch [923/1000] Loss: 6.6673 Val Loss: 16.3287\n",
      "Epoch [924/1000] Loss: 6.6359 Val Loss: 16.2702\n",
      "Epoch [925/1000] Loss: 6.6591 Val Loss: 16.2790\n",
      "Epoch [926/1000] Loss: 6.6632 Val Loss: 16.3223\n",
      "Epoch [927/1000] Loss: 6.5786 Val Loss: 16.3243\n",
      "Epoch [928/1000] Loss: 6.6945 Val Loss: 16.2880\n",
      "Epoch [929/1000] Loss: 6.6527 Val Loss: 16.3400\n",
      "Epoch [930/1000] Loss: 6.6453 Val Loss: 16.3310\n",
      "Epoch [931/1000] Loss: 6.6081 Val Loss: 16.3013\n",
      "Epoch [932/1000] Loss: 6.6001 Val Loss: 16.3413\n",
      "Epoch [933/1000] Loss: 6.5998 Val Loss: 16.3292\n",
      "Epoch [934/1000] Loss: 6.5855 Val Loss: 16.2963\n",
      "Epoch [935/1000] Loss: 6.5930 Val Loss: 16.3268\n",
      "Epoch [936/1000] Loss: 6.6048 Val Loss: 16.2462\n",
      "Epoch [937/1000] Loss: 6.5768 Val Loss: 16.2839\n",
      "Epoch [938/1000] Loss: 6.6212 Val Loss: 16.3338\n",
      "Epoch [939/1000] Loss: 6.5873 Val Loss: 16.3159\n",
      "Epoch [940/1000] Loss: 6.5910 Val Loss: 16.2998\n",
      "Epoch [941/1000] Loss: 6.6257 Val Loss: 16.2407\n",
      "Epoch [942/1000] Loss: 6.6044 Val Loss: 16.2710\n",
      "Epoch [943/1000] Loss: 6.6125 Val Loss: 16.3171\n",
      "Epoch [944/1000] Loss: 6.5727 Val Loss: 16.3117\n",
      "Epoch [945/1000] Loss: 6.5944 Val Loss: 16.3088\n",
      "Epoch [946/1000] Loss: 6.5764 Val Loss: 16.2984\n",
      "Epoch [947/1000] Loss: 6.5471 Val Loss: 16.2657\n",
      "Epoch [948/1000] Loss: 6.5528 Val Loss: 16.3770\n",
      "Epoch [949/1000] Loss: 6.5740 Val Loss: 16.3232\n",
      "Epoch [950/1000] Loss: 6.5710 Val Loss: 16.1982\n",
      "Epoch [951/1000] Loss: 6.5477 Val Loss: 16.2624\n",
      "Epoch [952/1000] Loss: 6.5432 Val Loss: 16.3140\n",
      "Epoch [953/1000] Loss: 6.5837 Val Loss: 16.3182\n",
      "Epoch [954/1000] Loss: 6.5614 Val Loss: 16.2897\n",
      "Epoch [955/1000] Loss: 6.5366 Val Loss: 16.2764\n",
      "Epoch [956/1000] Loss: 6.5700 Val Loss: 16.2421\n",
      "Epoch [957/1000] Loss: 6.5568 Val Loss: 16.3075\n",
      "Epoch [958/1000] Loss: 6.5598 Val Loss: 16.2336\n",
      "Epoch [959/1000] Loss: 6.5645 Val Loss: 16.3906\n",
      "Epoch [960/1000] Loss: 6.5673 Val Loss: 16.3912\n",
      "Epoch [961/1000] Loss: 6.5430 Val Loss: 16.3064\n",
      "Epoch [962/1000] Loss: 6.6154 Val Loss: 16.3214\n",
      "Epoch [963/1000] Loss: 6.5292 Val Loss: 16.2887\n",
      "Epoch [964/1000] Loss: 6.4961 Val Loss: 16.3104\n",
      "Epoch [965/1000] Loss: 6.5638 Val Loss: 16.2958\n",
      "Epoch [966/1000] Loss: 6.5607 Val Loss: 16.2871\n",
      "Epoch [967/1000] Loss: 6.5342 Val Loss: 16.3555\n",
      "Epoch [968/1000] Loss: 6.5349 Val Loss: 16.3305\n",
      "Epoch [969/1000] Loss: 6.5462 Val Loss: 16.2833\n",
      "Epoch [970/1000] Loss: 6.5686 Val Loss: 16.2379\n",
      "Epoch [971/1000] Loss: 6.5430 Val Loss: 16.2529\n",
      "Epoch [972/1000] Loss: 6.4716 Val Loss: 16.2880\n",
      "Epoch [973/1000] Loss: 6.5266 Val Loss: 16.2559\n",
      "Epoch [974/1000] Loss: 6.5064 Val Loss: 16.2459\n",
      "Epoch [975/1000] Loss: 6.4914 Val Loss: 16.3065\n",
      "Epoch [976/1000] Loss: 6.5045 Val Loss: 16.3266\n",
      "Epoch [977/1000] Loss: 6.5419 Val Loss: 16.3741\n",
      "Epoch [978/1000] Loss: 6.5266 Val Loss: 16.3051\n",
      "Epoch [979/1000] Loss: 6.5243 Val Loss: 16.3211\n",
      "Epoch [980/1000] Loss: 6.5279 Val Loss: 16.2135\n",
      "Epoch [981/1000] Loss: 6.5053 Val Loss: 16.2440\n",
      "Epoch [982/1000] Loss: 6.5378 Val Loss: 16.2924\n",
      "Epoch [983/1000] Loss: 6.5438 Val Loss: 16.2477\n",
      "Epoch [984/1000] Loss: 6.4574 Val Loss: 16.3169\n",
      "Epoch [985/1000] Loss: 6.5252 Val Loss: 16.3295\n",
      "Epoch [986/1000] Loss: 6.5477 Val Loss: 16.3226\n",
      "Epoch [987/1000] Loss: 6.4975 Val Loss: 16.3236\n",
      "Epoch [988/1000] Loss: 6.4906 Val Loss: 16.2658\n",
      "Epoch [989/1000] Loss: 6.4901 Val Loss: 16.3571\n",
      "Epoch [990/1000] Loss: 6.4861 Val Loss: 16.3285\n",
      "Epoch [991/1000] Loss: 6.4581 Val Loss: 16.3559\n",
      "Epoch [992/1000] Loss: 6.4986 Val Loss: 16.3648\n",
      "Epoch [993/1000] Loss: 6.5062 Val Loss: 16.3159\n",
      "Epoch [994/1000] Loss: 6.4922 Val Loss: 16.2916\n",
      "Epoch [995/1000] Loss: 6.4272 Val Loss: 16.3057\n",
      "Epoch [996/1000] Loss: 6.4578 Val Loss: 16.2842\n",
      "Epoch [997/1000] Loss: 6.4420 Val Loss: 16.3518\n",
      "Epoch [998/1000] Loss: 6.4637 Val Loss: 16.2496\n",
      "Epoch [999/1000] Loss: 6.4679 Val Loss: 16.2811\n",
      "Epoch [1000/1000] Loss: 6.4560 Val Loss: 16.2802\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AA_Classifier()\n",
    "#model.load_state_dict(torch.load('9_encder_layers_12092023.pt'))\n",
    "model.to(DEVICE) # put on GPU\n",
    "\n",
    "# Define a loss function (e.g., Mean Squared Error) and an optimizer (e.g., Adam)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'sum', label_smoothing = 0.1)\n",
    "#criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas = (0.9,0.98),eps =1e-9, lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "losses = []\n",
    "smallest = 39.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "        \n",
    "    for batch in train_dl:\n",
    "        model.train()\n",
    "        inputs = batch.to(DEVICE)\n",
    "        mask = []\n",
    "        for i in batch.mask:\n",
    "            mask += i \n",
    "        \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            for i, j in enumerate(mask):\n",
    "                if j == True:\n",
    "                    p = random.random()\n",
    "                    if p < 0.8:\n",
    "                        inputs.x[i] = torch.zeros(133)\n",
    "                    elif p >= 0.8 and p < 0.92:\n",
    "                        inputs.x[i] = AA_embeddings[random.choice(AA_3_letters)]\n",
    "        \n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #outputs, MN = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs[mask], inputs.y[mask])*inputs.weight\n",
    "        #loss = custom_loss([outputs[mask],MN], [inputs.y[mask],inputs.Kd], lossf)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inputs= inputs.to('cpu')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    for batch in val_dl:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "            \n",
    "      \n",
    "            for i, j in enumerate(inputs.mask):\n",
    "                if j == True:\n",
    "                    inputs.x[i] = torch.zeros(133)\n",
    "                    \n",
    "            outputs = model(inputs)\n",
    "            #outputs, MN = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs[inputs.mask], inputs.y[inputs.mask])*inputs.weight\n",
    "            #loss = custom_loss([outputs[inputs.mask],MN], [inputs.y[inputs.mask],inputs.Kd], lossf)\n",
    "        \n",
    "            inputs= inputs.to('cpu')\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / (train_batch_size*len(train_dl))\n",
    "    avg_val_loss = val_loss / len(val_dl)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Val Loss: {avg_val_loss:.4f}')\n",
    "    losses.append([avg_loss,avg_val_loss])\n",
    "    \n",
    "    if avg_val_loss < smallest:\n",
    "        torch.save(model.state_dict(), '02192024_grouping_split_reweight.pt')\n",
    "        smallest = avg_val_loss\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be6e276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '3_encder_layers_8heads_12272023_64_hidden_.pt')\n",
    "#torch.save(model.state_dict(), '02192024_down_sampled_grouping_split.pt')\n",
    "torch.save(optimizer.state_dict(),'02192024_grouping_split_reweight_opt.pt')\n",
    "torch.save(torch.Tensor(losses),'02192024_grouping_split_reweight_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cca0fc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4c4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.06430265395232\n"
     ]
    }
   ],
   "source": [
    "val_loss_hold = []\n",
    "for i in losses:\n",
    "    val_loss_hold.append(i[1])\n",
    "print(min(val_loss_hold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eeb056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
