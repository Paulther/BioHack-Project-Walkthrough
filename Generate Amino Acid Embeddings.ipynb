{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b696a815",
   "metadata": {},
   "source": [
    "# Getting Amino Acids and Small Molecules to \"Speak the Same Language\"\n",
    "![EmbeddingStrategy](images/EmbeddingStrategy.png)<br>\n",
    "## Overview\n",
    "The first challenge we faced was embedding amino acids and small molecules in the same latent space. The solution we came up with was to represent each atom in a small molecule as a graph node and treat amino acids as a sort of \"psuedo-atom\". <br>\n",
    "We used an encoder-decoder architechture that took as an input an amino acid represnted as a graph and output the amino acids associated [BLOSUM62](https://en.wikipedia.org/wiki/BLOSUM) matrix row. The graph contained an additional \"master\" node connected to all other nodes that facilitated the flow of information and served as a convenient read-out. The encoder consisted of three rounds of message passing. At this point, the encoded information is stored in the master node. The decoder is simply a single densley connected layer.<br>\n",
    "Each node in the graph represents an atom in the amino acid. The features of each atom came from [Fang et. al](https://www.nature.com/articles/s42256-023-00654-0). These are the same features we use for the sequence prediction network. Our idea was that by briding the gap between atomic and amino acid level information, we would be able to treat amino acids and atoms on equal footing. This is the weakest assumption in the construction of our model and needs furth investigation to determine whether this stradegy is sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ace1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import Bio.PDB\n",
    "import pickle as pickle\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GENConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.aggr import MeanAggregation\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Bio import PDB\n",
    "from rdkit import Chem\n",
    "import blosum as bl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1323c",
   "metadata": {},
   "source": [
    "**Replace File Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5eaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pdbfiles: str = \"/home/paul/Desktop/BioHack-Project-Walkthrough/pdbind-refined-set/\"\n",
    "    AA_mol2_files: str = \"/home/paul/Desktop/BioHack-Project-Walkthrough/AA_mol2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e2cc8",
   "metadata": {},
   "source": [
    "## Graph Construction\n",
    "This will be discussed in more detail in future notebooks. The graphs were constructed in the same way that the small molecule graphs will be constructed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70bb3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load atomic embeddings from Fang et al.\n",
    "with open('atom2emb.pkl', 'rb') as f:\n",
    "    atom2emb = pickle.load(f)\n",
    "\n",
    "# Load bond dictionary from earlier notebook \n",
    "with open('bond_type_dict.pkl', 'rb') as f:\n",
    "    bond_type_dict = pickle.load(f)\n",
    "    \n",
    "#Self-explanatory dictionary - if only people could use the amino acid codes\n",
    "one_letter_to_three_letter_dict = {'G':'gly',\n",
    "                                   'A':'ala',\n",
    "                                   'V':'val',\n",
    "                                   'C':'cys',\n",
    "                                   'P':'pro',\n",
    "                                   'L':'leu',\n",
    "                                   'I':'ile',\n",
    "                                   'M':'met',\n",
    "                                   'W':'trp',\n",
    "                                   'F':'phe',\n",
    "                                   'K':'lys',\n",
    "                                   'R':'arg',\n",
    "                                   'H':'his',\n",
    "                                   'S':'ser',\n",
    "                                   'T':'thr',\n",
    "                                   'Y':'tyr',\n",
    "                                   'N':'asn',\n",
    "                                   'Q':'gln',\n",
    "                                   'D':'asp',\n",
    "                                   'E':'glu'\n",
    "    \n",
    "}\n",
    "\n",
    "#Self-explanatory dictionary - if only people could use the amino acid codes\n",
    "upper2lower = {\n",
    "    \"ala\": \"ALA\",\n",
    "    \"arg\": \"ARG\",\n",
    "    \"asn\": \"ASN\",\n",
    "    \"asp\": \"ASP\",\n",
    "    \"cys\": \"CYS\",\n",
    "    \"gln\": \"GLN\",\n",
    "    \"glu\": \"GLU\",\n",
    "    \"gly\": \"GLY\",\n",
    "    \"his\": \"HIS\",\n",
    "    \"ile\": \"ILE\",\n",
    "    \"leu\": \"LEU\",\n",
    "    \"lys\": \"LYS\",\n",
    "    \"met\": \"MET\",\n",
    "    \"phe\": \"PHE\",\n",
    "    \"pro\": \"PRO\",\n",
    "    \"ser\": \"SER\",\n",
    "    \"thr\": \"THR\",\n",
    "    \"trp\": \"TRP\",\n",
    "    \"tyr\": \"TYR\",\n",
    "    \"val\": \"VAL\",\n",
    "}\n",
    "\n",
    "def BLOSUM_encode_single(seq,AA_dict):\n",
    "    allowed = set(\"gavcplimwfkrhstynqdeuogavcplimwfkrhstynqde\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        raise ValueError(f\"Sequence has broken AA: {invalid}\")\n",
    "    vec = AA_dict[seq]\n",
    "    return vec\n",
    "\n",
    "matrix = bl.BLOSUM(62)\n",
    "allowed_AA = \"GAVCPLIMWFKRHSTYNQDE\"\n",
    "BLOSUM_dict_three_letter = {}\n",
    "for i in allowed_AA:\n",
    "    vec = []\n",
    "    for j in allowed_AA:\n",
    "        vec.append(matrix[i][j])\n",
    "    BLOSUM_dict_three_letter.update({one_letter_to_three_letter_dict[i]:torch.Tensor(vec)})\n",
    "\n",
    "    \n",
    "def read_mol2_bonds_and_atoms(mol2_file):\n",
    "    bonds = []\n",
    "    bond_types = []\n",
    "    atom_types = {}\n",
    "    atom_coordinates = {}\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip().startswith('@<TRIPOS>SUBSTRUCTURE'):\n",
    "                break\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                reading_bonds = False\n",
    "            elif reading_atoms and line.strip().startswith('@<TRIPOS>'):\n",
    "                reading_atoms = False\n",
    "\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    atom1_index = int(parts[1])\n",
    "                    atom2_index = int(parts[2])\n",
    "                    bond_type = parts[3]\n",
    "                    bonds.append((atom1_index, atom2_index))\n",
    "                    bond_types.append(bond_type)\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 6:\n",
    "                    atom_index = int(parts[0])\n",
    "                    atom_type = parts[5]\n",
    "                    x, y, z = float(parts[2]), float(parts[3]), float(parts[4])\n",
    "                    atom_types[atom_index] = atom_type.split('.')[0]\n",
    "                    atom_coordinates[atom_index] = (x, y, z)\n",
    "\n",
    "    return bonds, bond_types, atom_types, atom_coordinates  \n",
    "\n",
    "def molecule2graph_AA(filename,map_distance, norm_map_distance = 12.0):\n",
    "    node_feature = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    mol2_file = CFG.AA_mol2_files+filename\n",
    "    bonds, bond_types, atom_types, atom_coordinates = read_mol2_bonds_and_atoms(mol2_file)\n",
    "    for atom in atom_types:\n",
    "        node_feature.append(torch.Tensor(atom2emb[atom_types[atom]]))\n",
    "    \n",
    "\n",
    "    for atom1 in range(1, len(atom_types)+1):\n",
    "        for atom2 in range(atom1 + 1, len(atom_types)+1):\n",
    "            bonded_flag = 0\n",
    "            for i, bond in enumerate(bonds):\n",
    "                if (atom1 in bond) and (atom2 in bond):\n",
    "                    edge_index.append([bond[0] - 1,bond[1] - 1])\n",
    "                    coord1 = np.array(atom_coordinates[bond[0]])\n",
    "                    coord2 = np.array(atom_coordinates[bond[1]])\n",
    "                    dist = math.dist(coord1, coord2)\n",
    "                    d = []\n",
    "                    for l in range(12):\n",
    "                        d.append(np.exp((-1.0*(dist - 2.0*(l + 0.5))**2.0)/norm_map_distance))\n",
    "                    bond_type = bond_type_dict[bond_types[i]]\n",
    "                    edge_attr.append(np.hstack((d,d,d,d,d,d,d,d,d,bond_type)))\n",
    "                    bonded_flag = 1\n",
    "                \n",
    "            if bonded_flag == 0:\n",
    "                coord1 = np.array(atom_coordinates[atom1])\n",
    "                coord2 = np.array(atom_coordinates[atom2])\n",
    "                dist = math.dist(coord1, coord2)\n",
    "                if dist < map_distance:\n",
    "                    edge_index.append([atom1 - 1,atom2 - 1])\n",
    "                    d = []\n",
    "                    for l in range(12):\n",
    "                        d.append(np.exp((-1.0*(dist - 2.0*(l + 0.5))**2.0)/norm_map_distance))\n",
    "                    bond_type = bond_type_dict['nc']\n",
    "                    edge_attr.append(np.hstack((d,d,d,d,d,d,d,d,d,bond_type)))\n",
    "\n",
    "    \n",
    "    edge_index = np.array(edge_index)\n",
    "    edge_index = edge_index.transpose()\n",
    "    edge_index = torch.Tensor(edge_index)\n",
    "    edge_index = edge_index.to(torch.int64)\n",
    "    edge_attr = torch.Tensor(np.array(edge_attr))\n",
    "    node_feature = torch.stack(node_feature)\n",
    "    \n",
    "    #Master_node\n",
    "    new_edge_index = []\n",
    "    new_edge_attr = []\n",
    "    node_feature = torch.cat((node_feature,torch.zeros(len(atom2emb['N'])).unsqueeze(0)),dim = 0)\n",
    "    \n",
    "    for i in range(len(node_feature) - 1):\n",
    "        new_edge_index.append([i,int(len(node_feature)-1)])\n",
    "        bond_type = bond_type_dict['nc']\n",
    "        new_edge_attr.append(np.hstack((np.zeros(9*len(d)),bond_type)))\n",
    "    \n",
    "    new_edge_index = np.array(new_edge_index)\n",
    "    new_edge_index = new_edge_index.transpose()\n",
    "    new_edge_index = torch.Tensor(new_edge_index)\n",
    "    new_edge_index = new_edge_index.to(torch.int64)\n",
    "    new_edge_attr = torch.Tensor(np.array(new_edge_attr))    \n",
    "    \n",
    "    edge_index = torch.cat((edge_index,new_edge_index), dim = 1)\n",
    "    edge_attr = torch.cat((edge_attr,new_edge_attr), dim = 0)\n",
    "    \n",
    "    graph = Data(x = node_feature, edge_index = edge_index,edge_attr = edge_attr)#, pos = new_mol_coords)\n",
    "    graph.label = filename.split('.')[0]\n",
    "    softmax = nn.Softmax(dim = 0)\n",
    "    graph.y = softmax(BLOSUM_encode_single(graph.label,BLOSUM_dict_three_letter))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f621eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_graphs = []\n",
    "for filename in os.listdir(CFG.AA_mol2_files):\n",
    "    AA_graphs.append(molecule2graph_AA(filename,12.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ed27d",
   "metadata": {},
   "source": [
    "Read about the message passing module used in the encoder [here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html#torch_geometric.nn.conv.GENConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b41e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.node_feature_size = 133\n",
    "        self.node_feature_hidden_size = 133\n",
    "        self.node_feature_size_out = 20\n",
    "        self.conv1 = GENConv(self.node_feature_size,self.node_feature_hidden_size,aggr = 'mean',edge_dim = 114, num_layer = 2,norm = 'layer')\n",
    "        self.conv2 = GENConv(self.node_feature_hidden_size,self.node_feature_hidden_size,aggr = 'mean',edge_dim = 114,num_layer = 2,norm = 'layer')\n",
    "        self.conv3 = GENConv(self.node_feature_hidden_size,self.node_feature_hidden_size,aggr = 'mean',edge_dim = 114,num_layer = 2,norm = 'layer')\n",
    "        self.linear1 = nn.Linear(self.node_feature_hidden_size,self.node_feature_size_out)\n",
    "        self.ReLu = nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,graph):\n",
    "        x, edge_index, edge_attr = graph.x, graph.edge_index, graph.edge_attr\n",
    "        x1 = self.conv1(x, edge_index, edge_attr)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv2(x1, edge_index, edge_attr)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv3(x1, edge_index, edge_attr)\n",
    "        x1 = x1[-1]\n",
    "        x1 = torch.tanh(x1)\n",
    "        x1 = self.linear1(x1)\n",
    "        return x1\n",
    "    \n",
    "    def encode(self,graph):\n",
    "        x, edge_index, edge_attr = graph.x,graph.edge_index,graph.edge_attr\n",
    "        x1 = self.conv1(x, edge_index,edge_attr)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv2(x1, edge_index,edge_attr)\n",
    "        x1 = self.ReLu(x1)\n",
    "        x1 = self.conv3(x1, edge_index,edge_attr)\n",
    "        x1 = x1[-1]\n",
    "        x1 = torch.tanh(x1)\n",
    "        return x1\n",
    "    \n",
    "    def decode(self,encoding):\n",
    "        x1 = self.linear1(encoding)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ae0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_dl = DataLoader(AA_graphs,batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5186876",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The model was trained for 1000 epochs to optimize the mean squared error loss. The optimization was done using the adam optimizer with a learning rate of 1e-4 and L2 normaliztion of 1e-4. The learning rate was dropped one order of magnitude after 500 epochs and again after 800 epochs. No hyperparameter tuning was done. The model that achieved the lowest loss is saved to generate the amino acid embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6989511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Loss: 0.0530 Val Loss: 0.0405\n",
      "Epoch [2/1000] Loss: 0.0430 Val Loss: 0.0382\n",
      "Epoch [3/1000] Loss: 0.0417 Val Loss: 0.0389\n",
      "Epoch [4/1000] Loss: 0.0416 Val Loss: 0.0390\n",
      "Epoch [5/1000] Loss: 0.0426 Val Loss: 0.0377\n",
      "Epoch [6/1000] Loss: 0.0401 Val Loss: 0.0380\n",
      "Epoch [7/1000] Loss: 0.0408 Val Loss: 0.0383\n",
      "Epoch [8/1000] Loss: 0.0402 Val Loss: 0.0376\n",
      "Epoch [9/1000] Loss: 0.0395 Val Loss: 0.0368\n",
      "Epoch [10/1000] Loss: 0.0396 Val Loss: 0.0372\n",
      "Epoch [11/1000] Loss: 0.0405 Val Loss: 0.0366\n",
      "Epoch [12/1000] Loss: 0.0391 Val Loss: 0.0367\n",
      "Epoch [13/1000] Loss: 0.0394 Val Loss: 0.0377\n",
      "Epoch [14/1000] Loss: 0.0397 Val Loss: 0.0361\n",
      "Epoch [15/1000] Loss: 0.0387 Val Loss: 0.0366\n",
      "Epoch [16/1000] Loss: 0.0387 Val Loss: 0.0355\n",
      "Epoch [17/1000] Loss: 0.0381 Val Loss: 0.0355\n",
      "Epoch [18/1000] Loss: 0.0378 Val Loss: 0.0350\n",
      "Epoch [19/1000] Loss: 0.0374 Val Loss: 0.0348\n",
      "Epoch [20/1000] Loss: 0.0369 Val Loss: 0.0339\n",
      "Epoch [21/1000] Loss: 0.0370 Val Loss: 0.0351\n",
      "Epoch [22/1000] Loss: 0.0360 Val Loss: 0.0338\n",
      "Epoch [23/1000] Loss: 0.0366 Val Loss: 0.0334\n",
      "Epoch [24/1000] Loss: 0.0356 Val Loss: 0.0338\n",
      "Epoch [25/1000] Loss: 0.0359 Val Loss: 0.0327\n",
      "Epoch [26/1000] Loss: 0.0347 Val Loss: 0.0321\n",
      "Epoch [27/1000] Loss: 0.0351 Val Loss: 0.0327\n",
      "Epoch [28/1000] Loss: 0.0345 Val Loss: 0.0315\n",
      "Epoch [29/1000] Loss: 0.0338 Val Loss: 0.0319\n",
      "Epoch [30/1000] Loss: 0.0338 Val Loss: 0.0306\n",
      "Epoch [31/1000] Loss: 0.0334 Val Loss: 0.0299\n",
      "Epoch [32/1000] Loss: 0.0328 Val Loss: 0.0302\n",
      "Epoch [33/1000] Loss: 0.0322 Val Loss: 0.0291\n",
      "Epoch [34/1000] Loss: 0.0318 Val Loss: 0.0281\n",
      "Epoch [35/1000] Loss: 0.0321 Val Loss: 0.0280\n",
      "Epoch [36/1000] Loss: 0.0308 Val Loss: 0.0274\n",
      "Epoch [37/1000] Loss: 0.0299 Val Loss: 0.0270\n",
      "Epoch [38/1000] Loss: 0.0290 Val Loss: 0.0272\n",
      "Epoch [39/1000] Loss: 0.0302 Val Loss: 0.0266\n",
      "Epoch [40/1000] Loss: 0.0290 Val Loss: 0.0267\n",
      "Epoch [41/1000] Loss: 0.0288 Val Loss: 0.0263\n",
      "Epoch [42/1000] Loss: 0.0291 Val Loss: 0.0260\n",
      "Epoch [43/1000] Loss: 0.0284 Val Loss: 0.0248\n",
      "Epoch [44/1000] Loss: 0.0275 Val Loss: 0.0249\n",
      "Epoch [45/1000] Loss: 0.0259 Val Loss: 0.0242\n",
      "Epoch [46/1000] Loss: 0.0263 Val Loss: 0.0238\n",
      "Epoch [47/1000] Loss: 0.0266 Val Loss: 0.0242\n",
      "Epoch [48/1000] Loss: 0.0258 Val Loss: 0.0250\n",
      "Epoch [49/1000] Loss: 0.0261 Val Loss: 0.0231\n",
      "Epoch [50/1000] Loss: 0.0252 Val Loss: 0.0228\n",
      "Epoch [51/1000] Loss: 0.0250 Val Loss: 0.0217\n",
      "Epoch [52/1000] Loss: 0.0238 Val Loss: 0.0210\n",
      "Epoch [53/1000] Loss: 0.0242 Val Loss: 0.0217\n",
      "Epoch [54/1000] Loss: 0.0230 Val Loss: 0.0213\n",
      "Epoch [55/1000] Loss: 0.0227 Val Loss: 0.0200\n",
      "Epoch [56/1000] Loss: 0.0222 Val Loss: 0.0195\n",
      "Epoch [57/1000] Loss: 0.0220 Val Loss: 0.0186\n",
      "Epoch [58/1000] Loss: 0.0211 Val Loss: 0.0194\n",
      "Epoch [59/1000] Loss: 0.0215 Val Loss: 0.0209\n",
      "Epoch [60/1000] Loss: 0.0222 Val Loss: 0.0194\n",
      "Epoch [61/1000] Loss: 0.0203 Val Loss: 0.0175\n",
      "Epoch [62/1000] Loss: 0.0203 Val Loss: 0.0175\n",
      "Epoch [63/1000] Loss: 0.0193 Val Loss: 0.0169\n",
      "Epoch [64/1000] Loss: 0.0220 Val Loss: 0.0215\n",
      "Epoch [65/1000] Loss: 0.0222 Val Loss: 0.0174\n",
      "Epoch [66/1000] Loss: 0.0186 Val Loss: 0.0163\n",
      "Epoch [67/1000] Loss: 0.0190 Val Loss: 0.0162\n",
      "Epoch [68/1000] Loss: 0.0178 Val Loss: 0.0155\n",
      "Epoch [69/1000] Loss: 0.0171 Val Loss: 0.0172\n",
      "Epoch [70/1000] Loss: 0.0185 Val Loss: 0.0157\n",
      "Epoch [71/1000] Loss: 0.0167 Val Loss: 0.0146\n",
      "Epoch [72/1000] Loss: 0.0157 Val Loss: 0.0148\n",
      "Epoch [73/1000] Loss: 0.0158 Val Loss: 0.0122\n",
      "Epoch [74/1000] Loss: 0.0148 Val Loss: 0.0130\n",
      "Epoch [75/1000] Loss: 0.0140 Val Loss: 0.0123\n",
      "Epoch [76/1000] Loss: 0.0143 Val Loss: 0.0113\n",
      "Epoch [77/1000] Loss: 0.0151 Val Loss: 0.0118\n",
      "Epoch [78/1000] Loss: 0.0143 Val Loss: 0.0121\n",
      "Epoch [79/1000] Loss: 0.0122 Val Loss: 0.0121\n",
      "Epoch [80/1000] Loss: 0.0153 Val Loss: 0.0123\n",
      "Epoch [81/1000] Loss: 0.0142 Val Loss: 0.0111\n",
      "Epoch [82/1000] Loss: 0.0119 Val Loss: 0.0092\n",
      "Epoch [83/1000] Loss: 0.0105 Val Loss: 0.0097\n",
      "Epoch [84/1000] Loss: 0.0096 Val Loss: 0.0080\n",
      "Epoch [85/1000] Loss: 0.0101 Val Loss: 0.0089\n",
      "Epoch [86/1000] Loss: 0.0092 Val Loss: 0.0079\n",
      "Epoch [87/1000] Loss: 0.0102 Val Loss: 0.0079\n",
      "Epoch [88/1000] Loss: 0.0093 Val Loss: 0.0101\n",
      "Epoch [89/1000] Loss: 0.0139 Val Loss: 0.0110\n",
      "Epoch [90/1000] Loss: 0.0153 Val Loss: 0.0113\n",
      "Epoch [91/1000] Loss: 0.0118 Val Loss: 0.0091\n",
      "Epoch [92/1000] Loss: 0.0093 Val Loss: 0.0085\n",
      "Epoch [93/1000] Loss: 0.0095 Val Loss: 0.0106\n",
      "Epoch [94/1000] Loss: 0.0117 Val Loss: 0.0088\n",
      "Epoch [95/1000] Loss: 0.0096 Val Loss: 0.0067\n",
      "Epoch [96/1000] Loss: 0.0077 Val Loss: 0.0075\n",
      "Epoch [97/1000] Loss: 0.0073 Val Loss: 0.0053\n",
      "Epoch [98/1000] Loss: 0.0067 Val Loss: 0.0072\n",
      "Epoch [99/1000] Loss: 0.0092 Val Loss: 0.0099\n",
      "Epoch [100/1000] Loss: 0.0095 Val Loss: 0.0067\n",
      "Epoch [101/1000] Loss: 0.0070 Val Loss: 0.0074\n",
      "Epoch [102/1000] Loss: 0.0076 Val Loss: 0.0051\n",
      "Epoch [103/1000] Loss: 0.0070 Val Loss: 0.0090\n",
      "Epoch [104/1000] Loss: 0.0090 Val Loss: 0.0055\n",
      "Epoch [105/1000] Loss: 0.0056 Val Loss: 0.0043\n",
      "Epoch [106/1000] Loss: 0.0044 Val Loss: 0.0032\n",
      "Epoch [107/1000] Loss: 0.0038 Val Loss: 0.0035\n",
      "Epoch [108/1000] Loss: 0.0045 Val Loss: 0.0042\n",
      "Epoch [109/1000] Loss: 0.0043 Val Loss: 0.0037\n",
      "Epoch [110/1000] Loss: 0.0040 Val Loss: 0.0029\n",
      "Epoch [111/1000] Loss: 0.0038 Val Loss: 0.0032\n",
      "Epoch [112/1000] Loss: 0.0033 Val Loss: 0.0036\n",
      "Epoch [113/1000] Loss: 0.0033 Val Loss: 0.0027\n",
      "Epoch [114/1000] Loss: 0.0030 Val Loss: 0.0029\n",
      "Epoch [115/1000] Loss: 0.0029 Val Loss: 0.0024\n",
      "Epoch [116/1000] Loss: 0.0025 Val Loss: 0.0025\n",
      "Epoch [117/1000] Loss: 0.0027 Val Loss: 0.0025\n",
      "Epoch [118/1000] Loss: 0.0026 Val Loss: 0.0023\n",
      "Epoch [119/1000] Loss: 0.0031 Val Loss: 0.0027\n",
      "Epoch [120/1000] Loss: 0.0028 Val Loss: 0.0030\n",
      "Epoch [121/1000] Loss: 0.0066 Val Loss: 0.0057\n",
      "Epoch [122/1000] Loss: 0.0091 Val Loss: 0.0045\n",
      "Epoch [123/1000] Loss: 0.0106 Val Loss: 0.0055\n",
      "Epoch [124/1000] Loss: 0.0066 Val Loss: 0.0052\n",
      "Epoch [125/1000] Loss: 0.0039 Val Loss: 0.0024\n",
      "Epoch [126/1000] Loss: 0.0032 Val Loss: 0.0035\n",
      "Epoch [127/1000] Loss: 0.0037 Val Loss: 0.0034\n",
      "Epoch [128/1000] Loss: 0.0033 Val Loss: 0.0048\n",
      "Epoch [129/1000] Loss: 0.0041 Val Loss: 0.0035\n",
      "Epoch [130/1000] Loss: 0.0039 Val Loss: 0.0037\n",
      "Epoch [131/1000] Loss: 0.0047 Val Loss: 0.0039\n",
      "Epoch [132/1000] Loss: 0.0053 Val Loss: 0.0094\n",
      "Epoch [133/1000] Loss: 0.0066 Val Loss: 0.0094\n",
      "Epoch [134/1000] Loss: 0.0086 Val Loss: 0.0033\n",
      "Epoch [135/1000] Loss: 0.0056 Val Loss: 0.0029\n",
      "Epoch [136/1000] Loss: 0.0039 Val Loss: 0.0027\n",
      "Epoch [137/1000] Loss: 0.0034 Val Loss: 0.0029\n",
      "Epoch [138/1000] Loss: 0.0028 Val Loss: 0.0030\n",
      "Epoch [139/1000] Loss: 0.0026 Val Loss: 0.0031\n",
      "Epoch [140/1000] Loss: 0.0029 Val Loss: 0.0020\n",
      "Epoch [141/1000] Loss: 0.0022 Val Loss: 0.0017\n",
      "Epoch [142/1000] Loss: 0.0017 Val Loss: 0.0014\n",
      "Epoch [143/1000] Loss: 0.0015 Val Loss: 0.0014\n",
      "Epoch [144/1000] Loss: 0.0014 Val Loss: 0.0013\n",
      "Epoch [145/1000] Loss: 0.0014 Val Loss: 0.0011\n",
      "Epoch [146/1000] Loss: 0.0013 Val Loss: 0.0010\n",
      "Epoch [147/1000] Loss: 0.0012 Val Loss: 0.0011\n",
      "Epoch [148/1000] Loss: 0.0011 Val Loss: 0.0010\n",
      "Epoch [149/1000] Loss: 0.0012 Val Loss: 0.0010\n",
      "Epoch [150/1000] Loss: 0.0011 Val Loss: 0.0010\n",
      "Epoch [151/1000] Loss: 0.0011 Val Loss: 0.0011\n",
      "Epoch [152/1000] Loss: 0.0013 Val Loss: 0.0012\n",
      "Epoch [153/1000] Loss: 0.0014 Val Loss: 0.0010\n",
      "Epoch [154/1000] Loss: 0.0012 Val Loss: 0.0010\n",
      "Epoch [155/1000] Loss: 0.0018 Val Loss: 0.0014\n",
      "Epoch [156/1000] Loss: 0.0015 Val Loss: 0.0012\n",
      "Epoch [157/1000] Loss: 0.0012 Val Loss: 0.0009\n",
      "Epoch [158/1000] Loss: 0.0012 Val Loss: 0.0011\n",
      "Epoch [159/1000] Loss: 0.0012 Val Loss: 0.0013\n",
      "Epoch [160/1000] Loss: 0.0014 Val Loss: 0.0012\n",
      "Epoch [161/1000] Loss: 0.0027 Val Loss: 0.0017\n",
      "Epoch [162/1000] Loss: 0.0071 Val Loss: 0.0090\n",
      "Epoch [163/1000] Loss: 0.0113 Val Loss: 0.0060\n",
      "Epoch [164/1000] Loss: 0.0070 Val Loss: 0.0059\n",
      "Epoch [165/1000] Loss: 0.0046 Val Loss: 0.0066\n",
      "Epoch [166/1000] Loss: 0.0072 Val Loss: 0.0052\n",
      "Epoch [167/1000] Loss: 0.0055 Val Loss: 0.0076\n",
      "Epoch [168/1000] Loss: 0.0064 Val Loss: 0.0082\n",
      "Epoch [169/1000] Loss: 0.0081 Val Loss: 0.0077\n",
      "Epoch [170/1000] Loss: 0.0070 Val Loss: 0.0067\n",
      "Epoch [171/1000] Loss: 0.0090 Val Loss: 0.0059\n",
      "Epoch [172/1000] Loss: 0.0082 Val Loss: 0.0083\n",
      "Epoch [173/1000] Loss: 0.0074 Val Loss: 0.0037\n",
      "Epoch [174/1000] Loss: 0.0042 Val Loss: 0.0032\n",
      "Epoch [175/1000] Loss: 0.0037 Val Loss: 0.0019\n",
      "Epoch [176/1000] Loss: 0.0019 Val Loss: 0.0016\n",
      "Epoch [177/1000] Loss: 0.0016 Val Loss: 0.0011\n",
      "Epoch [178/1000] Loss: 0.0013 Val Loss: 0.0012\n",
      "Epoch [179/1000] Loss: 0.0013 Val Loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000] Loss: 0.0012 Val Loss: 0.0010\n",
      "Epoch [181/1000] Loss: 0.0012 Val Loss: 0.0009\n",
      "Epoch [182/1000] Loss: 0.0012 Val Loss: 0.0008\n",
      "Epoch [183/1000] Loss: 0.0010 Val Loss: 0.0010\n",
      "Epoch [184/1000] Loss: 0.0011 Val Loss: 0.0010\n",
      "Epoch [185/1000] Loss: 0.0013 Val Loss: 0.0009\n",
      "Epoch [186/1000] Loss: 0.0013 Val Loss: 0.0008\n",
      "Epoch [187/1000] Loss: 0.0012 Val Loss: 0.0010\n",
      "Epoch [188/1000] Loss: 0.0013 Val Loss: 0.0014\n",
      "Epoch [189/1000] Loss: 0.0011 Val Loss: 0.0012\n",
      "Epoch [190/1000] Loss: 0.0011 Val Loss: 0.0013\n",
      "Epoch [191/1000] Loss: 0.0010 Val Loss: 0.0012\n",
      "Epoch [192/1000] Loss: 0.0010 Val Loss: 0.0009\n",
      "Epoch [193/1000] Loss: 0.0012 Val Loss: 0.0008\n",
      "Epoch [194/1000] Loss: 0.0009 Val Loss: 0.0008\n",
      "Epoch [195/1000] Loss: 0.0009 Val Loss: 0.0007\n",
      "Epoch [196/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [197/1000] Loss: 0.0009 Val Loss: 0.0013\n",
      "Epoch [198/1000] Loss: 0.0009 Val Loss: 0.0010\n",
      "Epoch [199/1000] Loss: 0.0013 Val Loss: 0.0008\n",
      "Epoch [200/1000] Loss: 0.0010 Val Loss: 0.0019\n",
      "Epoch [201/1000] Loss: 0.0011 Val Loss: 0.0009\n",
      "Epoch [202/1000] Loss: 0.0012 Val Loss: 0.0012\n",
      "Epoch [203/1000] Loss: 0.0018 Val Loss: 0.0009\n",
      "Epoch [204/1000] Loss: 0.0009 Val Loss: 0.0008\n",
      "Epoch [205/1000] Loss: 0.0009 Val Loss: 0.0008\n",
      "Epoch [206/1000] Loss: 0.0012 Val Loss: 0.0013\n",
      "Epoch [207/1000] Loss: 0.0013 Val Loss: 0.0013\n",
      "Epoch [208/1000] Loss: 0.0011 Val Loss: 0.0011\n",
      "Epoch [209/1000] Loss: 0.0011 Val Loss: 0.0011\n",
      "Epoch [210/1000] Loss: 0.0011 Val Loss: 0.0013\n",
      "Epoch [211/1000] Loss: 0.0012 Val Loss: 0.0011\n",
      "Epoch [212/1000] Loss: 0.0012 Val Loss: 0.0010\n",
      "Epoch [213/1000] Loss: 0.0012 Val Loss: 0.0014\n",
      "Epoch [214/1000] Loss: 0.0022 Val Loss: 0.0022\n",
      "Epoch [215/1000] Loss: 0.0013 Val Loss: 0.0024\n",
      "Epoch [216/1000] Loss: 0.0012 Val Loss: 0.0011\n",
      "Epoch [217/1000] Loss: 0.0018 Val Loss: 0.0009\n",
      "Epoch [218/1000] Loss: 0.0014 Val Loss: 0.0012\n",
      "Epoch [219/1000] Loss: 0.0015 Val Loss: 0.0012\n",
      "Epoch [220/1000] Loss: 0.0009 Val Loss: 0.0017\n",
      "Epoch [221/1000] Loss: 0.0014 Val Loss: 0.0010\n",
      "Epoch [222/1000] Loss: 0.0009 Val Loss: 0.0008\n",
      "Epoch [223/1000] Loss: 0.0014 Val Loss: 0.0008\n",
      "Epoch [224/1000] Loss: 0.0009 Val Loss: 0.0007\n",
      "Epoch [225/1000] Loss: 0.0010 Val Loss: 0.0008\n",
      "Epoch [226/1000] Loss: 0.0007 Val Loss: 0.0007\n",
      "Epoch [227/1000] Loss: 0.0008 Val Loss: 0.0008\n",
      "Epoch [228/1000] Loss: 0.0010 Val Loss: 0.0012\n",
      "Epoch [229/1000] Loss: 0.0009 Val Loss: 0.0010\n",
      "Epoch [230/1000] Loss: 0.0013 Val Loss: 0.0010\n",
      "Epoch [231/1000] Loss: 0.0011 Val Loss: 0.0014\n",
      "Epoch [232/1000] Loss: 0.0017 Val Loss: 0.0022\n",
      "Epoch [233/1000] Loss: 0.0020 Val Loss: 0.0030\n",
      "Epoch [234/1000] Loss: 0.0043 Val Loss: 0.0043\n",
      "Epoch [235/1000] Loss: 0.0122 Val Loss: 0.0296\n",
      "Epoch [236/1000] Loss: 0.0341 Val Loss: 0.0223\n",
      "Epoch [237/1000] Loss: 0.0208 Val Loss: 0.0123\n",
      "Epoch [238/1000] Loss: 0.0103 Val Loss: 0.0062\n",
      "Epoch [239/1000] Loss: 0.0058 Val Loss: 0.0039\n",
      "Epoch [240/1000] Loss: 0.0036 Val Loss: 0.0028\n",
      "Epoch [241/1000] Loss: 0.0023 Val Loss: 0.0015\n",
      "Epoch [242/1000] Loss: 0.0017 Val Loss: 0.0012\n",
      "Epoch [243/1000] Loss: 0.0014 Val Loss: 0.0011\n",
      "Epoch [244/1000] Loss: 0.0014 Val Loss: 0.0010\n",
      "Epoch [245/1000] Loss: 0.0010 Val Loss: 0.0008\n",
      "Epoch [246/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [247/1000] Loss: 0.0008 Val Loss: 0.0006\n",
      "Epoch [248/1000] Loss: 0.0007 Val Loss: 0.0006\n",
      "Epoch [249/1000] Loss: 0.0007 Val Loss: 0.0007\n",
      "Epoch [250/1000] Loss: 0.0007 Val Loss: 0.0013\n",
      "Epoch [251/1000] Loss: 0.0010 Val Loss: 0.0014\n",
      "Epoch [252/1000] Loss: 0.0017 Val Loss: 0.0008\n",
      "Epoch [253/1000] Loss: 0.0008 Val Loss: 0.0006\n",
      "Epoch [254/1000] Loss: 0.0012 Val Loss: 0.0011\n",
      "Epoch [255/1000] Loss: 0.0017 Val Loss: 0.0012\n",
      "Epoch [256/1000] Loss: 0.0014 Val Loss: 0.0007\n",
      "Epoch [257/1000] Loss: 0.0014 Val Loss: 0.0010\n",
      "Epoch [258/1000] Loss: 0.0013 Val Loss: 0.0009\n",
      "Epoch [259/1000] Loss: 0.0010 Val Loss: 0.0006\n",
      "Epoch [260/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [261/1000] Loss: 0.0013 Val Loss: 0.0007\n",
      "Epoch [262/1000] Loss: 0.0006 Val Loss: 0.0008\n",
      "Epoch [263/1000] Loss: 0.0007 Val Loss: 0.0007\n",
      "Epoch [264/1000] Loss: 0.0009 Val Loss: 0.0010\n",
      "Epoch [265/1000] Loss: 0.0010 Val Loss: 0.0008\n",
      "Epoch [266/1000] Loss: 0.0010 Val Loss: 0.0008\n",
      "Epoch [267/1000] Loss: 0.0008 Val Loss: 0.0006\n",
      "Epoch [268/1000] Loss: 0.0013 Val Loss: 0.0011\n",
      "Epoch [269/1000] Loss: 0.0016 Val Loss: 0.0019\n",
      "Epoch [270/1000] Loss: 0.0014 Val Loss: 0.0010\n",
      "Epoch [271/1000] Loss: 0.0021 Val Loss: 0.0040\n",
      "Epoch [272/1000] Loss: 0.0052 Val Loss: 0.0022\n",
      "Epoch [273/1000] Loss: 0.0129 Val Loss: 0.0086\n",
      "Epoch [274/1000] Loss: 0.0186 Val Loss: 0.0164\n",
      "Epoch [275/1000] Loss: 0.0196 Val Loss: 0.0109\n",
      "Epoch [276/1000] Loss: 0.0078 Val Loss: 0.0045\n",
      "Epoch [277/1000] Loss: 0.0037 Val Loss: 0.0027\n",
      "Epoch [278/1000] Loss: 0.0020 Val Loss: 0.0017\n",
      "Epoch [279/1000] Loss: 0.0016 Val Loss: 0.0011\n",
      "Epoch [280/1000] Loss: 0.0011 Val Loss: 0.0009\n",
      "Epoch [281/1000] Loss: 0.0009 Val Loss: 0.0007\n",
      "Epoch [282/1000] Loss: 0.0008 Val Loss: 0.0006\n",
      "Epoch [283/1000] Loss: 0.0007 Val Loss: 0.0007\n",
      "Epoch [284/1000] Loss: 0.0007 Val Loss: 0.0012\n",
      "Epoch [285/1000] Loss: 0.0011 Val Loss: 0.0010\n",
      "Epoch [286/1000] Loss: 0.0016 Val Loss: 0.0008\n",
      "Epoch [287/1000] Loss: 0.0018 Val Loss: 0.0011\n",
      "Epoch [288/1000] Loss: 0.0007 Val Loss: 0.0008\n",
      "Epoch [289/1000] Loss: 0.0008 Val Loss: 0.0005\n",
      "Epoch [290/1000] Loss: 0.0008 Val Loss: 0.0005\n",
      "Epoch [291/1000] Loss: 0.0009 Val Loss: 0.0007\n",
      "Epoch [292/1000] Loss: 0.0011 Val Loss: 0.0009\n",
      "Epoch [293/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [294/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [295/1000] Loss: 0.0006 Val Loss: 0.0007\n",
      "Epoch [296/1000] Loss: 0.0009 Val Loss: 0.0006\n",
      "Epoch [297/1000] Loss: 0.0010 Val Loss: 0.0012\n",
      "Epoch [298/1000] Loss: 0.0009 Val Loss: 0.0010\n",
      "Epoch [299/1000] Loss: 0.0009 Val Loss: 0.0007\n",
      "Epoch [300/1000] Loss: 0.0011 Val Loss: 0.0006\n",
      "Epoch [301/1000] Loss: 0.0005 Val Loss: 0.0005\n",
      "Epoch [302/1000] Loss: 0.0006 Val Loss: 0.0006\n",
      "Epoch [303/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [304/1000] Loss: 0.0005 Val Loss: 0.0005\n",
      "Epoch [305/1000] Loss: 0.0006 Val Loss: 0.0005\n",
      "Epoch [306/1000] Loss: 0.0009 Val Loss: 0.0010\n",
      "Epoch [307/1000] Loss: 0.0008 Val Loss: 0.0005\n",
      "Epoch [308/1000] Loss: 0.0009 Val Loss: 0.0009\n",
      "Epoch [309/1000] Loss: 0.0008 Val Loss: 0.0007\n",
      "Epoch [310/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [311/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [312/1000] Loss: 0.0007 Val Loss: 0.0009\n",
      "Epoch [313/1000] Loss: 0.0007 Val Loss: 0.0006\n",
      "Epoch [314/1000] Loss: 0.0004 Val Loss: 0.0005\n",
      "Epoch [315/1000] Loss: 0.0013 Val Loss: 0.0014\n",
      "Epoch [316/1000] Loss: 0.0011 Val Loss: 0.0010\n",
      "Epoch [317/1000] Loss: 0.0011 Val Loss: 0.0007\n",
      "Epoch [318/1000] Loss: 0.0005 Val Loss: 0.0006\n",
      "Epoch [319/1000] Loss: 0.0005 Val Loss: 0.0006\n",
      "Epoch [320/1000] Loss: 0.0013 Val Loss: 0.0007\n",
      "Epoch [321/1000] Loss: 0.0024 Val Loss: 0.0009\n",
      "Epoch [322/1000] Loss: 0.0020 Val Loss: 0.0032\n",
      "Epoch [323/1000] Loss: 0.0078 Val Loss: 0.0034\n",
      "Epoch [324/1000] Loss: 0.0041 Val Loss: 0.0037\n",
      "Epoch [325/1000] Loss: 0.0054 Val Loss: 0.0047\n",
      "Epoch [326/1000] Loss: 0.0051 Val Loss: 0.0030\n",
      "Epoch [327/1000] Loss: 0.0090 Val Loss: 0.0106\n",
      "Epoch [328/1000] Loss: 0.0110 Val Loss: 0.0051\n",
      "Epoch [329/1000] Loss: 0.0043 Val Loss: 0.0042\n",
      "Epoch [330/1000] Loss: 0.0032 Val Loss: 0.0030\n",
      "Epoch [331/1000] Loss: 0.0026 Val Loss: 0.0016\n",
      "Epoch [332/1000] Loss: 0.0036 Val Loss: 0.0021\n",
      "Epoch [333/1000] Loss: 0.0020 Val Loss: 0.0008\n",
      "Epoch [334/1000] Loss: 0.0009 Val Loss: 0.0005\n",
      "Epoch [335/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [336/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [337/1000] Loss: 0.0004 Val Loss: 0.0003\n",
      "Epoch [338/1000] Loss: 0.0005 Val Loss: 0.0003\n",
      "Epoch [339/1000] Loss: 0.0005 Val Loss: 0.0004\n",
      "Epoch [340/1000] Loss: 0.0006 Val Loss: 0.0003\n",
      "Epoch [341/1000] Loss: 0.0004 Val Loss: 0.0003\n",
      "Epoch [342/1000] Loss: 0.0005 Val Loss: 0.0003\n",
      "Epoch [343/1000] Loss: 0.0003 Val Loss: 0.0003\n",
      "Epoch [344/1000] Loss: 0.0004 Val Loss: 0.0002\n",
      "Epoch [345/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [346/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [347/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [348/1000] Loss: 0.0003 Val Loss: 0.0003\n",
      "Epoch [349/1000] Loss: 0.0003 Val Loss: 0.0005\n",
      "Epoch [350/1000] Loss: 0.0009 Val Loss: 0.0004\n",
      "Epoch [351/1000] Loss: 0.0016 Val Loss: 0.0009\n",
      "Epoch [352/1000] Loss: 0.0012 Val Loss: 0.0008\n",
      "Epoch [353/1000] Loss: 0.0009 Val Loss: 0.0006\n",
      "Epoch [354/1000] Loss: 0.0005 Val Loss: 0.0008\n",
      "Epoch [355/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [356/1000] Loss: 0.0004 Val Loss: 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [357/1000] Loss: 0.0004 Val Loss: 0.0003\n",
      "Epoch [358/1000] Loss: 0.0005 Val Loss: 0.0009\n",
      "Epoch [359/1000] Loss: 0.0010 Val Loss: 0.0015\n",
      "Epoch [360/1000] Loss: 0.0024 Val Loss: 0.0017\n",
      "Epoch [361/1000] Loss: 0.0011 Val Loss: 0.0009\n",
      "Epoch [362/1000] Loss: 0.0008 Val Loss: 0.0006\n",
      "Epoch [363/1000] Loss: 0.0007 Val Loss: 0.0004\n",
      "Epoch [364/1000] Loss: 0.0004 Val Loss: 0.0003\n",
      "Epoch [365/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [366/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [367/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [368/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [369/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [370/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [371/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [372/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [373/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [374/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [375/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [376/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [377/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [378/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [379/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [380/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [381/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [382/1000] Loss: 0.0006 Val Loss: 0.0001\n",
      "Epoch [383/1000] Loss: 0.0011 Val Loss: 0.0008\n",
      "Epoch [384/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [385/1000] Loss: 0.0005 Val Loss: 0.0006\n",
      "Epoch [386/1000] Loss: 0.0006 Val Loss: 0.0005\n",
      "Epoch [387/1000] Loss: 0.0005 Val Loss: 0.0005\n",
      "Epoch [388/1000] Loss: 0.0005 Val Loss: 0.0004\n",
      "Epoch [389/1000] Loss: 0.0005 Val Loss: 0.0003\n",
      "Epoch [390/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [391/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [392/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [393/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [394/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [395/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [396/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [397/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [398/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [399/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [400/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [401/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [402/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [403/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [404/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [405/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [406/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [407/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [408/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [409/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [410/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [411/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [412/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [413/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [414/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [415/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [416/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [417/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [418/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [419/1000] Loss: 0.0003 Val Loss: 0.0005\n",
      "Epoch [420/1000] Loss: 0.0004 Val Loss: 0.0006\n",
      "Epoch [421/1000] Loss: 0.0008 Val Loss: 0.0005\n",
      "Epoch [422/1000] Loss: 0.0006 Val Loss: 0.0009\n",
      "Epoch [423/1000] Loss: 0.0012 Val Loss: 0.0056\n",
      "Epoch [424/1000] Loss: 0.0121 Val Loss: 0.0096\n",
      "Epoch [425/1000] Loss: 0.0121 Val Loss: 0.0127\n",
      "Epoch [426/1000] Loss: 0.0139 Val Loss: 0.0080\n",
      "Epoch [427/1000] Loss: 0.0098 Val Loss: 0.0080\n",
      "Epoch [428/1000] Loss: 0.0090 Val Loss: 0.0065\n",
      "Epoch [429/1000] Loss: 0.0059 Val Loss: 0.0037\n",
      "Epoch [430/1000] Loss: 0.0078 Val Loss: 0.0035\n",
      "Epoch [431/1000] Loss: 0.0083 Val Loss: 0.0199\n",
      "Epoch [432/1000] Loss: 0.0218 Val Loss: 0.0140\n",
      "Epoch [433/1000] Loss: 0.0117 Val Loss: 0.0154\n",
      "Epoch [434/1000] Loss: 0.0092 Val Loss: 0.0035\n",
      "Epoch [435/1000] Loss: 0.0037 Val Loss: 0.0013\n",
      "Epoch [436/1000] Loss: 0.0018 Val Loss: 0.0008\n",
      "Epoch [437/1000] Loss: 0.0011 Val Loss: 0.0006\n",
      "Epoch [438/1000] Loss: 0.0014 Val Loss: 0.0011\n",
      "Epoch [439/1000] Loss: 0.0009 Val Loss: 0.0009\n",
      "Epoch [440/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [441/1000] Loss: 0.0004 Val Loss: 0.0003\n",
      "Epoch [442/1000] Loss: 0.0005 Val Loss: 0.0007\n",
      "Epoch [443/1000] Loss: 0.0008 Val Loss: 0.0016\n",
      "Epoch [444/1000] Loss: 0.0007 Val Loss: 0.0006\n",
      "Epoch [445/1000] Loss: 0.0024 Val Loss: 0.0011\n",
      "Epoch [446/1000] Loss: 0.0005 Val Loss: 0.0016\n",
      "Epoch [447/1000] Loss: 0.0014 Val Loss: 0.0009\n",
      "Epoch [448/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [449/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [450/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [451/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [452/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [453/1000] Loss: 0.0002 Val Loss: 0.0003\n",
      "Epoch [454/1000] Loss: 0.0009 Val Loss: 0.0003\n",
      "Epoch [455/1000] Loss: 0.0005 Val Loss: 0.0001\n",
      "Epoch [456/1000] Loss: 0.0003 Val Loss: 0.0001\n",
      "Epoch [457/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [458/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [459/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [460/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [461/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [462/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [463/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [464/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [465/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [466/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [467/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [468/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [469/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [470/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [471/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [472/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [473/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [474/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [475/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [476/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [477/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [478/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [479/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [480/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [481/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [482/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [483/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [484/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [485/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [486/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [487/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [488/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [489/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [490/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [491/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [492/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [493/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [494/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [495/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [496/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [497/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [498/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [499/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [500/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [501/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [502/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [503/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [504/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [505/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [506/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [507/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [508/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [509/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [510/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [511/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [512/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [513/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [514/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [515/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [516/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [517/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [518/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [519/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [520/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [521/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [522/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [523/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [524/1000] Loss: 0.0002 Val Loss: 0.0003\n",
      "Epoch [525/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [526/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [527/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [528/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [529/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [530/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [531/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [532/1000] Loss: 0.0002 Val Loss: 0.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [533/1000] Loss: 0.0002 Val Loss: 0.0003\n",
      "Epoch [534/1000] Loss: 0.0009 Val Loss: 0.0031\n",
      "Epoch [535/1000] Loss: 0.0091 Val Loss: 0.0176\n",
      "Epoch [536/1000] Loss: 0.0234 Val Loss: 0.0186\n",
      "Epoch [537/1000] Loss: 0.0169 Val Loss: 0.0113\n",
      "Epoch [538/1000] Loss: 0.0080 Val Loss: 0.0055\n",
      "Epoch [539/1000] Loss: 0.0061 Val Loss: 0.0076\n",
      "Epoch [540/1000] Loss: 0.0142 Val Loss: 0.0086\n",
      "Epoch [541/1000] Loss: 0.0087 Val Loss: 0.0046\n",
      "Epoch [542/1000] Loss: 0.0056 Val Loss: 0.0061\n",
      "Epoch [543/1000] Loss: 0.0041 Val Loss: 0.0021\n",
      "Epoch [544/1000] Loss: 0.0017 Val Loss: 0.0009\n",
      "Epoch [545/1000] Loss: 0.0009 Val Loss: 0.0005\n",
      "Epoch [546/1000] Loss: 0.0005 Val Loss: 0.0003\n",
      "Epoch [547/1000] Loss: 0.0004 Val Loss: 0.0005\n",
      "Epoch [548/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [549/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [550/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [551/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [552/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [553/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [554/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [555/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [556/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [557/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [558/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [559/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [560/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [561/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [562/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [563/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [564/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [565/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [566/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [567/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [568/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [569/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [570/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [571/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [572/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [573/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [574/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [575/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [576/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [577/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [578/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [579/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [580/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [581/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [582/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [583/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [584/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [585/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [586/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [587/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [588/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [589/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [590/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [591/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [592/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [593/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [594/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [595/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [596/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [597/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [598/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [599/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [600/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [601/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [602/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [603/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [604/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [605/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [606/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [607/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [608/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [609/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [610/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [611/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [612/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [613/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [614/1000] Loss: 0.0000 Val Loss: 0.0001\n",
      "Epoch [615/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [616/1000] Loss: 0.0003 Val Loss: 0.0004\n",
      "Epoch [617/1000] Loss: 0.0052 Val Loss: 0.0104\n",
      "Epoch [618/1000] Loss: 0.0168 Val Loss: 0.0079\n",
      "Epoch [619/1000] Loss: 0.0085 Val Loss: 0.0053\n",
      "Epoch [620/1000] Loss: 0.0056 Val Loss: 0.0055\n",
      "Epoch [621/1000] Loss: 0.0066 Val Loss: 0.0080\n",
      "Epoch [622/1000] Loss: 0.0094 Val Loss: 0.0065\n",
      "Epoch [623/1000] Loss: 0.0054 Val Loss: 0.0043\n",
      "Epoch [624/1000] Loss: 0.0062 Val Loss: 0.0039\n",
      "Epoch [625/1000] Loss: 0.0111 Val Loss: 0.0121\n",
      "Epoch [626/1000] Loss: 0.0067 Val Loss: 0.0039\n",
      "Epoch [627/1000] Loss: 0.0040 Val Loss: 0.0029\n",
      "Epoch [628/1000] Loss: 0.0026 Val Loss: 0.0012\n",
      "Epoch [629/1000] Loss: 0.0010 Val Loss: 0.0006\n",
      "Epoch [630/1000] Loss: 0.0006 Val Loss: 0.0004\n",
      "Epoch [631/1000] Loss: 0.0004 Val Loss: 0.0004\n",
      "Epoch [632/1000] Loss: 0.0005 Val Loss: 0.0002\n",
      "Epoch [633/1000] Loss: 0.0005 Val Loss: 0.0010\n",
      "Epoch [634/1000] Loss: 0.0012 Val Loss: 0.0004\n",
      "Epoch [635/1000] Loss: 0.0004 Val Loss: 0.0001\n",
      "Epoch [636/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [637/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [638/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [639/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [640/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [641/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [642/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [643/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [644/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [645/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [646/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [647/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [648/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [649/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [650/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [651/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [652/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [653/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [654/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [655/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [656/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [657/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [658/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [659/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [660/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [661/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [662/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [663/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [664/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [665/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [666/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [667/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [668/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [669/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [670/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [671/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [672/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [673/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [674/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [675/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [676/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [677/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [678/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [679/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [680/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [681/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [682/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [683/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [684/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [685/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [686/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [687/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [688/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [689/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [690/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [691/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [692/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [693/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [694/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [695/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [696/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [697/1000] Loss: 0.0000 Val Loss: 0.0001\n",
      "Epoch [698/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [699/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [700/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [701/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [702/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [703/1000] Loss: 0.0002 Val Loss: 0.0006\n",
      "Epoch [704/1000] Loss: 0.0003 Val Loss: 0.0009\n",
      "Epoch [705/1000] Loss: 0.0005 Val Loss: 0.0003\n",
      "Epoch [706/1000] Loss: 0.0004 Val Loss: 0.0005\n",
      "Epoch [707/1000] Loss: 0.0015 Val Loss: 0.0025\n",
      "Epoch [708/1000] Loss: 0.0040 Val Loss: 0.0110\n",
      "Epoch [709/1000] Loss: 0.0140 Val Loss: 0.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [710/1000] Loss: 0.0146 Val Loss: 0.0054\n",
      "Epoch [711/1000] Loss: 0.0055 Val Loss: 0.0050\n",
      "Epoch [712/1000] Loss: 0.0048 Val Loss: 0.0066\n",
      "Epoch [713/1000] Loss: 0.0044 Val Loss: 0.0044\n",
      "Epoch [714/1000] Loss: 0.0035 Val Loss: 0.0015\n",
      "Epoch [715/1000] Loss: 0.0015 Val Loss: 0.0007\n",
      "Epoch [716/1000] Loss: 0.0010 Val Loss: 0.0004\n",
      "Epoch [717/1000] Loss: 0.0004 Val Loss: 0.0005\n",
      "Epoch [718/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [719/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [720/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [721/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [722/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [723/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [724/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [725/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [726/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [727/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [728/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [729/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [730/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [731/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [732/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [733/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [734/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [735/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [736/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [737/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [738/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [739/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [740/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [741/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [742/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [743/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [744/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [745/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [746/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [747/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [748/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [749/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [750/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [751/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [752/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [753/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [754/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [755/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [756/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [757/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [758/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [759/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [760/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [761/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [762/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [763/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [764/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [765/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [766/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [767/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [768/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [769/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [770/1000] Loss: 0.0000 Val Loss: 0.0001\n",
      "Epoch [771/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [772/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [773/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [774/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [775/1000] Loss: 0.0003 Val Loss: 0.0011\n",
      "Epoch [776/1000] Loss: 0.0010 Val Loss: 0.0025\n",
      "Epoch [777/1000] Loss: 0.0064 Val Loss: 0.0222\n",
      "Epoch [778/1000] Loss: 0.0365 Val Loss: 0.0156\n",
      "Epoch [779/1000] Loss: 0.0184 Val Loss: 0.0099\n",
      "Epoch [780/1000] Loss: 0.0126 Val Loss: 0.0049\n",
      "Epoch [781/1000] Loss: 0.0096 Val Loss: 0.0064\n",
      "Epoch [782/1000] Loss: 0.0057 Val Loss: 0.0027\n",
      "Epoch [783/1000] Loss: 0.0025 Val Loss: 0.0015\n",
      "Epoch [784/1000] Loss: 0.0017 Val Loss: 0.0008\n",
      "Epoch [785/1000] Loss: 0.0011 Val Loss: 0.0007\n",
      "Epoch [786/1000] Loss: 0.0007 Val Loss: 0.0004\n",
      "Epoch [787/1000] Loss: 0.0003 Val Loss: 0.0002\n",
      "Epoch [788/1000] Loss: 0.0003 Val Loss: 0.0001\n",
      "Epoch [789/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [790/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [791/1000] Loss: 0.0001 Val Loss: 0.0002\n",
      "Epoch [792/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [793/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [794/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [795/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [796/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [797/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [798/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [799/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [800/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [801/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [802/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [803/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [804/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [805/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [806/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [807/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [808/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [809/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [810/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [811/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [812/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [813/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [814/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [815/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [816/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [817/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [818/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [819/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [820/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [821/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [822/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [823/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [824/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [825/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [826/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [827/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [828/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [829/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [830/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [831/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [832/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [833/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [834/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [835/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [836/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [837/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [838/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [839/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [840/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [841/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [842/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [843/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [844/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [845/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [846/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [847/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [848/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [849/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [850/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [851/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [852/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [853/1000] Loss: 0.0000 Val Loss: 0.0001\n",
      "Epoch [854/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [855/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [856/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [857/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [858/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [859/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [860/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [861/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [862/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [863/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [864/1000] Loss: 0.0035 Val Loss: 0.0054\n",
      "Epoch [865/1000] Loss: 0.0128 Val Loss: 0.0093\n",
      "Epoch [866/1000] Loss: 0.0129 Val Loss: 0.0042\n",
      "Epoch [867/1000] Loss: 0.0061 Val Loss: 0.0076\n",
      "Epoch [868/1000] Loss: 0.0056 Val Loss: 0.0022\n",
      "Epoch [869/1000] Loss: 0.0023 Val Loss: 0.0012\n",
      "Epoch [870/1000] Loss: 0.0013 Val Loss: 0.0013\n",
      "Epoch [871/1000] Loss: 0.0014 Val Loss: 0.0009\n",
      "Epoch [872/1000] Loss: 0.0007 Val Loss: 0.0016\n",
      "Epoch [873/1000] Loss: 0.0009 Val Loss: 0.0011\n",
      "Epoch [874/1000] Loss: 0.0016 Val Loss: 0.0003\n",
      "Epoch [875/1000] Loss: 0.0004 Val Loss: 0.0001\n",
      "Epoch [876/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [877/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [878/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [879/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [880/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [881/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [882/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [883/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [884/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [885/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [886/1000] Loss: 0.0000 Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [887/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [888/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [889/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [890/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [891/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [892/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [893/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [894/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [895/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [896/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [897/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [898/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [899/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [900/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [901/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [902/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [903/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [904/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [905/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [906/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [907/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [908/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [909/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [910/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [911/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [912/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [913/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [914/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [915/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [916/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [917/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [918/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [919/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [920/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [921/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [922/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [923/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [924/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [925/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [926/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [927/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [928/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [929/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [930/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [931/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [932/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [933/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [934/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [935/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [936/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [937/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [938/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [939/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [940/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [941/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [942/1000] Loss: 0.0000 Val Loss: 0.0001\n",
      "Epoch [943/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [944/1000] Loss: 0.0002 Val Loss: 0.0002\n",
      "Epoch [945/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [946/1000] Loss: 0.0002 Val Loss: 0.0003\n",
      "Epoch [947/1000] Loss: 0.0003 Val Loss: 0.0004\n",
      "Epoch [948/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [949/1000] Loss: 0.0006 Val Loss: 0.0006\n",
      "Epoch [950/1000] Loss: 0.0007 Val Loss: 0.0005\n",
      "Epoch [951/1000] Loss: 0.0010 Val Loss: 0.0015\n",
      "Epoch [952/1000] Loss: 0.0036 Val Loss: 0.0031\n",
      "Epoch [953/1000] Loss: 0.0093 Val Loss: 0.0057\n",
      "Epoch [954/1000] Loss: 0.0087 Val Loss: 0.0069\n",
      "Epoch [955/1000] Loss: 0.0137 Val Loss: 0.0118\n",
      "Epoch [956/1000] Loss: 0.0094 Val Loss: 0.0059\n",
      "Epoch [957/1000] Loss: 0.0070 Val Loss: 0.0044\n",
      "Epoch [958/1000] Loss: 0.0062 Val Loss: 0.0032\n",
      "Epoch [959/1000] Loss: 0.0044 Val Loss: 0.0015\n",
      "Epoch [960/1000] Loss: 0.0028 Val Loss: 0.0022\n",
      "Epoch [961/1000] Loss: 0.0018 Val Loss: 0.0006\n",
      "Epoch [962/1000] Loss: 0.0006 Val Loss: 0.0002\n",
      "Epoch [963/1000] Loss: 0.0002 Val Loss: 0.0001\n",
      "Epoch [964/1000] Loss: 0.0001 Val Loss: 0.0001\n",
      "Epoch [965/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [966/1000] Loss: 0.0001 Val Loss: 0.0000\n",
      "Epoch [967/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [968/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [969/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [970/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [971/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [972/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [973/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [974/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [975/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [976/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [977/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [978/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [979/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [980/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [981/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [982/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [983/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [984/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [985/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [986/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [987/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [988/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [989/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [990/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [991/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [992/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [993/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [994/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [995/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [996/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [997/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [998/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [999/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Epoch [1000/1000] Loss: 0.0000 Val Loss: 0.0000\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(12345)\n",
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,800], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "losses = []\n",
    "lowest = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "        \n",
    "    for batch in train_dl:\n",
    "        model.train()\n",
    "        inputs = batch[0].to(DEVICE)\n",
    "\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, inputs.y)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inputs= inputs.to('cpu')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "            \n",
    "    for batch in train_dl:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, inputs.y)\n",
    "        \n",
    "            inputs= inputs.to('cpu')\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    val_avg_loss = val_loss / len(train_dl)\n",
    "    \n",
    "    if lowest > val_avg_loss:\n",
    "        torch.save(model.state_dict(), 'AA_encoder.pt')\n",
    "        lowest = val_avg_loss\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Val Loss: {val_avg_loss:.4f}')\n",
    "    \n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e696221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('AA_encoder.pt'))\n",
    "with torch.no_grad():\n",
    "    AA_embeddings = {}\n",
    "    for graph in AA_graphs:\n",
    "        pred = model.encode(graph.to(DEVICE))\n",
    "        AA_embeddings.update({upper2lower[graph.label]:pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfe81375",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AA_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(AA_embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
