{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4593a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import obonet\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import Bio.PDB\n",
    "import urllib.request\n",
    "import py3Dmol\n",
    "import pylab\n",
    "import pickle as pickle\n",
    "import torch.nn as nn\n",
    "from torch.nn import Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.utils import erdos_renyi_graph\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import LayerNorm\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Bio import PDB\n",
    "from rdkit import Chem\n",
    "import blosum as bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0856970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMask(graph,indicies,num_masked):\n",
    "    size = graph.x.size()[0]\n",
    "    protein_mask = [False]*size\n",
    "    true_mask = [True] * num_masked\n",
    "    indicies_mask = [False]*(len(indicies) - num_masked)\n",
    "    design_mask = np.hstack((true_mask,indicies_mask))\n",
    "    random.shuffle(design_mask)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(protein_mask)):\n",
    "        if i in indicies:\n",
    "            protein_mask[i] = design_mask[count]\n",
    "            count += 1\n",
    "            \n",
    "    for i, j in enumerate(protein_mask):\n",
    "        if j == 1.0:\n",
    "            protein_mask[i] = True\n",
    "    \n",
    "    return protein_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d77ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5316\n"
     ]
    }
   ],
   "source": [
    "graph_list = torch.load('binding_pocket_graphs.pt')\n",
    "\n",
    "smallest = 5\n",
    "count = 0\n",
    "graph_list_clean = []\n",
    "for entry in graph_list:\n",
    "    if len(entry.designable_indicies) > smallest:\n",
    "        graph_list_clean.append(entry)\n",
    "        count += 1\n",
    "print(count)\n",
    "        \n",
    "        \n",
    "for i, graph in enumerate(graph_list_clean):\n",
    "    graph_list_clean[i].mask = createMask(graph,graph.designable_indicies,int(len(graph.designable_indicies)))\n",
    "    graph_list_clean[i].inv_mask = [not i for i in graph_list_clean[i].mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed2287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_assignment = torch.load('group_assignment_30p.pt')\n",
    "group_size = torch.load('group_size_30p.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8703523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, graph in enumerate(graph_list_clean):\n",
    "    graph_list_clean[i].group = group_assignment[graph.label]\n",
    "    #graph_list_clean[i].weight = 1.0/group_size[graph_list_clean[i].group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1049730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[74, 133], edge_index=[2, 1827], edge_attr=[1827, 114], pos=[73, 3], y=[74, 20], label='3l4w', designable_indicies=[22], mask=[74], inv_mask=[74], group='5nn6')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(graph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099e7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Attn(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels, nheads = 8, attn_dropout = 0.5, mlp_dropout = 0.0, neg_slope = 0.2):\n",
    "        super(Graph_Attn, self).__init__(node_dim=0, aggr='add')  # 'add' aggregation for summing messages\n",
    "        \n",
    "        self.in_channels = float(in_channels)\n",
    "        self.neg_slope = neg_slope\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.relu = nn.LeakyReLU(negative_slope = neg_slope)\n",
    "        \n",
    "        self.nheads = nheads\n",
    "        self.c = hidden_channels        \n",
    "        \n",
    "        self.Wq = Linear(in_channels, nheads*hidden_channels)\n",
    "        self.Wz = Linear((in_channels+edge_attr_dim), nheads*hidden_channels)\n",
    "        self.Wv = Linear((in_channels+edge_attr_dim), nheads*hidden_channels)\n",
    "        self.W0 = Linear(nheads*hidden_channels, out_channels)\n",
    "        #self.W0 = MLP(in_channels=nheads*out_channels, hidden_channels= 2*nheads*out_channels ,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = mlp_dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = self.attn_dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "        return self.W0(out.view(-1,self.nheads*self.c))\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor, index)-> Tensor:\n",
    "        rij = torch.cat([x_j , edge_attr], dim=-1)\n",
    "        qi = self.Wq(x_i).view(-1,self.nheads,self.c)\n",
    "        zij = self.Wz(rij).view(-1,self.nheads,self.c)\n",
    "        vij = self.Wv(rij).view(-1,self.nheads,self.c)\n",
    "        mij = torch.sum(qi * zij * ((1.0/self.in_channels) ** 0.5), dim = -1)\n",
    "        alphaij = softmax(mij, index)\n",
    "        alphaij = self.dropout(alphaij)\n",
    "        msg = vij*alphaij.unsqueeze(-1)\n",
    "        return  msg\n",
    "    \n",
    "x = torch.rand(14,7)\n",
    "edge_index  = erdos_renyi_graph(14, 0.5)\n",
    "edge_attr = torch.rand(edge_index.size()[1], 5)\n",
    "\n",
    "graph = Data(x = x, edge_index = edge_index, edge_attr = edge_attr)\n",
    "model = Graph_Attn(7,5,10,7)\n",
    "out = model(graph.x,graph.edge_index,graph.edge_attr)\n",
    "#print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f25c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Conv_nodes(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels):\n",
    "        super(MLP_Conv_nodes, self).__init__(aggr='add')  # 'add' aggregation for summing messages\n",
    "        #self.mlp = nn.Sequential(\n",
    "        #    nn.Linear(2*in_channels + edge_attr_dim, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, out_channels)\n",
    "        #    \n",
    "        #)\n",
    "        self.mlp=MLP(in_channels= 2 * in_channels + edge_attr_dim, hidden_channels= hidden_channels,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor)-> Tensor:\n",
    "        return self.mlp(torch.cat([x_j, x_i, edge_attr], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f7ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Conv_edges(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_attr_dim, hidden_channels, out_channels):\n",
    "        super(MLP_Conv_edges, self).__init__(aggr='add')  # 'add' aggregation for summing messages\n",
    "        #self.mlp = nn.Sequential(\n",
    "        #    nn.Linear(2*in_channels + edge_attr_dim, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, hidden_channels),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(hidden_channels, out_channels)\n",
    "        #    \n",
    "        #)\n",
    "        self.mlp=MLP(in_channels= 2 * in_channels + edge_attr_dim, hidden_channels= hidden_channels,out_channels=out_channels, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr)-> Tensor:\n",
    "        return self.edge_updater(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def edge_update(self, x_j: Tensor, x_i: Tensor, edge_attr: Tensor)-> Tensor:\n",
    "        return self.mlp(torch.cat([x_j, x_i, edge_attr], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6695e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Layer(torch.nn.Module):\n",
    "    def __init__(self, node_size, edge_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.msg1 = Graph_Attn(node_size,edge_size,hidden_size,node_size, nheads = 3)\n",
    "        #self.msg1 = MLP_Conv_nodes(node_size,edge_size,hidden_size,node_size)\n",
    "        self.norm_node1 = LayerNorm(node_size, mode = 'node')\n",
    "        self.norm_node2 = LayerNorm(node_size, mode = 'node')\n",
    "        self.norm_edge1 = LayerNorm(edge_size, mode = 'node')\n",
    "        #self.feed_forward = MLP(in_channels=node_size, hidden_channels= 4*node_size,out_channels=node_size, num_layers=2, norm = 'layer', dropout = 0.1)\n",
    "        self.feed_forward = MLP(in_channels=node_size, hidden_channels= 4*node_size,out_channels=node_size, num_layers=2, norm = 'layer', dropout = 0.1, act = 'gelu')\n",
    "        self.edge_message = MLP_Conv_edges(node_size,edge_size,int(4*hidden_size),edge_size)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        msg = self.msg1(x,edge_index,edge_attr)\n",
    "        x1 = self.norm_node1(x + self.dropout(msg))\n",
    "        x2 = self.feed_forward(x1)\n",
    "        x3 = self.norm_node2(x1 + self.dropout(x2))\n",
    "        edge_msg = self.edge_message(x3,edge_index,edge_attr)\n",
    "        edge_attr1 = self.norm_edge1(edge_attr + self.dropout(edge_msg))\n",
    "        return x3, edge_attr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd6f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AA_Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AA_Classifier, self).__init__()\n",
    "        self.node_feature_size = 133\n",
    "        self.node_feature_hidden_size = 128\n",
    "        self.node_feature_size_out = 133\n",
    "        self.edge_dim = 114\n",
    "        self.dropout = 0.1\n",
    "        self.Droput = nn.Dropout(p = self.dropout)\n",
    "        #self.ff_out = MLP(in_channels=self.node_feature_size, hidden_channels= 64,out_channels=20, num_layers=2, norm = 'layer', dropout = 0.0)\n",
    "        self.ff_out = Linear(self.node_feature_size, 20)\n",
    "        #self.ff_out2 = Linear(self.node_feature_size, 11)\n",
    "        #self.relu = nn.LeakyReLU(negative_slope = 0.2)\n",
    "        \n",
    "        self.conv1 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv2 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv3 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        self.conv4 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv5 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv6 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv7 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv8 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        #self.conv9 = Encoder_Layer(self.node_feature_size,self.edge_dim,self.node_feature_hidden_size)\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self,graph):\n",
    "        x, edge_index, edge_attr = graph.x,graph.edge_index,graph.edge_attr\n",
    "        \n",
    "        x1, new_attr = self.conv1(x, edge_index,edge_attr)\n",
    "        x1, new_attr = self.conv2(x1, edge_index,new_attr)\n",
    "        x1, new_attr = self.conv3(x1, edge_index,new_attr)\n",
    "        x1, new_attr = self.conv4(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv5(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv6(x1, edge_index,new_attr)    \n",
    "        #x1, new_attr = self.conv7(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv8(x1, edge_index,new_attr)\n",
    "        #x1, new_attr = self.conv9(x1, edge_index,new_attr)\n",
    "        \n",
    "        return self.ff_out(x1)#, self.ff_out2(x1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b3d4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_list_clean = torch.load('full_graphs_mn_rm_6_11302023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90cf2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = torch.load('train_name.pt')\n",
    "test_name = torch.load('test_name.pt')\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for graph in graph_list_clean:\n",
    "    if graph.label in train_name:\n",
    "        train_data.append(graph)\n",
    "    elif graph.label in test_name:\n",
    "        val_data.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f579a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3986\n",
      "Val: 1330\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", len(train_data))\n",
    "print(\"Val:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "828931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxs = torch.load('idxs_11302023.pt')\n",
    "#train_idxs = idxs[0].detach().numpy()\n",
    "#val_idxs = idxs[1].detach().numpy()\n",
    "#train_data = [graph_list_clean[int(i)] for i in train_idxs]\n",
    "#val_data = [graph_list_clean[int(i)] for i in val_idxs]\n",
    "#torch.save(train_data, 'train_data_12152023')\n",
    "#torch.save(val_data, 'val_data_12152023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8e11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_data, 'train_data_12202023')\n",
    "#torch.save(val_data, 'val_data_12202023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84b4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_batch_size = 8\n",
    "full_dl = DataLoader(graph_list,batch_size = 1, shuffle = True)\n",
    "train_dl = DataLoader(train_data,batch_size = train_batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_data,batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "662598a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AA_embeddings.pkl', 'rb') as f:\n",
    "    AA_embeddings = pickle.load(f)\n",
    "\n",
    "AA_3_letters = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE','LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51399da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossf = torch.nn.CrossEntropyLoss(reduction = 'mean', label_smoothing = 0.1)\n",
    "def custom_loss(predict, truth, lossf):\n",
    "    loss1 = lossf(predict[0], truth[0])\n",
    "    loss2 = lossf(predict[1], truth[1])\n",
    "    return (loss1 + loss2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd24a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Loss: 63.0116 Val Loss: 61.4250\n",
      "Epoch [2/1000] Loss: 59.1496 Val Loss: 58.6495\n",
      "Epoch [3/1000] Loss: 56.3227 Val Loss: 56.2046\n",
      "Epoch [4/1000] Loss: 54.3516 Val Loss: 54.8734\n",
      "Epoch [5/1000] Loss: 52.9044 Val Loss: 53.2981\n",
      "Epoch [6/1000] Loss: 51.5698 Val Loss: 52.0437\n",
      "Epoch [7/1000] Loss: 50.5457 Val Loss: 51.3342\n",
      "Epoch [8/1000] Loss: 49.4939 Val Loss: 50.1952\n",
      "Epoch [9/1000] Loss: 48.6505 Val Loss: 49.5509\n",
      "Epoch [10/1000] Loss: 47.7795 Val Loss: 48.5526\n",
      "Epoch [11/1000] Loss: 46.9801 Val Loss: 47.7358\n",
      "Epoch [12/1000] Loss: 46.2745 Val Loss: 47.0487\n",
      "Epoch [13/1000] Loss: 45.6871 Val Loss: 46.3933\n",
      "Epoch [14/1000] Loss: 44.9222 Val Loss: 45.6543\n",
      "Epoch [15/1000] Loss: 44.3258 Val Loss: 45.2036\n",
      "Epoch [16/1000] Loss: 43.7518 Val Loss: 44.6805\n",
      "Epoch [17/1000] Loss: 43.1555 Val Loss: 44.1447\n",
      "Epoch [18/1000] Loss: 42.6600 Val Loss: 43.5076\n",
      "Epoch [19/1000] Loss: 42.1390 Val Loss: 43.0104\n",
      "Epoch [20/1000] Loss: 41.7516 Val Loss: 42.7103\n",
      "Epoch [21/1000] Loss: 41.1286 Val Loss: 42.2626\n",
      "Epoch [22/1000] Loss: 40.7387 Val Loss: 42.1123\n",
      "Epoch [23/1000] Loss: 40.3438 Val Loss: 41.4166\n",
      "Epoch [24/1000] Loss: 39.9596 Val Loss: 41.0724\n",
      "Epoch [25/1000] Loss: 39.5460 Val Loss: 40.6133\n",
      "Epoch [26/1000] Loss: 39.1032 Val Loss: 40.4626\n",
      "Epoch [27/1000] Loss: 38.7748 Val Loss: 40.0627\n",
      "Epoch [28/1000] Loss: 38.4033 Val Loss: 39.9099\n",
      "Epoch [29/1000] Loss: 38.1109 Val Loss: 39.7004\n",
      "Epoch [30/1000] Loss: 37.8380 Val Loss: 39.3779\n",
      "Epoch [31/1000] Loss: 37.4206 Val Loss: 38.9718\n",
      "Epoch [32/1000] Loss: 37.1626 Val Loss: 38.9606\n",
      "Epoch [33/1000] Loss: 36.7854 Val Loss: 38.3982\n",
      "Epoch [34/1000] Loss: 36.5911 Val Loss: 38.1279\n",
      "Epoch [35/1000] Loss: 36.3117 Val Loss: 37.9142\n",
      "Epoch [36/1000] Loss: 35.9424 Val Loss: 37.8860\n",
      "Epoch [37/1000] Loss: 35.7410 Val Loss: 37.7684\n",
      "Epoch [38/1000] Loss: 35.4943 Val Loss: 37.5064\n",
      "Epoch [39/1000] Loss: 35.2732 Val Loss: 37.0536\n",
      "Epoch [40/1000] Loss: 34.9949 Val Loss: 37.2972\n",
      "Epoch [41/1000] Loss: 34.8175 Val Loss: 36.9115\n",
      "Epoch [42/1000] Loss: 34.4527 Val Loss: 36.7104\n",
      "Epoch [43/1000] Loss: 34.3450 Val Loss: 36.6662\n",
      "Epoch [44/1000] Loss: 34.0804 Val Loss: 36.3506\n",
      "Epoch [45/1000] Loss: 33.9588 Val Loss: 36.3997\n",
      "Epoch [46/1000] Loss: 33.7360 Val Loss: 35.8902\n",
      "Epoch [47/1000] Loss: 33.5109 Val Loss: 36.1712\n",
      "Epoch [48/1000] Loss: 33.3007 Val Loss: 35.8095\n",
      "Epoch [49/1000] Loss: 33.1092 Val Loss: 35.6163\n",
      "Epoch [50/1000] Loss: 32.8966 Val Loss: 35.5891\n",
      "Epoch [51/1000] Loss: 32.6308 Val Loss: 35.3945\n",
      "Epoch [52/1000] Loss: 32.5463 Val Loss: 35.2932\n",
      "Epoch [53/1000] Loss: 32.4189 Val Loss: 35.0748\n",
      "Epoch [54/1000] Loss: 32.1969 Val Loss: 35.3245\n",
      "Epoch [55/1000] Loss: 32.0011 Val Loss: 34.9333\n",
      "Epoch [56/1000] Loss: 31.8467 Val Loss: 34.9374\n",
      "Epoch [57/1000] Loss: 31.7128 Val Loss: 34.6782\n",
      "Epoch [58/1000] Loss: 31.5621 Val Loss: 34.5801\n",
      "Epoch [59/1000] Loss: 31.4317 Val Loss: 34.4892\n",
      "Epoch [60/1000] Loss: 31.1812 Val Loss: 34.2401\n",
      "Epoch [61/1000] Loss: 31.1741 Val Loss: 34.2403\n",
      "Epoch [62/1000] Loss: 30.9989 Val Loss: 34.2975\n",
      "Epoch [63/1000] Loss: 30.8759 Val Loss: 34.2732\n",
      "Epoch [64/1000] Loss: 30.7476 Val Loss: 34.1152\n",
      "Epoch [65/1000] Loss: 30.5842 Val Loss: 34.0463\n",
      "Epoch [66/1000] Loss: 30.3954 Val Loss: 33.7567\n",
      "Epoch [67/1000] Loss: 30.2407 Val Loss: 33.8432\n",
      "Epoch [68/1000] Loss: 30.2192 Val Loss: 33.4953\n",
      "Epoch [69/1000] Loss: 30.0239 Val Loss: 33.5078\n",
      "Epoch [70/1000] Loss: 29.9146 Val Loss: 33.5204\n",
      "Epoch [71/1000] Loss: 29.7749 Val Loss: 33.2778\n",
      "Epoch [72/1000] Loss: 29.6888 Val Loss: 33.4067\n",
      "Epoch [73/1000] Loss: 29.4622 Val Loss: 33.2076\n",
      "Epoch [74/1000] Loss: 29.4489 Val Loss: 33.2912\n",
      "Epoch [75/1000] Loss: 29.4120 Val Loss: 33.1014\n",
      "Epoch [76/1000] Loss: 29.2268 Val Loss: 33.1465\n",
      "Epoch [77/1000] Loss: 29.0930 Val Loss: 32.7692\n",
      "Epoch [78/1000] Loss: 29.0027 Val Loss: 32.9672\n",
      "Epoch [79/1000] Loss: 28.8450 Val Loss: 32.6696\n",
      "Epoch [80/1000] Loss: 28.8909 Val Loss: 32.8651\n",
      "Epoch [81/1000] Loss: 28.6496 Val Loss: 32.8000\n",
      "Epoch [82/1000] Loss: 28.5201 Val Loss: 32.6572\n",
      "Epoch [83/1000] Loss: 28.4415 Val Loss: 32.5997\n",
      "Epoch [84/1000] Loss: 28.4239 Val Loss: 32.6521\n",
      "Epoch [85/1000] Loss: 28.3015 Val Loss: 32.6825\n",
      "Epoch [86/1000] Loss: 28.2087 Val Loss: 32.4301\n",
      "Epoch [87/1000] Loss: 28.1415 Val Loss: 32.4552\n",
      "Epoch [88/1000] Loss: 28.0188 Val Loss: 32.3864\n",
      "Epoch [89/1000] Loss: 27.9338 Val Loss: 32.1529\n",
      "Epoch [90/1000] Loss: 27.8533 Val Loss: 32.1717\n",
      "Epoch [91/1000] Loss: 27.8091 Val Loss: 32.3300\n",
      "Epoch [92/1000] Loss: 27.6845 Val Loss: 32.0463\n",
      "Epoch [93/1000] Loss: 27.4760 Val Loss: 32.3501\n",
      "Epoch [94/1000] Loss: 27.5035 Val Loss: 32.0067\n",
      "Epoch [95/1000] Loss: 27.3940 Val Loss: 32.0097\n",
      "Epoch [96/1000] Loss: 27.2931 Val Loss: 31.9320\n",
      "Epoch [97/1000] Loss: 27.1664 Val Loss: 31.8292\n",
      "Epoch [98/1000] Loss: 27.0762 Val Loss: 31.8153\n",
      "Epoch [99/1000] Loss: 27.0268 Val Loss: 31.7721\n",
      "Epoch [100/1000] Loss: 27.0621 Val Loss: 31.7452\n",
      "Epoch [101/1000] Loss: 26.9307 Val Loss: 31.6858\n",
      "Epoch [102/1000] Loss: 26.8127 Val Loss: 31.7866\n",
      "Epoch [103/1000] Loss: 26.6908 Val Loss: 31.6466\n",
      "Epoch [104/1000] Loss: 26.6123 Val Loss: 31.5902\n",
      "Epoch [105/1000] Loss: 26.6536 Val Loss: 31.6367\n",
      "Epoch [106/1000] Loss: 26.5660 Val Loss: 31.5479\n",
      "Epoch [107/1000] Loss: 26.4710 Val Loss: 31.3768\n",
      "Epoch [108/1000] Loss: 26.3671 Val Loss: 31.4028\n",
      "Epoch [109/1000] Loss: 26.4141 Val Loss: 31.2669\n",
      "Epoch [110/1000] Loss: 26.2277 Val Loss: 31.4235\n",
      "Epoch [111/1000] Loss: 26.1576 Val Loss: 31.2194\n",
      "Epoch [112/1000] Loss: 26.1262 Val Loss: 31.2326\n",
      "Epoch [113/1000] Loss: 26.0103 Val Loss: 31.1781\n",
      "Epoch [114/1000] Loss: 25.9802 Val Loss: 31.1947\n",
      "Epoch [115/1000] Loss: 25.9231 Val Loss: 31.2104\n",
      "Epoch [116/1000] Loss: 25.8594 Val Loss: 31.2960\n",
      "Epoch [117/1000] Loss: 25.8931 Val Loss: 31.1488\n",
      "Epoch [118/1000] Loss: 25.7458 Val Loss: 31.2657\n",
      "Epoch [119/1000] Loss: 25.6998 Val Loss: 31.1096\n",
      "Epoch [120/1000] Loss: 25.5836 Val Loss: 31.2339\n",
      "Epoch [121/1000] Loss: 25.5824 Val Loss: 30.9527\n",
      "Epoch [122/1000] Loss: 25.4266 Val Loss: 31.1241\n",
      "Epoch [123/1000] Loss: 25.4703 Val Loss: 30.8956\n",
      "Epoch [124/1000] Loss: 25.3282 Val Loss: 30.9700\n",
      "Epoch [125/1000] Loss: 25.2461 Val Loss: 30.9131\n",
      "Epoch [126/1000] Loss: 25.2307 Val Loss: 30.8212\n",
      "Epoch [127/1000] Loss: 25.1006 Val Loss: 30.9631\n",
      "Epoch [128/1000] Loss: 25.0627 Val Loss: 30.8083\n",
      "Epoch [129/1000] Loss: 25.0280 Val Loss: 30.8306\n",
      "Epoch [130/1000] Loss: 25.0750 Val Loss: 31.0920\n",
      "Epoch [131/1000] Loss: 24.9124 Val Loss: 30.9001\n",
      "Epoch [132/1000] Loss: 24.8793 Val Loss: 31.0423\n",
      "Epoch [133/1000] Loss: 24.8405 Val Loss: 30.7497\n",
      "Epoch [134/1000] Loss: 24.6747 Val Loss: 30.7958\n",
      "Epoch [135/1000] Loss: 24.7205 Val Loss: 30.8540\n",
      "Epoch [136/1000] Loss: 24.7359 Val Loss: 30.9183\n",
      "Epoch [137/1000] Loss: 24.6413 Val Loss: 30.5081\n",
      "Epoch [138/1000] Loss: 24.5358 Val Loss: 30.7466\n",
      "Epoch [139/1000] Loss: 24.5603 Val Loss: 30.7041\n",
      "Epoch [140/1000] Loss: 24.3682 Val Loss: 30.5693\n",
      "Epoch [141/1000] Loss: 24.3482 Val Loss: 30.4921\n",
      "Epoch [142/1000] Loss: 24.4453 Val Loss: 30.6113\n",
      "Epoch [143/1000] Loss: 24.4388 Val Loss: 30.7534\n",
      "Epoch [144/1000] Loss: 24.3533 Val Loss: 30.4454\n",
      "Epoch [145/1000] Loss: 24.3588 Val Loss: 30.5245\n",
      "Epoch [146/1000] Loss: 24.1716 Val Loss: 30.5242\n",
      "Epoch [147/1000] Loss: 24.2467 Val Loss: 30.4143\n",
      "Epoch [148/1000] Loss: 24.0152 Val Loss: 30.4944\n",
      "Epoch [149/1000] Loss: 23.9914 Val Loss: 30.5929\n",
      "Epoch [150/1000] Loss: 23.9221 Val Loss: 30.6301\n",
      "Epoch [151/1000] Loss: 23.9743 Val Loss: 30.3381\n",
      "Epoch [152/1000] Loss: 23.8783 Val Loss: 30.5905\n",
      "Epoch [153/1000] Loss: 23.8301 Val Loss: 30.4314\n",
      "Epoch [154/1000] Loss: 23.8874 Val Loss: 30.3729\n",
      "Epoch [155/1000] Loss: 23.5732 Val Loss: 30.3882\n",
      "Epoch [156/1000] Loss: 23.7074 Val Loss: 30.4475\n",
      "Epoch [157/1000] Loss: 23.6210 Val Loss: 30.2130\n",
      "Epoch [158/1000] Loss: 23.5106 Val Loss: 30.3328\n",
      "Epoch [159/1000] Loss: 23.5673 Val Loss: 30.2946\n",
      "Epoch [160/1000] Loss: 23.4883 Val Loss: 30.4370\n",
      "Epoch [161/1000] Loss: 23.5214 Val Loss: 30.3837\n",
      "Epoch [162/1000] Loss: 23.4550 Val Loss: 30.3278\n",
      "Epoch [163/1000] Loss: 23.4928 Val Loss: 30.4413\n",
      "Epoch [164/1000] Loss: 23.3223 Val Loss: 30.4167\n",
      "Epoch [165/1000] Loss: 23.3405 Val Loss: 30.3468\n",
      "Epoch [166/1000] Loss: 23.3700 Val Loss: 30.2576\n",
      "Epoch [167/1000] Loss: 23.2162 Val Loss: 30.2385\n",
      "Epoch [168/1000] Loss: 23.2242 Val Loss: 30.1635\n",
      "Epoch [169/1000] Loss: 23.1744 Val Loss: 30.2623\n",
      "Epoch [170/1000] Loss: 23.2054 Val Loss: 30.3066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/1000] Loss: 23.1099 Val Loss: 30.2915\n",
      "Epoch [172/1000] Loss: 23.1676 Val Loss: 30.1474\n",
      "Epoch [173/1000] Loss: 23.0170 Val Loss: 30.1410\n",
      "Epoch [174/1000] Loss: 22.9844 Val Loss: 30.1811\n",
      "Epoch [175/1000] Loss: 23.0127 Val Loss: 30.2083\n",
      "Epoch [176/1000] Loss: 22.8763 Val Loss: 30.0818\n",
      "Epoch [177/1000] Loss: 22.8616 Val Loss: 30.2758\n",
      "Epoch [178/1000] Loss: 22.9524 Val Loss: 30.0438\n",
      "Epoch [179/1000] Loss: 22.9220 Val Loss: 30.1708\n",
      "Epoch [180/1000] Loss: 22.7496 Val Loss: 30.0705\n",
      "Epoch [181/1000] Loss: 22.7447 Val Loss: 30.1613\n",
      "Epoch [182/1000] Loss: 22.8077 Val Loss: 30.1353\n",
      "Epoch [183/1000] Loss: 22.6641 Val Loss: 30.1368\n",
      "Epoch [184/1000] Loss: 22.7240 Val Loss: 30.1154\n",
      "Epoch [185/1000] Loss: 22.6533 Val Loss: 29.9972\n",
      "Epoch [186/1000] Loss: 22.6394 Val Loss: 29.9983\n",
      "Epoch [187/1000] Loss: 22.5844 Val Loss: 30.0182\n",
      "Epoch [188/1000] Loss: 22.5488 Val Loss: 30.0142\n",
      "Epoch [189/1000] Loss: 22.4246 Val Loss: 30.0838\n",
      "Epoch [190/1000] Loss: 22.4040 Val Loss: 30.0600\n",
      "Epoch [191/1000] Loss: 22.4635 Val Loss: 30.0367\n",
      "Epoch [192/1000] Loss: 22.4781 Val Loss: 30.0760\n",
      "Epoch [193/1000] Loss: 22.3798 Val Loss: 30.0548\n",
      "Epoch [194/1000] Loss: 22.2358 Val Loss: 30.0102\n",
      "Epoch [195/1000] Loss: 22.3209 Val Loss: 30.1098\n",
      "Epoch [196/1000] Loss: 22.3007 Val Loss: 29.9108\n",
      "Epoch [197/1000] Loss: 22.1783 Val Loss: 29.9440\n",
      "Epoch [198/1000] Loss: 22.2646 Val Loss: 29.9876\n",
      "Epoch [199/1000] Loss: 22.2175 Val Loss: 29.8684\n",
      "Epoch [200/1000] Loss: 22.1948 Val Loss: 30.1349\n",
      "Epoch [201/1000] Loss: 22.1120 Val Loss: 29.9427\n",
      "Epoch [202/1000] Loss: 22.0637 Val Loss: 30.0110\n",
      "Epoch [203/1000] Loss: 22.1091 Val Loss: 29.9221\n",
      "Epoch [204/1000] Loss: 22.1347 Val Loss: 29.9155\n",
      "Epoch [205/1000] Loss: 22.0059 Val Loss: 29.8731\n",
      "Epoch [206/1000] Loss: 22.0278 Val Loss: 29.8499\n",
      "Epoch [207/1000] Loss: 22.0799 Val Loss: 29.8859\n",
      "Epoch [208/1000] Loss: 21.8855 Val Loss: 29.7759\n",
      "Epoch [209/1000] Loss: 21.9215 Val Loss: 29.9415\n",
      "Epoch [210/1000] Loss: 21.9121 Val Loss: 29.8729\n",
      "Epoch [211/1000] Loss: 21.8105 Val Loss: 29.8759\n",
      "Epoch [212/1000] Loss: 21.8841 Val Loss: 29.8738\n",
      "Epoch [213/1000] Loss: 21.8343 Val Loss: 29.9598\n",
      "Epoch [214/1000] Loss: 21.8419 Val Loss: 29.9456\n",
      "Epoch [215/1000] Loss: 21.7898 Val Loss: 29.7159\n",
      "Epoch [216/1000] Loss: 21.7523 Val Loss: 29.8922\n",
      "Epoch [217/1000] Loss: 21.7489 Val Loss: 29.8625\n",
      "Epoch [218/1000] Loss: 21.6951 Val Loss: 29.8729\n",
      "Epoch [219/1000] Loss: 21.6741 Val Loss: 29.7605\n",
      "Epoch [220/1000] Loss: 21.6945 Val Loss: 29.8777\n",
      "Epoch [221/1000] Loss: 21.5945 Val Loss: 29.8399\n",
      "Epoch [222/1000] Loss: 21.6072 Val Loss: 29.8625\n",
      "Epoch [223/1000] Loss: 21.5532 Val Loss: 29.8558\n",
      "Epoch [224/1000] Loss: 21.5377 Val Loss: 29.9733\n",
      "Epoch [225/1000] Loss: 21.4846 Val Loss: 29.7789\n",
      "Epoch [226/1000] Loss: 21.5007 Val Loss: 29.9069\n",
      "Epoch [227/1000] Loss: 21.4547 Val Loss: 29.8054\n",
      "Epoch [228/1000] Loss: 21.4192 Val Loss: 29.8768\n",
      "Epoch [229/1000] Loss: 21.4804 Val Loss: 29.7795\n",
      "Epoch [230/1000] Loss: 21.5258 Val Loss: 29.6467\n",
      "Epoch [231/1000] Loss: 21.4768 Val Loss: 29.8225\n",
      "Epoch [232/1000] Loss: 21.3698 Val Loss: 29.7404\n",
      "Epoch [233/1000] Loss: 21.3797 Val Loss: 29.9058\n",
      "Epoch [234/1000] Loss: 21.3614 Val Loss: 29.6857\n",
      "Epoch [235/1000] Loss: 21.2518 Val Loss: 29.8350\n",
      "Epoch [236/1000] Loss: 21.1968 Val Loss: 29.8328\n",
      "Epoch [237/1000] Loss: 21.2267 Val Loss: 29.9060\n",
      "Epoch [238/1000] Loss: 21.2673 Val Loss: 29.6627\n",
      "Epoch [239/1000] Loss: 21.2257 Val Loss: 29.7248\n",
      "Epoch [240/1000] Loss: 21.2262 Val Loss: 29.9139\n",
      "Epoch [241/1000] Loss: 21.1765 Val Loss: 29.7388\n",
      "Epoch [242/1000] Loss: 21.1933 Val Loss: 29.7193\n",
      "Epoch [243/1000] Loss: 21.0466 Val Loss: 29.6209\n",
      "Epoch [244/1000] Loss: 21.1068 Val Loss: 29.7580\n",
      "Epoch [245/1000] Loss: 21.0336 Val Loss: 29.7904\n",
      "Epoch [246/1000] Loss: 20.9955 Val Loss: 29.9271\n",
      "Epoch [247/1000] Loss: 21.0604 Val Loss: 29.6421\n",
      "Epoch [248/1000] Loss: 20.9255 Val Loss: 29.8554\n",
      "Epoch [249/1000] Loss: 21.1212 Val Loss: 29.8126\n",
      "Epoch [250/1000] Loss: 20.9971 Val Loss: 29.7175\n",
      "Epoch [251/1000] Loss: 20.9807 Val Loss: 29.7754\n",
      "Epoch [252/1000] Loss: 20.8571 Val Loss: 29.7732\n",
      "Epoch [253/1000] Loss: 20.9104 Val Loss: 29.7888\n",
      "Epoch [254/1000] Loss: 20.9223 Val Loss: 29.7029\n",
      "Epoch [255/1000] Loss: 20.8802 Val Loss: 29.8034\n",
      "Epoch [256/1000] Loss: 20.8776 Val Loss: 29.8261\n",
      "Epoch [257/1000] Loss: 20.8770 Val Loss: 29.6679\n",
      "Epoch [258/1000] Loss: 20.8084 Val Loss: 29.6807\n",
      "Epoch [259/1000] Loss: 20.7563 Val Loss: 29.8679\n",
      "Epoch [260/1000] Loss: 20.8388 Val Loss: 29.6804\n",
      "Epoch [261/1000] Loss: 20.6916 Val Loss: 29.8500\n",
      "Epoch [262/1000] Loss: 20.7489 Val Loss: 29.7670\n",
      "Epoch [263/1000] Loss: 20.7734 Val Loss: 29.7306\n",
      "Epoch [264/1000] Loss: 20.6182 Val Loss: 29.7983\n",
      "Epoch [265/1000] Loss: 20.6993 Val Loss: 29.7793\n",
      "Epoch [266/1000] Loss: 20.6826 Val Loss: 29.7482\n",
      "Epoch [267/1000] Loss: 20.7267 Val Loss: 29.7647\n",
      "Epoch [268/1000] Loss: 20.6660 Val Loss: 29.6385\n",
      "Epoch [269/1000] Loss: 20.6722 Val Loss: 29.6633\n",
      "Epoch [270/1000] Loss: 20.5583 Val Loss: 29.7146\n",
      "Epoch [271/1000] Loss: 20.5951 Val Loss: 29.5879\n",
      "Epoch [272/1000] Loss: 20.6266 Val Loss: 29.6818\n",
      "Epoch [273/1000] Loss: 20.5876 Val Loss: 29.6211\n",
      "Epoch [274/1000] Loss: 20.5666 Val Loss: 29.4884\n",
      "Epoch [275/1000] Loss: 20.4868 Val Loss: 29.7571\n",
      "Epoch [276/1000] Loss: 20.5360 Val Loss: 29.6028\n",
      "Epoch [277/1000] Loss: 20.5266 Val Loss: 29.6011\n",
      "Epoch [278/1000] Loss: 20.5632 Val Loss: 29.6626\n",
      "Epoch [279/1000] Loss: 20.4092 Val Loss: 29.5713\n",
      "Epoch [280/1000] Loss: 20.5098 Val Loss: 29.6810\n",
      "Epoch [281/1000] Loss: 20.4838 Val Loss: 29.6782\n",
      "Epoch [282/1000] Loss: 20.3759 Val Loss: 29.6757\n",
      "Epoch [283/1000] Loss: 20.4247 Val Loss: 29.5760\n",
      "Epoch [284/1000] Loss: 20.3910 Val Loss: 29.6469\n",
      "Epoch [285/1000] Loss: 20.3790 Val Loss: 29.5929\n",
      "Epoch [286/1000] Loss: 20.4040 Val Loss: 29.5683\n",
      "Epoch [287/1000] Loss: 20.3850 Val Loss: 29.6799\n",
      "Epoch [288/1000] Loss: 20.3585 Val Loss: 29.5846\n",
      "Epoch [289/1000] Loss: 20.3861 Val Loss: 29.6034\n",
      "Epoch [290/1000] Loss: 20.3993 Val Loss: 29.7213\n",
      "Epoch [291/1000] Loss: 20.3088 Val Loss: 29.5864\n",
      "Epoch [292/1000] Loss: 20.2475 Val Loss: 29.6905\n",
      "Epoch [293/1000] Loss: 20.3251 Val Loss: 29.6014\n",
      "Epoch [294/1000] Loss: 20.2298 Val Loss: 29.6491\n",
      "Epoch [295/1000] Loss: 20.2768 Val Loss: 29.5642\n",
      "Epoch [296/1000] Loss: 20.1766 Val Loss: 29.6022\n",
      "Epoch [297/1000] Loss: 20.1865 Val Loss: 29.6159\n",
      "Epoch [298/1000] Loss: 20.2586 Val Loss: 29.5980\n",
      "Epoch [299/1000] Loss: 20.2325 Val Loss: 29.6799\n",
      "Epoch [300/1000] Loss: 20.1859 Val Loss: 29.5691\n",
      "Epoch [301/1000] Loss: 20.1171 Val Loss: 29.6682\n",
      "Epoch [302/1000] Loss: 20.1926 Val Loss: 29.5795\n",
      "Epoch [303/1000] Loss: 20.0907 Val Loss: 29.5917\n",
      "Epoch [304/1000] Loss: 20.0791 Val Loss: 29.5774\n",
      "Epoch [305/1000] Loss: 20.1177 Val Loss: 29.5106\n",
      "Epoch [306/1000] Loss: 20.1002 Val Loss: 29.6065\n",
      "Epoch [307/1000] Loss: 20.0214 Val Loss: 29.7403\n",
      "Epoch [308/1000] Loss: 19.9808 Val Loss: 29.5890\n",
      "Epoch [309/1000] Loss: 20.0515 Val Loss: 29.5494\n",
      "Epoch [310/1000] Loss: 20.0335 Val Loss: 29.4968\n",
      "Epoch [311/1000] Loss: 20.0548 Val Loss: 29.5802\n",
      "Epoch [312/1000] Loss: 20.0303 Val Loss: 29.5098\n",
      "Epoch [313/1000] Loss: 20.0192 Val Loss: 29.5679\n",
      "Epoch [314/1000] Loss: 19.9799 Val Loss: 29.5420\n",
      "Epoch [315/1000] Loss: 20.0382 Val Loss: 29.4774\n",
      "Epoch [316/1000] Loss: 19.9416 Val Loss: 29.6267\n",
      "Epoch [317/1000] Loss: 19.9618 Val Loss: 29.5213\n",
      "Epoch [318/1000] Loss: 19.9694 Val Loss: 29.5213\n",
      "Epoch [319/1000] Loss: 19.9031 Val Loss: 29.5515\n",
      "Epoch [320/1000] Loss: 19.9154 Val Loss: 29.6990\n",
      "Epoch [321/1000] Loss: 19.9724 Val Loss: 29.5966\n",
      "Epoch [322/1000] Loss: 19.9050 Val Loss: 29.5028\n",
      "Epoch [323/1000] Loss: 19.7723 Val Loss: 29.6346\n",
      "Epoch [324/1000] Loss: 19.7372 Val Loss: 29.6313\n",
      "Epoch [325/1000] Loss: 19.8715 Val Loss: 29.6247\n",
      "Epoch [326/1000] Loss: 19.8025 Val Loss: 29.7019\n",
      "Epoch [327/1000] Loss: 19.8677 Val Loss: 29.6759\n",
      "Epoch [328/1000] Loss: 19.7979 Val Loss: 29.6264\n",
      "Epoch [329/1000] Loss: 19.7821 Val Loss: 29.5695\n",
      "Epoch [330/1000] Loss: 19.7724 Val Loss: 29.5011\n",
      "Epoch [331/1000] Loss: 19.8835 Val Loss: 29.6998\n",
      "Epoch [332/1000] Loss: 19.7329 Val Loss: 29.6990\n",
      "Epoch [333/1000] Loss: 19.7492 Val Loss: 29.5261\n",
      "Epoch [334/1000] Loss: 19.8168 Val Loss: 29.6908\n",
      "Epoch [335/1000] Loss: 19.8815 Val Loss: 29.5843\n",
      "Epoch [336/1000] Loss: 19.6600 Val Loss: 29.6122\n",
      "Epoch [337/1000] Loss: 19.7147 Val Loss: 29.5369\n",
      "Epoch [338/1000] Loss: 19.7500 Val Loss: 29.6200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [339/1000] Loss: 19.6607 Val Loss: 29.5941\n",
      "Epoch [340/1000] Loss: 19.6685 Val Loss: 29.6420\n",
      "Epoch [341/1000] Loss: 19.7432 Val Loss: 29.5717\n",
      "Epoch [342/1000] Loss: 19.6833 Val Loss: 29.7362\n",
      "Epoch [343/1000] Loss: 19.7124 Val Loss: 29.6018\n",
      "Epoch [344/1000] Loss: 19.6513 Val Loss: 29.5897\n",
      "Epoch [345/1000] Loss: 19.6165 Val Loss: 29.7433\n",
      "Epoch [346/1000] Loss: 19.5965 Val Loss: 29.6349\n",
      "Epoch [347/1000] Loss: 19.5647 Val Loss: 29.6732\n",
      "Epoch [348/1000] Loss: 19.6521 Val Loss: 29.6217\n",
      "Epoch [349/1000] Loss: 19.5466 Val Loss: 29.6944\n",
      "Epoch [350/1000] Loss: 19.5649 Val Loss: 29.6070\n",
      "Epoch [351/1000] Loss: 19.5210 Val Loss: 29.7090\n",
      "Epoch [352/1000] Loss: 19.5551 Val Loss: 29.6323\n",
      "Epoch [353/1000] Loss: 19.5076 Val Loss: 29.5540\n",
      "Epoch [354/1000] Loss: 19.6072 Val Loss: 29.6480\n",
      "Epoch [355/1000] Loss: 19.5388 Val Loss: 29.7681\n",
      "Epoch [356/1000] Loss: 19.5834 Val Loss: 29.6729\n",
      "Epoch [357/1000] Loss: 19.4584 Val Loss: 29.6726\n",
      "Epoch [358/1000] Loss: 19.4399 Val Loss: 29.6689\n",
      "Epoch [359/1000] Loss: 19.5086 Val Loss: 29.4715\n",
      "Epoch [360/1000] Loss: 19.5015 Val Loss: 29.6197\n",
      "Epoch [361/1000] Loss: 19.4666 Val Loss: 29.6271\n",
      "Epoch [362/1000] Loss: 19.4822 Val Loss: 29.4230\n",
      "Epoch [363/1000] Loss: 19.4183 Val Loss: 29.5071\n",
      "Epoch [364/1000] Loss: 19.5029 Val Loss: 29.5077\n",
      "Epoch [365/1000] Loss: 19.3843 Val Loss: 29.6538\n",
      "Epoch [366/1000] Loss: 19.3536 Val Loss: 29.7550\n",
      "Epoch [367/1000] Loss: 19.4728 Val Loss: 29.5126\n",
      "Epoch [368/1000] Loss: 19.3395 Val Loss: 29.5108\n",
      "Epoch [369/1000] Loss: 19.4389 Val Loss: 29.5287\n",
      "Epoch [370/1000] Loss: 19.4380 Val Loss: 29.5656\n",
      "Epoch [371/1000] Loss: 19.4173 Val Loss: 29.5901\n",
      "Epoch [372/1000] Loss: 19.3218 Val Loss: 29.5184\n",
      "Epoch [373/1000] Loss: 19.3090 Val Loss: 29.5458\n",
      "Epoch [374/1000] Loss: 19.3783 Val Loss: 29.4233\n",
      "Epoch [375/1000] Loss: 19.3098 Val Loss: 29.6107\n",
      "Epoch [376/1000] Loss: 19.4265 Val Loss: 29.7143\n",
      "Epoch [377/1000] Loss: 19.3216 Val Loss: 29.5512\n",
      "Epoch [378/1000] Loss: 19.3273 Val Loss: 29.5548\n",
      "Epoch [379/1000] Loss: 19.3326 Val Loss: 29.5719\n",
      "Epoch [380/1000] Loss: 19.3079 Val Loss: 29.4390\n",
      "Epoch [381/1000] Loss: 19.3140 Val Loss: 29.3738\n",
      "Epoch [382/1000] Loss: 19.3242 Val Loss: 29.5317\n",
      "Epoch [383/1000] Loss: 19.2978 Val Loss: 29.5459\n",
      "Epoch [384/1000] Loss: 19.2173 Val Loss: 29.5521\n",
      "Epoch [385/1000] Loss: 19.2113 Val Loss: 29.5897\n",
      "Epoch [386/1000] Loss: 19.2374 Val Loss: 29.4040\n",
      "Epoch [387/1000] Loss: 19.2268 Val Loss: 29.5485\n",
      "Epoch [388/1000] Loss: 19.1986 Val Loss: 29.7055\n",
      "Epoch [389/1000] Loss: 19.1844 Val Loss: 29.5873\n",
      "Epoch [390/1000] Loss: 19.1846 Val Loss: 29.5034\n",
      "Epoch [391/1000] Loss: 19.1802 Val Loss: 29.5366\n",
      "Epoch [392/1000] Loss: 19.1564 Val Loss: 29.5136\n",
      "Epoch [393/1000] Loss: 19.2225 Val Loss: 29.5230\n",
      "Epoch [394/1000] Loss: 19.1869 Val Loss: 29.6071\n",
      "Epoch [395/1000] Loss: 19.1165 Val Loss: 29.6747\n",
      "Epoch [396/1000] Loss: 19.1628 Val Loss: 29.3952\n",
      "Epoch [397/1000] Loss: 19.2033 Val Loss: 29.4260\n",
      "Epoch [398/1000] Loss: 19.1848 Val Loss: 29.4673\n",
      "Epoch [399/1000] Loss: 19.1965 Val Loss: 29.4044\n",
      "Epoch [400/1000] Loss: 19.0981 Val Loss: 29.5279\n",
      "Epoch [401/1000] Loss: 19.1022 Val Loss: 29.5339\n",
      "Epoch [402/1000] Loss: 19.0984 Val Loss: 29.4898\n",
      "Epoch [403/1000] Loss: 19.1173 Val Loss: 29.4719\n",
      "Epoch [404/1000] Loss: 19.1008 Val Loss: 29.4489\n",
      "Epoch [405/1000] Loss: 19.1176 Val Loss: 29.5340\n",
      "Epoch [406/1000] Loss: 19.0836 Val Loss: 29.4858\n",
      "Epoch [407/1000] Loss: 19.0511 Val Loss: 29.5063\n",
      "Epoch [408/1000] Loss: 19.0129 Val Loss: 29.3966\n",
      "Epoch [409/1000] Loss: 19.0560 Val Loss: 29.3871\n",
      "Epoch [410/1000] Loss: 19.0311 Val Loss: 29.4628\n",
      "Epoch [411/1000] Loss: 18.9886 Val Loss: 29.3917\n",
      "Epoch [412/1000] Loss: 19.0535 Val Loss: 29.5459\n",
      "Epoch [413/1000] Loss: 19.0414 Val Loss: 29.3909\n",
      "Epoch [414/1000] Loss: 19.0115 Val Loss: 29.3798\n",
      "Epoch [415/1000] Loss: 19.0564 Val Loss: 29.4953\n",
      "Epoch [416/1000] Loss: 19.0100 Val Loss: 29.3586\n",
      "Epoch [417/1000] Loss: 19.0000 Val Loss: 29.6618\n",
      "Epoch [418/1000] Loss: 19.0095 Val Loss: 29.4543\n",
      "Epoch [419/1000] Loss: 19.0501 Val Loss: 29.5346\n",
      "Epoch [420/1000] Loss: 18.9960 Val Loss: 29.4657\n",
      "Epoch [421/1000] Loss: 18.9541 Val Loss: 29.4280\n",
      "Epoch [422/1000] Loss: 18.9274 Val Loss: 29.5219\n",
      "Epoch [423/1000] Loss: 19.0428 Val Loss: 29.4307\n",
      "Epoch [424/1000] Loss: 18.7798 Val Loss: 29.5343\n",
      "Epoch [425/1000] Loss: 18.9868 Val Loss: 29.3885\n",
      "Epoch [426/1000] Loss: 18.8507 Val Loss: 29.4745\n",
      "Epoch [427/1000] Loss: 18.9573 Val Loss: 29.5387\n",
      "Epoch [428/1000] Loss: 18.9400 Val Loss: 29.5672\n",
      "Epoch [429/1000] Loss: 18.8750 Val Loss: 29.4868\n",
      "Epoch [430/1000] Loss: 18.8974 Val Loss: 29.4531\n",
      "Epoch [431/1000] Loss: 18.9001 Val Loss: 29.3824\n",
      "Epoch [432/1000] Loss: 18.8283 Val Loss: 29.5237\n",
      "Epoch [433/1000] Loss: 18.8212 Val Loss: 29.4485\n",
      "Epoch [434/1000] Loss: 18.7897 Val Loss: 29.6295\n",
      "Epoch [435/1000] Loss: 18.8421 Val Loss: 29.3387\n",
      "Epoch [436/1000] Loss: 18.8435 Val Loss: 29.5328\n",
      "Epoch [437/1000] Loss: 18.9007 Val Loss: 29.3677\n",
      "Epoch [438/1000] Loss: 18.8421 Val Loss: 29.4518\n",
      "Epoch [439/1000] Loss: 18.8945 Val Loss: 29.2855\n",
      "Epoch [440/1000] Loss: 18.7935 Val Loss: 29.5734\n",
      "Epoch [441/1000] Loss: 18.7916 Val Loss: 29.5050\n",
      "Epoch [442/1000] Loss: 18.8106 Val Loss: 29.4804\n",
      "Epoch [443/1000] Loss: 18.8045 Val Loss: 29.4564\n",
      "Epoch [444/1000] Loss: 18.7658 Val Loss: 29.5336\n",
      "Epoch [445/1000] Loss: 18.7400 Val Loss: 29.6363\n",
      "Epoch [446/1000] Loss: 18.7909 Val Loss: 29.4914\n",
      "Epoch [447/1000] Loss: 18.8542 Val Loss: 29.4280\n",
      "Epoch [448/1000] Loss: 18.8244 Val Loss: 29.3728\n",
      "Epoch [449/1000] Loss: 18.6812 Val Loss: 29.3875\n",
      "Epoch [450/1000] Loss: 18.7852 Val Loss: 29.4305\n",
      "Epoch [451/1000] Loss: 18.7088 Val Loss: 29.4143\n",
      "Epoch [452/1000] Loss: 18.8212 Val Loss: 29.3381\n",
      "Epoch [453/1000] Loss: 18.7246 Val Loss: 29.3650\n",
      "Epoch [454/1000] Loss: 18.7484 Val Loss: 29.4627\n",
      "Epoch [455/1000] Loss: 18.7959 Val Loss: 29.4232\n",
      "Epoch [456/1000] Loss: 18.7523 Val Loss: 29.3746\n",
      "Epoch [457/1000] Loss: 18.7000 Val Loss: 29.4712\n",
      "Epoch [458/1000] Loss: 18.7047 Val Loss: 29.4771\n",
      "Epoch [459/1000] Loss: 18.6610 Val Loss: 29.5345\n",
      "Epoch [460/1000] Loss: 18.6843 Val Loss: 29.4047\n",
      "Epoch [461/1000] Loss: 18.6752 Val Loss: 29.4621\n",
      "Epoch [462/1000] Loss: 18.6069 Val Loss: 29.5158\n",
      "Epoch [463/1000] Loss: 18.6503 Val Loss: 29.4241\n",
      "Epoch [464/1000] Loss: 18.6399 Val Loss: 29.5980\n",
      "Epoch [465/1000] Loss: 18.6259 Val Loss: 29.5150\n",
      "Epoch [466/1000] Loss: 18.7184 Val Loss: 29.5066\n",
      "Epoch [467/1000] Loss: 18.6293 Val Loss: 29.5396\n",
      "Epoch [468/1000] Loss: 18.6729 Val Loss: 29.5241\n",
      "Epoch [469/1000] Loss: 18.5958 Val Loss: 29.5151\n",
      "Epoch [470/1000] Loss: 18.6455 Val Loss: 29.4368\n",
      "Epoch [471/1000] Loss: 18.6301 Val Loss: 29.6358\n",
      "Epoch [472/1000] Loss: 18.6675 Val Loss: 29.3674\n",
      "Epoch [473/1000] Loss: 18.6320 Val Loss: 29.3305\n",
      "Epoch [474/1000] Loss: 18.5741 Val Loss: 29.5556\n",
      "Epoch [475/1000] Loss: 18.6429 Val Loss: 29.2883\n",
      "Epoch [476/1000] Loss: 18.6080 Val Loss: 29.4961\n",
      "Epoch [477/1000] Loss: 18.5555 Val Loss: 29.4712\n",
      "Epoch [478/1000] Loss: 18.6249 Val Loss: 29.5441\n",
      "Epoch [479/1000] Loss: 18.6026 Val Loss: 29.5092\n",
      "Epoch [480/1000] Loss: 18.5914 Val Loss: 29.5748\n",
      "Epoch [481/1000] Loss: 18.6295 Val Loss: 29.6208\n",
      "Epoch [482/1000] Loss: 18.5353 Val Loss: 29.5298\n",
      "Epoch [483/1000] Loss: 18.5512 Val Loss: 29.4058\n",
      "Epoch [484/1000] Loss: 18.4940 Val Loss: 29.5077\n",
      "Epoch [485/1000] Loss: 18.5931 Val Loss: 29.4501\n",
      "Epoch [486/1000] Loss: 18.5455 Val Loss: 29.5410\n",
      "Epoch [487/1000] Loss: 18.5401 Val Loss: 29.3096\n",
      "Epoch [488/1000] Loss: 18.6026 Val Loss: 29.4252\n",
      "Epoch [489/1000] Loss: 18.4737 Val Loss: 29.6206\n",
      "Epoch [490/1000] Loss: 18.5777 Val Loss: 29.5227\n",
      "Epoch [491/1000] Loss: 18.5433 Val Loss: 29.5239\n",
      "Epoch [492/1000] Loss: 18.4660 Val Loss: 29.4485\n",
      "Epoch [493/1000] Loss: 18.5537 Val Loss: 29.4180\n",
      "Epoch [494/1000] Loss: 18.5369 Val Loss: 29.5573\n",
      "Epoch [495/1000] Loss: 18.4618 Val Loss: 29.4345\n",
      "Epoch [496/1000] Loss: 18.4544 Val Loss: 29.5550\n",
      "Epoch [497/1000] Loss: 18.4797 Val Loss: 29.4836\n",
      "Epoch [498/1000] Loss: 18.5442 Val Loss: 29.4291\n",
      "Epoch [499/1000] Loss: 18.4167 Val Loss: 29.4608\n",
      "Epoch [500/1000] Loss: 18.4718 Val Loss: 29.4161\n",
      "Epoch [501/1000] Loss: 18.4346 Val Loss: 29.4378\n",
      "Epoch [502/1000] Loss: 18.4550 Val Loss: 29.3731\n",
      "Epoch [503/1000] Loss: 18.4692 Val Loss: 29.4654\n",
      "Epoch [504/1000] Loss: 18.4671 Val Loss: 29.3676\n",
      "Epoch [505/1000] Loss: 18.4386 Val Loss: 29.4442\n",
      "Epoch [506/1000] Loss: 18.4279 Val Loss: 29.4050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [507/1000] Loss: 18.4067 Val Loss: 29.4010\n",
      "Epoch [508/1000] Loss: 18.4109 Val Loss: 29.4453\n",
      "Epoch [509/1000] Loss: 18.4371 Val Loss: 29.4811\n",
      "Epoch [510/1000] Loss: 18.3991 Val Loss: 29.3622\n",
      "Epoch [511/1000] Loss: 18.3871 Val Loss: 29.3036\n",
      "Epoch [512/1000] Loss: 18.3581 Val Loss: 29.3967\n",
      "Epoch [513/1000] Loss: 18.3972 Val Loss: 29.5496\n",
      "Epoch [514/1000] Loss: 18.3879 Val Loss: 29.3920\n",
      "Epoch [515/1000] Loss: 18.3300 Val Loss: 29.4629\n",
      "Epoch [516/1000] Loss: 18.3854 Val Loss: 29.5667\n",
      "Epoch [517/1000] Loss: 18.3856 Val Loss: 29.4216\n",
      "Epoch [518/1000] Loss: 18.3914 Val Loss: 29.3803\n",
      "Epoch [519/1000] Loss: 18.4605 Val Loss: 29.4037\n",
      "Epoch [520/1000] Loss: 18.4186 Val Loss: 29.4472\n",
      "Epoch [521/1000] Loss: 18.3778 Val Loss: 29.4698\n",
      "Epoch [522/1000] Loss: 18.3312 Val Loss: 29.4138\n",
      "Epoch [523/1000] Loss: 18.3399 Val Loss: 29.4556\n",
      "Epoch [524/1000] Loss: 18.3370 Val Loss: 29.5329\n",
      "Epoch [525/1000] Loss: 18.2767 Val Loss: 29.5373\n",
      "Epoch [526/1000] Loss: 18.3478 Val Loss: 29.3744\n",
      "Epoch [527/1000] Loss: 18.2504 Val Loss: 29.4391\n",
      "Epoch [528/1000] Loss: 18.2538 Val Loss: 29.4739\n",
      "Epoch [529/1000] Loss: 18.2408 Val Loss: 29.4761\n",
      "Epoch [530/1000] Loss: 18.3531 Val Loss: 29.5665\n",
      "Epoch [531/1000] Loss: 18.2816 Val Loss: 29.5329\n",
      "Epoch [532/1000] Loss: 18.3570 Val Loss: 29.4407\n",
      "Epoch [533/1000] Loss: 18.2870 Val Loss: 29.4449\n",
      "Epoch [534/1000] Loss: 18.1990 Val Loss: 29.5314\n",
      "Epoch [535/1000] Loss: 18.3423 Val Loss: 29.4587\n",
      "Epoch [536/1000] Loss: 18.2793 Val Loss: 29.4377\n",
      "Epoch [537/1000] Loss: 18.3391 Val Loss: 29.6156\n",
      "Epoch [538/1000] Loss: 18.2568 Val Loss: 29.4488\n",
      "Epoch [539/1000] Loss: 18.2557 Val Loss: 29.5228\n",
      "Epoch [540/1000] Loss: 18.3051 Val Loss: 29.5651\n",
      "Epoch [541/1000] Loss: 18.2688 Val Loss: 29.3909\n",
      "Epoch [542/1000] Loss: 18.2680 Val Loss: 29.4486\n",
      "Epoch [543/1000] Loss: 18.2291 Val Loss: 29.6234\n",
      "Epoch [544/1000] Loss: 18.2140 Val Loss: 29.5002\n",
      "Epoch [545/1000] Loss: 18.2905 Val Loss: 29.4184\n",
      "Epoch [546/1000] Loss: 18.3286 Val Loss: 29.3894\n",
      "Epoch [547/1000] Loss: 18.2432 Val Loss: 29.4970\n",
      "Epoch [548/1000] Loss: 18.2125 Val Loss: 29.3380\n",
      "Epoch [549/1000] Loss: 18.2540 Val Loss: 29.3538\n",
      "Epoch [550/1000] Loss: 18.1985 Val Loss: 29.5076\n",
      "Epoch [551/1000] Loss: 18.1670 Val Loss: 29.5369\n",
      "Epoch [552/1000] Loss: 18.2253 Val Loss: 29.4583\n",
      "Epoch [553/1000] Loss: 18.1923 Val Loss: 29.4590\n",
      "Epoch [554/1000] Loss: 18.1970 Val Loss: 29.3954\n",
      "Epoch [555/1000] Loss: 18.1958 Val Loss: 29.3210\n",
      "Epoch [556/1000] Loss: 18.2542 Val Loss: 29.4214\n",
      "Epoch [557/1000] Loss: 18.2237 Val Loss: 29.4723\n",
      "Epoch [558/1000] Loss: 18.2013 Val Loss: 29.6088\n",
      "Epoch [559/1000] Loss: 18.1793 Val Loss: 29.4751\n",
      "Epoch [560/1000] Loss: 18.1793 Val Loss: 29.6146\n",
      "Epoch [561/1000] Loss: 18.1279 Val Loss: 29.6158\n",
      "Epoch [562/1000] Loss: 18.1753 Val Loss: 29.4315\n",
      "Epoch [563/1000] Loss: 18.2015 Val Loss: 29.4758\n",
      "Epoch [564/1000] Loss: 18.1570 Val Loss: 29.5070\n",
      "Epoch [565/1000] Loss: 18.2087 Val Loss: 29.5167\n",
      "Epoch [566/1000] Loss: 18.1311 Val Loss: 29.4500\n",
      "Epoch [567/1000] Loss: 18.0853 Val Loss: 29.6722\n",
      "Epoch [568/1000] Loss: 18.2075 Val Loss: 29.5034\n",
      "Epoch [569/1000] Loss: 18.1579 Val Loss: 29.4801\n",
      "Epoch [570/1000] Loss: 18.0880 Val Loss: 29.5643\n",
      "Epoch [571/1000] Loss: 18.1694 Val Loss: 29.4331\n",
      "Epoch [572/1000] Loss: 18.1231 Val Loss: 29.4904\n",
      "Epoch [573/1000] Loss: 18.1941 Val Loss: 29.4739\n",
      "Epoch [574/1000] Loss: 18.1549 Val Loss: 29.4672\n",
      "Epoch [575/1000] Loss: 18.1120 Val Loss: 29.4838\n",
      "Epoch [576/1000] Loss: 18.1371 Val Loss: 29.4521\n",
      "Epoch [577/1000] Loss: 18.1144 Val Loss: 29.3788\n",
      "Epoch [578/1000] Loss: 18.0814 Val Loss: 29.5318\n",
      "Epoch [579/1000] Loss: 18.1154 Val Loss: 29.3981\n",
      "Epoch [580/1000] Loss: 18.1360 Val Loss: 29.4691\n",
      "Epoch [581/1000] Loss: 17.9589 Val Loss: 29.3898\n",
      "Epoch [582/1000] Loss: 18.0663 Val Loss: 29.4467\n",
      "Epoch [583/1000] Loss: 18.0725 Val Loss: 29.4742\n",
      "Epoch [584/1000] Loss: 18.1158 Val Loss: 29.3971\n",
      "Epoch [585/1000] Loss: 18.1048 Val Loss: 29.4130\n",
      "Epoch [586/1000] Loss: 18.0965 Val Loss: 29.3326\n",
      "Epoch [587/1000] Loss: 18.0906 Val Loss: 29.3674\n",
      "Epoch [588/1000] Loss: 17.9947 Val Loss: 29.5071\n",
      "Epoch [589/1000] Loss: 18.0287 Val Loss: 29.5204\n",
      "Epoch [590/1000] Loss: 18.1088 Val Loss: 29.3955\n",
      "Epoch [591/1000] Loss: 18.0731 Val Loss: 29.3960\n",
      "Epoch [592/1000] Loss: 18.1106 Val Loss: 29.3479\n",
      "Epoch [593/1000] Loss: 18.0816 Val Loss: 29.3972\n",
      "Epoch [594/1000] Loss: 18.0138 Val Loss: 29.4561\n",
      "Epoch [595/1000] Loss: 18.0458 Val Loss: 29.4834\n",
      "Epoch [596/1000] Loss: 18.0420 Val Loss: 29.4771\n",
      "Epoch [597/1000] Loss: 18.1335 Val Loss: 29.4381\n",
      "Epoch [598/1000] Loss: 18.0498 Val Loss: 29.4940\n",
      "Epoch [599/1000] Loss: 18.0744 Val Loss: 29.4399\n",
      "Epoch [600/1000] Loss: 18.0358 Val Loss: 29.3988\n",
      "Epoch [601/1000] Loss: 17.9937 Val Loss: 29.4438\n",
      "Epoch [602/1000] Loss: 17.9866 Val Loss: 29.4491\n",
      "Epoch [603/1000] Loss: 18.0251 Val Loss: 29.4291\n",
      "Epoch [604/1000] Loss: 17.9516 Val Loss: 29.4504\n",
      "Epoch [605/1000] Loss: 18.0218 Val Loss: 29.4383\n",
      "Epoch [606/1000] Loss: 18.0344 Val Loss: 29.3895\n",
      "Epoch [607/1000] Loss: 18.1066 Val Loss: 29.2719\n",
      "Epoch [608/1000] Loss: 18.0123 Val Loss: 29.4546\n",
      "Epoch [609/1000] Loss: 17.9571 Val Loss: 29.4073\n",
      "Epoch [610/1000] Loss: 17.9737 Val Loss: 29.2761\n",
      "Epoch [611/1000] Loss: 18.0528 Val Loss: 29.5032\n",
      "Epoch [612/1000] Loss: 17.9476 Val Loss: 29.4175\n",
      "Epoch [613/1000] Loss: 18.0692 Val Loss: 29.3063\n",
      "Epoch [614/1000] Loss: 17.9971 Val Loss: 29.4443\n",
      "Epoch [615/1000] Loss: 18.0232 Val Loss: 29.4957\n",
      "Epoch [616/1000] Loss: 17.9333 Val Loss: 29.4655\n",
      "Epoch [617/1000] Loss: 18.0198 Val Loss: 29.4641\n",
      "Epoch [618/1000] Loss: 17.9443 Val Loss: 29.3780\n",
      "Epoch [619/1000] Loss: 17.9415 Val Loss: 29.4577\n",
      "Epoch [620/1000] Loss: 17.9477 Val Loss: 29.4602\n",
      "Epoch [621/1000] Loss: 17.9980 Val Loss: 29.4310\n",
      "Epoch [622/1000] Loss: 17.9494 Val Loss: 29.3812\n",
      "Epoch [623/1000] Loss: 17.9106 Val Loss: 29.3903\n",
      "Epoch [624/1000] Loss: 17.9376 Val Loss: 29.5282\n",
      "Epoch [625/1000] Loss: 17.9286 Val Loss: 29.3101\n",
      "Epoch [626/1000] Loss: 17.9514 Val Loss: 29.4269\n",
      "Epoch [627/1000] Loss: 17.9391 Val Loss: 29.3888\n",
      "Epoch [628/1000] Loss: 17.9641 Val Loss: 29.6019\n",
      "Epoch [629/1000] Loss: 17.8759 Val Loss: 29.5169\n",
      "Epoch [630/1000] Loss: 17.8886 Val Loss: 29.3310\n",
      "Epoch [631/1000] Loss: 17.8960 Val Loss: 29.3575\n",
      "Epoch [632/1000] Loss: 17.8873 Val Loss: 29.4318\n",
      "Epoch [633/1000] Loss: 17.9318 Val Loss: 29.2550\n",
      "Epoch [634/1000] Loss: 17.8749 Val Loss: 29.3140\n",
      "Epoch [635/1000] Loss: 17.9116 Val Loss: 29.4219\n",
      "Epoch [636/1000] Loss: 17.9725 Val Loss: 29.3520\n",
      "Epoch [637/1000] Loss: 17.9386 Val Loss: 29.4309\n",
      "Epoch [638/1000] Loss: 17.8802 Val Loss: 29.4044\n",
      "Epoch [639/1000] Loss: 17.9064 Val Loss: 29.4162\n",
      "Epoch [640/1000] Loss: 17.8839 Val Loss: 29.4243\n",
      "Epoch [641/1000] Loss: 17.9105 Val Loss: 29.4370\n",
      "Epoch [642/1000] Loss: 17.8606 Val Loss: 29.3969\n",
      "Epoch [643/1000] Loss: 17.8507 Val Loss: 29.3028\n",
      "Epoch [644/1000] Loss: 17.8137 Val Loss: 29.4347\n",
      "Epoch [645/1000] Loss: 17.8909 Val Loss: 29.3063\n",
      "Epoch [646/1000] Loss: 17.8328 Val Loss: 29.2466\n",
      "Epoch [647/1000] Loss: 17.8857 Val Loss: 29.4195\n",
      "Epoch [648/1000] Loss: 17.8536 Val Loss: 29.3851\n",
      "Epoch [649/1000] Loss: 17.8642 Val Loss: 29.2953\n",
      "Epoch [650/1000] Loss: 17.8849 Val Loss: 29.3147\n",
      "Epoch [651/1000] Loss: 17.8696 Val Loss: 29.2737\n",
      "Epoch [652/1000] Loss: 17.8093 Val Loss: 29.3383\n",
      "Epoch [653/1000] Loss: 17.8672 Val Loss: 29.3536\n",
      "Epoch [654/1000] Loss: 17.8909 Val Loss: 29.5065\n",
      "Epoch [655/1000] Loss: 17.7269 Val Loss: 29.3912\n",
      "Epoch [656/1000] Loss: 17.8555 Val Loss: 29.3849\n",
      "Epoch [657/1000] Loss: 17.8433 Val Loss: 29.4104\n",
      "Epoch [658/1000] Loss: 17.8080 Val Loss: 29.3624\n",
      "Epoch [659/1000] Loss: 17.8219 Val Loss: 29.3274\n",
      "Epoch [660/1000] Loss: 17.8192 Val Loss: 29.3269\n",
      "Epoch [661/1000] Loss: 17.8325 Val Loss: 29.3854\n",
      "Epoch [662/1000] Loss: 17.7629 Val Loss: 29.4397\n",
      "Epoch [663/1000] Loss: 17.8479 Val Loss: 29.4653\n",
      "Epoch [664/1000] Loss: 17.7930 Val Loss: 29.3777\n",
      "Epoch [665/1000] Loss: 17.7697 Val Loss: 29.3694\n",
      "Epoch [666/1000] Loss: 17.7747 Val Loss: 29.4221\n",
      "Epoch [667/1000] Loss: 17.8211 Val Loss: 29.4404\n",
      "Epoch [668/1000] Loss: 17.7489 Val Loss: 29.3662\n",
      "Epoch [669/1000] Loss: 17.7925 Val Loss: 29.3690\n",
      "Epoch [670/1000] Loss: 17.8192 Val Loss: 29.4690\n",
      "Epoch [671/1000] Loss: 17.8323 Val Loss: 29.4047\n",
      "Epoch [672/1000] Loss: 17.7423 Val Loss: 29.5066\n",
      "Epoch [673/1000] Loss: 17.7579 Val Loss: 29.4222\n",
      "Epoch [674/1000] Loss: 17.7423 Val Loss: 29.4113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [675/1000] Loss: 17.7576 Val Loss: 29.5364\n",
      "Epoch [676/1000] Loss: 17.7675 Val Loss: 29.4274\n",
      "Epoch [677/1000] Loss: 17.7374 Val Loss: 29.3421\n",
      "Epoch [678/1000] Loss: 17.7956 Val Loss: 29.4276\n",
      "Epoch [679/1000] Loss: 17.7528 Val Loss: 29.4312\n",
      "Epoch [680/1000] Loss: 17.7873 Val Loss: 29.3346\n",
      "Epoch [681/1000] Loss: 17.7127 Val Loss: 29.4427\n",
      "Epoch [682/1000] Loss: 17.7892 Val Loss: 29.3953\n",
      "Epoch [683/1000] Loss: 17.7585 Val Loss: 29.4313\n",
      "Epoch [684/1000] Loss: 17.6908 Val Loss: 29.4820\n",
      "Epoch [685/1000] Loss: 17.7552 Val Loss: 29.3658\n",
      "Epoch [686/1000] Loss: 17.7107 Val Loss: 29.4479\n",
      "Epoch [687/1000] Loss: 17.7568 Val Loss: 29.4288\n",
      "Epoch [688/1000] Loss: 17.7008 Val Loss: 29.4227\n",
      "Epoch [689/1000] Loss: 17.7192 Val Loss: 29.3288\n",
      "Epoch [690/1000] Loss: 17.6952 Val Loss: 29.4615\n",
      "Epoch [691/1000] Loss: 17.7786 Val Loss: 29.3310\n",
      "Epoch [692/1000] Loss: 17.7421 Val Loss: 29.3402\n",
      "Epoch [693/1000] Loss: 17.7720 Val Loss: 29.3282\n",
      "Epoch [694/1000] Loss: 17.7229 Val Loss: 29.3381\n",
      "Epoch [695/1000] Loss: 17.7205 Val Loss: 29.4883\n",
      "Epoch [696/1000] Loss: 17.7689 Val Loss: 29.3124\n",
      "Epoch [697/1000] Loss: 17.7151 Val Loss: 29.2907\n",
      "Epoch [698/1000] Loss: 17.6898 Val Loss: 29.3434\n",
      "Epoch [699/1000] Loss: 17.7052 Val Loss: 29.4653\n",
      "Epoch [700/1000] Loss: 17.7025 Val Loss: 29.3672\n",
      "Epoch [701/1000] Loss: 17.6692 Val Loss: 29.5065\n",
      "Epoch [702/1000] Loss: 17.6095 Val Loss: 29.4341\n",
      "Epoch [703/1000] Loss: 17.6198 Val Loss: 29.4289\n",
      "Epoch [704/1000] Loss: 17.7419 Val Loss: 29.3972\n",
      "Epoch [705/1000] Loss: 17.6954 Val Loss: 29.4142\n",
      "Epoch [706/1000] Loss: 17.6203 Val Loss: 29.5087\n",
      "Epoch [707/1000] Loss: 17.6013 Val Loss: 29.4252\n",
      "Epoch [708/1000] Loss: 17.6954 Val Loss: 29.4761\n",
      "Epoch [709/1000] Loss: 17.5834 Val Loss: 29.4265\n",
      "Epoch [710/1000] Loss: 17.7037 Val Loss: 29.3905\n",
      "Epoch [711/1000] Loss: 17.6479 Val Loss: 29.4779\n",
      "Epoch [712/1000] Loss: 17.6618 Val Loss: 29.4373\n",
      "Epoch [713/1000] Loss: 17.6685 Val Loss: 29.4752\n",
      "Epoch [714/1000] Loss: 17.6456 Val Loss: 29.3424\n",
      "Epoch [715/1000] Loss: 17.6728 Val Loss: 29.4986\n",
      "Epoch [716/1000] Loss: 17.6043 Val Loss: 29.5890\n",
      "Epoch [717/1000] Loss: 17.6664 Val Loss: 29.4251\n",
      "Epoch [718/1000] Loss: 17.6932 Val Loss: 29.3960\n",
      "Epoch [719/1000] Loss: 17.6413 Val Loss: 29.3942\n",
      "Epoch [720/1000] Loss: 17.6493 Val Loss: 29.5283\n",
      "Epoch [721/1000] Loss: 17.6475 Val Loss: 29.4051\n",
      "Epoch [722/1000] Loss: 17.6360 Val Loss: 29.4694\n",
      "Epoch [723/1000] Loss: 17.5998 Val Loss: 29.4557\n",
      "Epoch [724/1000] Loss: 17.6668 Val Loss: 29.4866\n",
      "Epoch [725/1000] Loss: 17.6170 Val Loss: 29.4212\n",
      "Epoch [726/1000] Loss: 17.6253 Val Loss: 29.4556\n",
      "Epoch [727/1000] Loss: 17.5815 Val Loss: 29.3710\n",
      "Epoch [728/1000] Loss: 17.6243 Val Loss: 29.4269\n",
      "Epoch [729/1000] Loss: 17.6164 Val Loss: 29.2811\n",
      "Epoch [730/1000] Loss: 17.6474 Val Loss: 29.3625\n",
      "Epoch [731/1000] Loss: 17.5810 Val Loss: 29.4264\n",
      "Epoch [732/1000] Loss: 17.6718 Val Loss: 29.3338\n",
      "Epoch [733/1000] Loss: 17.6970 Val Loss: 29.4385\n",
      "Epoch [734/1000] Loss: 17.5057 Val Loss: 29.3629\n",
      "Epoch [735/1000] Loss: 17.5031 Val Loss: 29.4925\n",
      "Epoch [736/1000] Loss: 17.5976 Val Loss: 29.3648\n",
      "Epoch [737/1000] Loss: 17.6596 Val Loss: 29.3732\n",
      "Epoch [738/1000] Loss: 17.5342 Val Loss: 29.5316\n",
      "Epoch [739/1000] Loss: 17.6267 Val Loss: 29.3557\n",
      "Epoch [740/1000] Loss: 17.5789 Val Loss: 29.3512\n",
      "Epoch [741/1000] Loss: 17.5603 Val Loss: 29.4104\n",
      "Epoch [742/1000] Loss: 17.6737 Val Loss: 29.4083\n",
      "Epoch [743/1000] Loss: 17.5862 Val Loss: 29.4321\n",
      "Epoch [744/1000] Loss: 17.5793 Val Loss: 29.3303\n",
      "Epoch [745/1000] Loss: 17.4962 Val Loss: 29.3736\n",
      "Epoch [746/1000] Loss: 17.6136 Val Loss: 29.4295\n",
      "Epoch [747/1000] Loss: 17.5445 Val Loss: 29.3923\n",
      "Epoch [748/1000] Loss: 17.5467 Val Loss: 29.4564\n",
      "Epoch [749/1000] Loss: 17.5362 Val Loss: 29.4381\n",
      "Epoch [750/1000] Loss: 17.6110 Val Loss: 29.4083\n",
      "Epoch [751/1000] Loss: 17.5105 Val Loss: 29.4168\n",
      "Epoch [752/1000] Loss: 17.5874 Val Loss: 29.3922\n",
      "Epoch [753/1000] Loss: 17.5524 Val Loss: 29.3383\n",
      "Epoch [754/1000] Loss: 17.5384 Val Loss: 29.3661\n",
      "Epoch [755/1000] Loss: 17.5742 Val Loss: 29.3879\n",
      "Epoch [756/1000] Loss: 17.4921 Val Loss: 29.4503\n",
      "Epoch [757/1000] Loss: 17.5461 Val Loss: 29.3660\n",
      "Epoch [758/1000] Loss: 17.5471 Val Loss: 29.4084\n",
      "Epoch [759/1000] Loss: 17.5715 Val Loss: 29.4145\n",
      "Epoch [760/1000] Loss: 17.5653 Val Loss: 29.4133\n",
      "Epoch [761/1000] Loss: 17.5300 Val Loss: 29.3549\n",
      "Epoch [762/1000] Loss: 17.5365 Val Loss: 29.3698\n",
      "Epoch [763/1000] Loss: 17.6001 Val Loss: 29.2682\n",
      "Epoch [764/1000] Loss: 17.5337 Val Loss: 29.4452\n",
      "Epoch [765/1000] Loss: 17.4869 Val Loss: 29.4278\n",
      "Epoch [766/1000] Loss: 17.5168 Val Loss: 29.3515\n",
      "Epoch [767/1000] Loss: 17.4995 Val Loss: 29.3944\n",
      "Epoch [768/1000] Loss: 17.5549 Val Loss: 29.3788\n",
      "Epoch [769/1000] Loss: 17.5459 Val Loss: 29.3840\n",
      "Epoch [770/1000] Loss: 17.4658 Val Loss: 29.3965\n",
      "Epoch [771/1000] Loss: 17.4932 Val Loss: 29.4759\n",
      "Epoch [772/1000] Loss: 17.5234 Val Loss: 29.3748\n",
      "Epoch [773/1000] Loss: 17.5599 Val Loss: 29.4344\n",
      "Epoch [774/1000] Loss: 17.4671 Val Loss: 29.3286\n",
      "Epoch [775/1000] Loss: 17.5288 Val Loss: 29.3504\n",
      "Epoch [776/1000] Loss: 17.4897 Val Loss: 29.4037\n",
      "Epoch [777/1000] Loss: 17.4700 Val Loss: 29.4342\n",
      "Epoch [778/1000] Loss: 17.5535 Val Loss: 29.4310\n",
      "Epoch [779/1000] Loss: 17.4789 Val Loss: 29.3705\n",
      "Epoch [780/1000] Loss: 17.5259 Val Loss: 29.4254\n",
      "Epoch [781/1000] Loss: 17.5072 Val Loss: 29.4943\n",
      "Epoch [782/1000] Loss: 17.5287 Val Loss: 29.4629\n",
      "Epoch [783/1000] Loss: 17.5063 Val Loss: 29.3950\n",
      "Epoch [784/1000] Loss: 17.4660 Val Loss: 29.4949\n",
      "Epoch [785/1000] Loss: 17.4895 Val Loss: 29.4196\n",
      "Epoch [786/1000] Loss: 17.4920 Val Loss: 29.3437\n",
      "Epoch [787/1000] Loss: 17.4336 Val Loss: 29.4712\n",
      "Epoch [788/1000] Loss: 17.4812 Val Loss: 29.3547\n",
      "Epoch [789/1000] Loss: 17.4091 Val Loss: 29.4171\n",
      "Epoch [790/1000] Loss: 17.3839 Val Loss: 29.4670\n",
      "Epoch [791/1000] Loss: 17.4209 Val Loss: 29.4586\n",
      "Epoch [792/1000] Loss: 17.4656 Val Loss: 29.4482\n",
      "Epoch [793/1000] Loss: 17.4398 Val Loss: 29.3749\n",
      "Epoch [794/1000] Loss: 17.4700 Val Loss: 29.5574\n",
      "Epoch [795/1000] Loss: 17.4512 Val Loss: 29.4734\n",
      "Epoch [796/1000] Loss: 17.4654 Val Loss: 29.3871\n",
      "Epoch [797/1000] Loss: 17.4235 Val Loss: 29.3386\n",
      "Epoch [798/1000] Loss: 17.4493 Val Loss: 29.3379\n",
      "Epoch [799/1000] Loss: 17.4679 Val Loss: 29.4496\n",
      "Epoch [800/1000] Loss: 17.4167 Val Loss: 29.4289\n",
      "Epoch [801/1000] Loss: 17.3823 Val Loss: 29.4294\n",
      "Epoch [802/1000] Loss: 17.4489 Val Loss: 29.3703\n",
      "Epoch [803/1000] Loss: 17.4729 Val Loss: 29.4287\n",
      "Epoch [804/1000] Loss: 17.4307 Val Loss: 29.5031\n",
      "Epoch [805/1000] Loss: 17.4763 Val Loss: 29.3652\n",
      "Epoch [806/1000] Loss: 17.4172 Val Loss: 29.5223\n",
      "Epoch [807/1000] Loss: 17.4404 Val Loss: 29.4142\n",
      "Epoch [808/1000] Loss: 17.4784 Val Loss: 29.3320\n",
      "Epoch [809/1000] Loss: 17.4464 Val Loss: 29.4354\n",
      "Epoch [810/1000] Loss: 17.4987 Val Loss: 29.2576\n",
      "Epoch [811/1000] Loss: 17.4265 Val Loss: 29.4544\n",
      "Epoch [812/1000] Loss: 17.4001 Val Loss: 29.4239\n",
      "Epoch [813/1000] Loss: 17.4182 Val Loss: 29.4320\n",
      "Epoch [814/1000] Loss: 17.3881 Val Loss: 29.5293\n",
      "Epoch [815/1000] Loss: 17.3883 Val Loss: 29.4005\n",
      "Epoch [816/1000] Loss: 17.4218 Val Loss: 29.4821\n",
      "Epoch [817/1000] Loss: 17.4172 Val Loss: 29.3771\n",
      "Epoch [818/1000] Loss: 17.3877 Val Loss: 29.4580\n",
      "Epoch [819/1000] Loss: 17.4349 Val Loss: 29.4299\n",
      "Epoch [820/1000] Loss: 17.4056 Val Loss: 29.5208\n",
      "Epoch [821/1000] Loss: 17.3711 Val Loss: 29.5797\n",
      "Epoch [822/1000] Loss: 17.3455 Val Loss: 29.4557\n",
      "Epoch [823/1000] Loss: 17.4884 Val Loss: 29.4018\n",
      "Epoch [824/1000] Loss: 17.3795 Val Loss: 29.4200\n",
      "Epoch [825/1000] Loss: 17.3756 Val Loss: 29.4109\n",
      "Epoch [826/1000] Loss: 17.3981 Val Loss: 29.5692\n",
      "Epoch [827/1000] Loss: 17.3761 Val Loss: 29.4098\n",
      "Epoch [828/1000] Loss: 17.4080 Val Loss: 29.4512\n",
      "Epoch [829/1000] Loss: 17.3641 Val Loss: 29.5429\n",
      "Epoch [830/1000] Loss: 17.3083 Val Loss: 29.4692\n",
      "Epoch [831/1000] Loss: 17.4257 Val Loss: 29.4134\n",
      "Epoch [832/1000] Loss: 17.4041 Val Loss: 29.4389\n",
      "Epoch [833/1000] Loss: 17.3449 Val Loss: 29.4065\n",
      "Epoch [834/1000] Loss: 17.3324 Val Loss: 29.4592\n",
      "Epoch [835/1000] Loss: 17.3602 Val Loss: 29.3888\n",
      "Epoch [836/1000] Loss: 17.3750 Val Loss: 29.4455\n",
      "Epoch [837/1000] Loss: 17.3912 Val Loss: 29.5049\n",
      "Epoch [838/1000] Loss: 17.3259 Val Loss: 29.4098\n",
      "Epoch [839/1000] Loss: 17.3978 Val Loss: 29.4158\n",
      "Epoch [840/1000] Loss: 17.4174 Val Loss: 29.4222\n",
      "Epoch [841/1000] Loss: 17.3692 Val Loss: 29.4038\n",
      "Epoch [842/1000] Loss: 17.3753 Val Loss: 29.3909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [843/1000] Loss: 17.3060 Val Loss: 29.4692\n",
      "Epoch [844/1000] Loss: 17.3480 Val Loss: 29.4170\n",
      "Epoch [845/1000] Loss: 17.3086 Val Loss: 29.3855\n",
      "Epoch [846/1000] Loss: 17.3699 Val Loss: 29.4689\n",
      "Epoch [847/1000] Loss: 17.3810 Val Loss: 29.3899\n",
      "Epoch [848/1000] Loss: 17.3443 Val Loss: 29.5274\n",
      "Epoch [849/1000] Loss: 17.3356 Val Loss: 29.4645\n",
      "Epoch [850/1000] Loss: 17.3496 Val Loss: 29.3534\n",
      "Epoch [851/1000] Loss: 17.2887 Val Loss: 29.4570\n",
      "Epoch [852/1000] Loss: 17.3164 Val Loss: 29.3949\n",
      "Epoch [853/1000] Loss: 17.2886 Val Loss: 29.4845\n",
      "Epoch [854/1000] Loss: 17.3042 Val Loss: 29.3013\n",
      "Epoch [855/1000] Loss: 17.2705 Val Loss: 29.4354\n",
      "Epoch [856/1000] Loss: 17.3375 Val Loss: 29.4602\n",
      "Epoch [857/1000] Loss: 17.3679 Val Loss: 29.4262\n",
      "Epoch [858/1000] Loss: 17.3237 Val Loss: 29.4511\n",
      "Epoch [859/1000] Loss: 17.3199 Val Loss: 29.4846\n",
      "Epoch [860/1000] Loss: 17.3537 Val Loss: 29.4953\n",
      "Epoch [861/1000] Loss: 17.2827 Val Loss: 29.4097\n",
      "Epoch [862/1000] Loss: 17.2996 Val Loss: 29.4428\n",
      "Epoch [863/1000] Loss: 17.2931 Val Loss: 29.4333\n",
      "Epoch [864/1000] Loss: 17.3720 Val Loss: 29.4268\n",
      "Epoch [865/1000] Loss: 17.3201 Val Loss: 29.3848\n",
      "Epoch [866/1000] Loss: 17.2873 Val Loss: 29.4340\n",
      "Epoch [867/1000] Loss: 17.3340 Val Loss: 29.4676\n",
      "Epoch [868/1000] Loss: 17.3301 Val Loss: 29.4232\n",
      "Epoch [869/1000] Loss: 17.2990 Val Loss: 29.4459\n",
      "Epoch [870/1000] Loss: 17.3350 Val Loss: 29.4853\n",
      "Epoch [871/1000] Loss: 17.2492 Val Loss: 29.4121\n",
      "Epoch [872/1000] Loss: 17.2730 Val Loss: 29.4737\n",
      "Epoch [873/1000] Loss: 17.2614 Val Loss: 29.4465\n",
      "Epoch [874/1000] Loss: 17.2331 Val Loss: 29.5363\n",
      "Epoch [875/1000] Loss: 17.2538 Val Loss: 29.3999\n",
      "Epoch [876/1000] Loss: 17.2890 Val Loss: 29.4710\n",
      "Epoch [877/1000] Loss: 17.2605 Val Loss: 29.3330\n",
      "Epoch [878/1000] Loss: 17.3158 Val Loss: 29.3992\n",
      "Epoch [879/1000] Loss: 17.3010 Val Loss: 29.4436\n",
      "Epoch [880/1000] Loss: 17.3303 Val Loss: 29.3120\n",
      "Epoch [881/1000] Loss: 17.2778 Val Loss: 29.4218\n",
      "Epoch [882/1000] Loss: 17.2453 Val Loss: 29.3273\n",
      "Epoch [883/1000] Loss: 17.2672 Val Loss: 29.3399\n",
      "Epoch [884/1000] Loss: 17.2215 Val Loss: 29.4778\n",
      "Epoch [885/1000] Loss: 17.2407 Val Loss: 29.3853\n",
      "Epoch [886/1000] Loss: 17.2526 Val Loss: 29.3644\n",
      "Epoch [887/1000] Loss: 17.1570 Val Loss: 29.3948\n",
      "Epoch [888/1000] Loss: 17.2840 Val Loss: 29.3641\n",
      "Epoch [889/1000] Loss: 17.2462 Val Loss: 29.4546\n",
      "Epoch [890/1000] Loss: 17.2201 Val Loss: 29.5206\n",
      "Epoch [891/1000] Loss: 17.2640 Val Loss: 29.4113\n",
      "Epoch [892/1000] Loss: 17.2059 Val Loss: 29.4023\n",
      "Epoch [893/1000] Loss: 17.2590 Val Loss: 29.4003\n",
      "Epoch [894/1000] Loss: 17.2589 Val Loss: 29.4386\n",
      "Epoch [895/1000] Loss: 17.2492 Val Loss: 29.3726\n",
      "Epoch [896/1000] Loss: 17.2665 Val Loss: 29.2819\n",
      "Epoch [897/1000] Loss: 17.1738 Val Loss: 29.4351\n",
      "Epoch [898/1000] Loss: 17.2696 Val Loss: 29.4233\n",
      "Epoch [899/1000] Loss: 17.2461 Val Loss: 29.3459\n",
      "Epoch [900/1000] Loss: 17.2228 Val Loss: 29.3589\n",
      "Epoch [901/1000] Loss: 17.1990 Val Loss: 29.4830\n",
      "Epoch [902/1000] Loss: 17.2279 Val Loss: 29.3003\n",
      "Epoch [903/1000] Loss: 17.2224 Val Loss: 29.3338\n",
      "Epoch [904/1000] Loss: 17.2288 Val Loss: 29.4444\n",
      "Epoch [905/1000] Loss: 17.2337 Val Loss: 29.3915\n",
      "Epoch [906/1000] Loss: 17.2022 Val Loss: 29.4190\n",
      "Epoch [907/1000] Loss: 17.2636 Val Loss: 29.3658\n",
      "Epoch [908/1000] Loss: 17.3125 Val Loss: 29.2988\n",
      "Epoch [909/1000] Loss: 17.2415 Val Loss: 29.3531\n",
      "Epoch [910/1000] Loss: 17.1201 Val Loss: 29.4448\n",
      "Epoch [911/1000] Loss: 17.2132 Val Loss: 29.3030\n",
      "Epoch [912/1000] Loss: 17.2138 Val Loss: 29.3409\n",
      "Epoch [913/1000] Loss: 17.2083 Val Loss: 29.3442\n",
      "Epoch [914/1000] Loss: 17.1613 Val Loss: 29.4526\n",
      "Epoch [915/1000] Loss: 17.2001 Val Loss: 29.3538\n",
      "Epoch [916/1000] Loss: 17.2079 Val Loss: 29.3676\n",
      "Epoch [917/1000] Loss: 17.1994 Val Loss: 29.3788\n",
      "Epoch [918/1000] Loss: 17.2155 Val Loss: 29.3438\n",
      "Epoch [919/1000] Loss: 17.2192 Val Loss: 29.4058\n",
      "Epoch [920/1000] Loss: 17.1175 Val Loss: 29.3432\n",
      "Epoch [921/1000] Loss: 17.1969 Val Loss: 29.2525\n",
      "Epoch [922/1000] Loss: 17.1952 Val Loss: 29.4508\n",
      "Epoch [923/1000] Loss: 17.2144 Val Loss: 29.5436\n",
      "Epoch [924/1000] Loss: 17.2572 Val Loss: 29.4262\n",
      "Epoch [925/1000] Loss: 17.1910 Val Loss: 29.4033\n",
      "Epoch [926/1000] Loss: 17.1376 Val Loss: 29.4321\n",
      "Epoch [927/1000] Loss: 17.1854 Val Loss: 29.5540\n",
      "Epoch [928/1000] Loss: 17.1635 Val Loss: 29.4988\n",
      "Epoch [929/1000] Loss: 17.1645 Val Loss: 29.4641\n",
      "Epoch [930/1000] Loss: 17.1940 Val Loss: 29.4074\n",
      "Epoch [931/1000] Loss: 17.1517 Val Loss: 29.4019\n",
      "Epoch [932/1000] Loss: 17.1595 Val Loss: 29.3585\n",
      "Epoch [933/1000] Loss: 17.2220 Val Loss: 29.3881\n",
      "Epoch [934/1000] Loss: 17.1899 Val Loss: 29.3441\n",
      "Epoch [935/1000] Loss: 17.1861 Val Loss: 29.3111\n",
      "Epoch [936/1000] Loss: 17.1515 Val Loss: 29.4094\n",
      "Epoch [937/1000] Loss: 17.1998 Val Loss: 29.5024\n",
      "Epoch [938/1000] Loss: 17.1377 Val Loss: 29.4286\n",
      "Epoch [939/1000] Loss: 17.1970 Val Loss: 29.3711\n",
      "Epoch [940/1000] Loss: 17.1657 Val Loss: 29.4604\n",
      "Epoch [941/1000] Loss: 17.1017 Val Loss: 29.4270\n",
      "Epoch [942/1000] Loss: 17.1990 Val Loss: 29.4097\n",
      "Epoch [943/1000] Loss: 17.1970 Val Loss: 29.3057\n",
      "Epoch [944/1000] Loss: 17.1219 Val Loss: 29.3400\n",
      "Epoch [945/1000] Loss: 17.1782 Val Loss: 29.3039\n",
      "Epoch [946/1000] Loss: 17.1424 Val Loss: 29.2858\n",
      "Epoch [947/1000] Loss: 17.1787 Val Loss: 29.3033\n",
      "Epoch [948/1000] Loss: 17.1136 Val Loss: 29.3709\n",
      "Epoch [949/1000] Loss: 17.1385 Val Loss: 29.3685\n",
      "Epoch [950/1000] Loss: 17.1280 Val Loss: 29.3662\n",
      "Epoch [951/1000] Loss: 17.1318 Val Loss: 29.3368\n",
      "Epoch [952/1000] Loss: 17.1752 Val Loss: 29.3845\n",
      "Epoch [953/1000] Loss: 17.1000 Val Loss: 29.4276\n",
      "Epoch [954/1000] Loss: 17.1168 Val Loss: 29.3759\n",
      "Epoch [955/1000] Loss: 17.1188 Val Loss: 29.3163\n",
      "Epoch [956/1000] Loss: 17.1127 Val Loss: 29.3554\n",
      "Epoch [957/1000] Loss: 17.1603 Val Loss: 29.3844\n",
      "Epoch [958/1000] Loss: 17.0819 Val Loss: 29.3860\n",
      "Epoch [959/1000] Loss: 17.1577 Val Loss: 29.4895\n",
      "Epoch [960/1000] Loss: 17.1341 Val Loss: 29.3611\n",
      "Epoch [961/1000] Loss: 17.1053 Val Loss: 29.4186\n",
      "Epoch [962/1000] Loss: 17.0617 Val Loss: 29.4184\n",
      "Epoch [963/1000] Loss: 17.1906 Val Loss: 29.4127\n",
      "Epoch [964/1000] Loss: 17.0984 Val Loss: 29.5505\n",
      "Epoch [965/1000] Loss: 17.1771 Val Loss: 29.4261\n",
      "Epoch [966/1000] Loss: 17.1039 Val Loss: 29.3322\n",
      "Epoch [967/1000] Loss: 17.1560 Val Loss: 29.3997\n",
      "Epoch [968/1000] Loss: 17.1015 Val Loss: 29.3310\n",
      "Epoch [969/1000] Loss: 17.2271 Val Loss: 29.3351\n",
      "Epoch [970/1000] Loss: 17.0828 Val Loss: 29.4367\n",
      "Epoch [971/1000] Loss: 17.1379 Val Loss: 29.4048\n",
      "Epoch [972/1000] Loss: 17.1372 Val Loss: 29.3804\n",
      "Epoch [973/1000] Loss: 17.1427 Val Loss: 29.3849\n",
      "Epoch [974/1000] Loss: 17.0846 Val Loss: 29.5455\n",
      "Epoch [975/1000] Loss: 17.0489 Val Loss: 29.3695\n",
      "Epoch [976/1000] Loss: 17.1160 Val Loss: 29.3995\n",
      "Epoch [977/1000] Loss: 17.0738 Val Loss: 29.3587\n",
      "Epoch [978/1000] Loss: 17.0737 Val Loss: 29.3848\n",
      "Epoch [979/1000] Loss: 17.0700 Val Loss: 29.4567\n",
      "Epoch [980/1000] Loss: 17.1387 Val Loss: 29.2982\n",
      "Epoch [981/1000] Loss: 17.1062 Val Loss: 29.3671\n",
      "Epoch [982/1000] Loss: 17.0619 Val Loss: 29.3789\n",
      "Epoch [983/1000] Loss: 17.0895 Val Loss: 29.4773\n",
      "Epoch [984/1000] Loss: 17.1182 Val Loss: 29.3097\n",
      "Epoch [985/1000] Loss: 17.0684 Val Loss: 29.4725\n",
      "Epoch [986/1000] Loss: 17.1029 Val Loss: 29.3537\n",
      "Epoch [987/1000] Loss: 17.0050 Val Loss: 29.4430\n",
      "Epoch [988/1000] Loss: 17.1304 Val Loss: 29.3898\n",
      "Epoch [989/1000] Loss: 17.0264 Val Loss: 29.3332\n",
      "Epoch [990/1000] Loss: 17.0762 Val Loss: 29.4328\n",
      "Epoch [991/1000] Loss: 17.1356 Val Loss: 29.3020\n",
      "Epoch [992/1000] Loss: 17.0340 Val Loss: 29.4589\n",
      "Epoch [993/1000] Loss: 17.0491 Val Loss: 29.3396\n",
      "Epoch [994/1000] Loss: 17.0709 Val Loss: 29.4209\n",
      "Epoch [995/1000] Loss: 17.1077 Val Loss: 29.3890\n",
      "Epoch [996/1000] Loss: 17.0432 Val Loss: 29.3061\n",
      "Epoch [997/1000] Loss: 17.1077 Val Loss: 29.3749\n",
      "Epoch [998/1000] Loss: 17.0158 Val Loss: 29.4457\n",
      "Epoch [999/1000] Loss: 17.0260 Val Loss: 29.3986\n",
      "Epoch [1000/1000] Loss: 17.0793 Val Loss: 29.2475\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AA_Classifier()\n",
    "#model.load_state_dict(torch.load('9_encder_layers_12092023.pt'))\n",
    "model.to(DEVICE) # put on GPU\n",
    "\n",
    "# Define a loss function (e.g., Mean Squared Error) and an optimizer (e.g., Adam)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'sum', label_smoothing = 0.1)\n",
    "#criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas = (0.9,0.98),eps =1e-9, lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "losses = []\n",
    "smallest = 39.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "        \n",
    "    for batch in train_dl:\n",
    "        model.train()\n",
    "        inputs = batch.to(DEVICE)\n",
    "        mask = []\n",
    "        for i in batch.mask:\n",
    "            mask += i \n",
    "        \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            for i, j in enumerate(mask):\n",
    "                if j == True:\n",
    "                    p = random.random()\n",
    "                    if p < 0.8:\n",
    "                        inputs.x[i] = torch.zeros(133)\n",
    "                    elif p >= 0.8 and p < 0.92:\n",
    "                        inputs.x[i] = AA_embeddings[random.choice(AA_3_letters)]\n",
    "        \n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #outputs, MN = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs[mask], inputs.y[mask])\n",
    "        #loss = custom_loss([outputs[mask],MN], [inputs.y[mask],inputs.Kd], lossf)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inputs= inputs.to('cpu')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    for batch in val_dl:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            inputs = batch[0].to(DEVICE)\n",
    "            \n",
    "      \n",
    "            for i, j in enumerate(inputs.mask):\n",
    "                if j == True:\n",
    "                    inputs.x[i] = torch.zeros(133)\n",
    "                    \n",
    "            outputs = model(inputs)\n",
    "            #outputs, MN = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs[inputs.mask], inputs.y[inputs.mask])\n",
    "            #loss = custom_loss([outputs[inputs.mask],MN], [inputs.y[inputs.mask],inputs.Kd], lossf)\n",
    "        \n",
    "            inputs= inputs.to('cpu')\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / (train_batch_size*len(train_dl))\n",
    "    avg_val_loss = val_loss / len(val_dl)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Val Loss: {avg_val_loss:.4f}')\n",
    "    losses.append([avg_loss,avg_val_loss])\n",
    "    \n",
    "    if avg_val_loss < smallest:\n",
    "        torch.save(model.state_dict(), 'model_weights.pt')\n",
    "        smallest = avg_val_loss\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6e276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(losses),'loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eeb056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
